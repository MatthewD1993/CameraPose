{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EuRoC data handler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# class euroc_data():\n",
    "# \tdef __init__(self,groundtruth_path):\n",
    "# \t\tgt_frame = pd.read_csv(groundtruth_path,'r')\n",
    "GROUNDTRUTH_PATH = '/home/cdeng/EuRoC/mav0/\\\n",
    "state_groundtruth_estimate0/data.csv'\n",
    "IMU_PATH = '/home/cdeng/EuRoC/mav0/imu0/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403636580838555648 1403636762738555392\n",
      "(36381, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_RS_R_x [m]</th>\n",
       "      <th>p_RS_R_y [m]</th>\n",
       "      <th>p_RS_R_z [m]</th>\n",
       "      <th>q_RS_w []</th>\n",
       "      <th>q_RS_x []</th>\n",
       "      <th>q_RS_y []</th>\n",
       "      <th>q_RS_z []</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1403636580838555648</th>\n",
       "      <td>4.688319</td>\n",
       "      <td>-1.786938</td>\n",
       "      <td>0.783338</td>\n",
       "      <td>0.534108</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>-0.827383</td>\n",
       "      <td>-0.082152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580843555328</th>\n",
       "      <td>4.688177</td>\n",
       "      <td>-1.786770</td>\n",
       "      <td>0.787350</td>\n",
       "      <td>0.534640</td>\n",
       "      <td>-0.152990</td>\n",
       "      <td>-0.826976</td>\n",
       "      <td>-0.082863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580848555520</th>\n",
       "      <td>4.688028</td>\n",
       "      <td>-1.786598</td>\n",
       "      <td>0.791382</td>\n",
       "      <td>0.535178</td>\n",
       "      <td>-0.152945</td>\n",
       "      <td>-0.826562</td>\n",
       "      <td>-0.083605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580853555456</th>\n",
       "      <td>4.687878</td>\n",
       "      <td>-1.786421</td>\n",
       "      <td>0.795429</td>\n",
       "      <td>0.535715</td>\n",
       "      <td>-0.152884</td>\n",
       "      <td>-0.826146</td>\n",
       "      <td>-0.084391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580858555648</th>\n",
       "      <td>4.687727</td>\n",
       "      <td>-1.786240</td>\n",
       "      <td>0.799484</td>\n",
       "      <td>0.536244</td>\n",
       "      <td>-0.152821</td>\n",
       "      <td>-0.825731</td>\n",
       "      <td>-0.085213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      p_RS_R_x [m]   p_RS_R_y [m]   p_RS_R_z [m]   q_RS_w []  \\\n",
       "#timestamp                                                                     \n",
       "1403636580838555648       4.688319      -1.786938       0.783338    0.534108   \n",
       "1403636580843555328       4.688177      -1.786770       0.787350    0.534640   \n",
       "1403636580848555520       4.688028      -1.786598       0.791382    0.535178   \n",
       "1403636580853555456       4.687878      -1.786421       0.795429    0.535715   \n",
       "1403636580858555648       4.687727      -1.786240       0.799484    0.536244   \n",
       "\n",
       "                      q_RS_x []   q_RS_y []   q_RS_z []  \n",
       "#timestamp                                               \n",
       "1403636580838555648   -0.153029   -0.827383   -0.082152  \n",
       "1403636580843555328   -0.152990   -0.826976   -0.082863  \n",
       "1403636580848555520   -0.152945   -0.826562   -0.083605  \n",
       "1403636580853555456   -0.152884   -0.826146   -0.084391  \n",
       "1403636580858555648   -0.152821   -0.825731   -0.085213  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dataframe = pd.read_csv(GROUNDTRUTH_PATH, index_col=0)\n",
    "imu_dataframe = pd.read_csv(IMU_PATH, index_col=0)\n",
    "# print(gt_dataframe.head(3))\n",
    "# print('-----------------')\n",
    "# print(imu_dataframe.head(3))\n",
    "\n",
    "# Find the start and end time stamp of the ground truth, \n",
    "# and extract relevant imu data frame, gt and imu should be same length  \n",
    "groudtruth_data = gt_dataframe.iloc[:-1,0:7]\n",
    "start_time, end_time = groudtruth_data.index[0], groudtruth_data.index[-1]\n",
    "\n",
    "# Use groundtruth_data.iloc[0,:] as reference cordinate frame\n",
    "groudtruth_data_rela_pos = groudtruth_data.iloc[:,:]-groudtruth_data.iloc[0,:]\n",
    "print (start_time, end_time)\n",
    "print(groudtruth_data_rela_pos.shape)\n",
    "groudtruth_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 36596\n",
      "(36380, 6)\n",
      "(36381, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_RS_R_x [m]</th>\n",
       "      <th>p_RS_R_y [m]</th>\n",
       "      <th>p_RS_R_z [m]</th>\n",
       "      <th>q_RS_w []</th>\n",
       "      <th>q_RS_x []</th>\n",
       "      <th>q_RS_y []</th>\n",
       "      <th>q_RS_z []</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1403636580838555648</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580843555328</th>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.000711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580848555520</th>\n",
       "      <td>-0.000291</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>-0.001453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580853555456</th>\n",
       "      <td>-0.000441</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>-0.002239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580858555648</th>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.016146</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>-0.003061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580863555328</th>\n",
       "      <td>-0.000740</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>-0.003897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580868555520</th>\n",
       "      <td>-0.000884</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>-0.004738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580873555456</th>\n",
       "      <td>-0.001024</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>-0.005573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580878555648</th>\n",
       "      <td>-0.001161</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.032344</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.006401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580883555328</th>\n",
       "      <td>-0.001294</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.036374</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>-0.007219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580888555520</th>\n",
       "      <td>-0.001426</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.040396</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>-0.008018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580893555456</th>\n",
       "      <td>-0.001556</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.044411</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>-0.008791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580898555392</th>\n",
       "      <td>-0.001688</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.048421</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>-0.009525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580903555328</th>\n",
       "      <td>-0.001824</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.052427</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>-0.010210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580908555520</th>\n",
       "      <td>-0.001964</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>-0.010834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580913555456</th>\n",
       "      <td>-0.002111</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.060439</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>-0.011391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580918555392</th>\n",
       "      <td>-0.002264</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.064445</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>-0.011887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580923555584</th>\n",
       "      <td>-0.002423</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>0.068449</td>\n",
       "      <td>0.008980</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>-0.012339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580928555520</th>\n",
       "      <td>-0.002584</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.072449</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.007451</td>\n",
       "      <td>-0.012761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580933555712</th>\n",
       "      <td>-0.002746</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.076440</td>\n",
       "      <td>0.010167</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>-0.013163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580938555392</th>\n",
       "      <td>-0.002907</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>0.080417</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>-0.013554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580943555584</th>\n",
       "      <td>-0.003063</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.084375</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>-0.013932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580948555520</th>\n",
       "      <td>-0.003212</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.088314</td>\n",
       "      <td>0.011942</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>-0.014303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580953555456</th>\n",
       "      <td>-0.003353</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.092234</td>\n",
       "      <td>0.012521</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>-0.014673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580958555392</th>\n",
       "      <td>-0.003485</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.096133</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>-0.015049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580963555584</th>\n",
       "      <td>-0.003607</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.013625</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>-0.015441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580968555520</th>\n",
       "      <td>-0.003722</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.103864</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.010676</td>\n",
       "      <td>-0.015839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580973555456</th>\n",
       "      <td>-0.003832</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.107695</td>\n",
       "      <td>0.014688</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.011041</td>\n",
       "      <td>-0.016215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580978555392</th>\n",
       "      <td>-0.003938</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.111502</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>-0.016544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580983555584</th>\n",
       "      <td>-0.004041</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.115282</td>\n",
       "      <td>0.015782</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.011781</td>\n",
       "      <td>-0.016814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580988555520</th>\n",
       "      <td>-0.004144</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.119035</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>-0.017028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580993555456</th>\n",
       "      <td>-0.004248</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.122758</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.012533</td>\n",
       "      <td>-0.017198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      p_RS_R_x [m]   p_RS_R_y [m]   p_RS_R_z [m]   q_RS_w []  \\\n",
       "#timestamp                                                                     \n",
       "1403636580838555648       0.000000       0.000000       0.000000    0.000000   \n",
       "1403636580843555328      -0.000142       0.000168       0.004012    0.000532   \n",
       "1403636580848555520      -0.000291       0.000340       0.008044    0.001070   \n",
       "1403636580853555456      -0.000441       0.000517       0.012091    0.001607   \n",
       "1403636580858555648      -0.000592       0.000698       0.016146    0.002136   \n",
       "1403636580863555328      -0.000740       0.000879       0.020202    0.002660   \n",
       "1403636580868555520      -0.000884       0.001057       0.024256    0.003181   \n",
       "1403636580873555456      -0.001024       0.001229       0.028304    0.003696   \n",
       "1403636580878555648      -0.001161       0.001394       0.032344    0.004209   \n",
       "1403636580883555328      -0.001294       0.001548       0.036374    0.004720   \n",
       "1403636580888555520      -0.001426       0.001691       0.040396    0.005229   \n",
       "1403636580893555456      -0.001556       0.001822       0.044411    0.005738   \n",
       "1403636580898555392      -0.001688       0.001938       0.048421    0.006252   \n",
       "1403636580903555328      -0.001824       0.002040       0.052427    0.006775   \n",
       "1403636580908555520      -0.001964       0.002128       0.056433    0.007304   \n",
       "1403636580913555456      -0.002111       0.002203       0.060439    0.007848   \n",
       "1403636580918555392      -0.002264       0.002267       0.064445    0.008408   \n",
       "1403636580923555584      -0.002423       0.002322       0.068449    0.008980   \n",
       "1403636580928555520      -0.002584       0.002369       0.072449    0.009569   \n",
       "1403636580933555712      -0.002746       0.002407       0.076440    0.010167   \n",
       "1403636580938555392      -0.002907       0.002437       0.080417    0.010759   \n",
       "1403636580943555584      -0.003063       0.002461       0.084375    0.011350   \n",
       "1403636580948555520      -0.003212       0.002478       0.088314    0.011942   \n",
       "1403636580953555456      -0.003353       0.002490       0.092234    0.012521   \n",
       "1403636580958555392      -0.003485       0.002500       0.096133    0.013082   \n",
       "1403636580963555584      -0.003607       0.002509       0.100010    0.013625   \n",
       "1403636580968555520      -0.003722       0.002520       0.103864    0.014155   \n",
       "1403636580973555456      -0.003832       0.002531       0.107695    0.014688   \n",
       "1403636580978555392      -0.003938       0.002542       0.111502    0.015231   \n",
       "1403636580983555584      -0.004041       0.002549       0.115282    0.015782   \n",
       "1403636580988555520      -0.004144       0.002549       0.119035    0.016347   \n",
       "1403636580993555456      -0.004248       0.002542       0.122758    0.016923   \n",
       "\n",
       "                      q_RS_x []   q_RS_y []   q_RS_z []  \n",
       "#timestamp                                               \n",
       "1403636580838555648    0.000000    0.000000    0.000000  \n",
       "1403636580843555328    0.000039    0.000407   -0.000711  \n",
       "1403636580848555520    0.000084    0.000821   -0.001453  \n",
       "1403636580853555456    0.000145    0.001237   -0.002239  \n",
       "1403636580858555648    0.000208    0.001652   -0.003061  \n",
       "1403636580863555328    0.000261    0.002069   -0.003897  \n",
       "1403636580868555520    0.000304    0.002487   -0.004738  \n",
       "1403636580873555456    0.000349    0.002902   -0.005573  \n",
       "1403636580878555648    0.000402    0.003316   -0.006401  \n",
       "1403636580883555328    0.000463    0.003726   -0.007219  \n",
       "1403636580888555520    0.000533    0.004133   -0.008018  \n",
       "1403636580893555456    0.000602    0.004538   -0.008791  \n",
       "1403636580898555392    0.000662    0.004945   -0.009525  \n",
       "1403636580903555328    0.000717    0.005355   -0.010210  \n",
       "1403636580908555520    0.000763    0.005764   -0.010834  \n",
       "1403636580913555456    0.000801    0.006179   -0.011391  \n",
       "1403636580918555392    0.000835    0.006599   -0.011887  \n",
       "1403636580923555584    0.000872    0.007022   -0.012339  \n",
       "1403636580928555520    0.000924    0.007451   -0.012761  \n",
       "1403636580933555712    0.000988    0.007882   -0.013163  \n",
       "1403636580938555392    0.001061    0.008307   -0.013554  \n",
       "1403636580943555584    0.001151    0.008728   -0.013932  \n",
       "1403636580948555520    0.001263    0.009145   -0.014303  \n",
       "1403636580953555456    0.001404    0.009549   -0.014673  \n",
       "1403636580958555392    0.001571    0.009937   -0.015049  \n",
       "1403636580963555584    0.001764    0.010311   -0.015441  \n",
       "1403636580968555520    0.001970    0.010676   -0.015839  \n",
       "1403636580973555456    0.002177    0.011041   -0.016215  \n",
       "1403636580978555392    0.002371    0.011410   -0.016544  \n",
       "1403636580983555584    0.002547    0.011781   -0.016814  \n",
       "1403636580988555520    0.002722    0.012155   -0.017028  \n",
       "1403636580993555456    0.002896    0.012533   -0.017198  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***** Handling imu data ***************\n",
    "# Set threshold to 2 000 000 ns, because the time interval between imu data is 5 000 000 ns\n",
    "threshold = 1000000\n",
    "[imu_start_idx, imu_end_idx] = [idx for (idx, timestamp) in enumerate(imu_dataframe.index)  \\\n",
    "    if (abs(timestamp-start_time)<threshold or abs(timestamp-end_time)<threshold)]\n",
    "print (imu_start_idx, imu_end_idx)\n",
    "# ---------imu data at t-1 is used to predicate gt at t\n",
    "# Groundtruth is for image 1-n, [first_image_middle_exp_time, last_image_middle_exp_time], that't why the number of \n",
    "# datapoints end with 1.\n",
    "imu_data = imu_dataframe[imu_start_idx:imu_end_idx].copy()\n",
    "\n",
    "if (groudtruth_data_rela_pos.shape[0]-1 != imu_data.shape[0]):\n",
    "    print(\"'Error: groundtruth and imu data don't have the same lenght'\")\n",
    "print (imu_data.shape)\n",
    "print (groudtruth_data_rela_pos.shape)\n",
    "\n",
    "groudtruth_data_rela_pos.iloc[:32,:]\n",
    "# imu_data.iloc[1:3]\n",
    "# imu_dataframe[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_RS_S_x [rad s^-1]</th>\n",
       "      <th>w_RS_S_y [rad s^-1]</th>\n",
       "      <th>w_RS_S_z [rad s^-1]</th>\n",
       "      <th>a_RS_S_x [m s^-2]</th>\n",
       "      <th>a_RS_S_y [m s^-2]</th>\n",
       "      <th>a_RS_S_z [m s^-2]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#timestamp [ns]</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1403636580838555392</th>\n",
       "      <td>-0.206647</td>\n",
       "      <td>0.325329</td>\n",
       "      <td>-0.040492</td>\n",
       "      <td>10.035472</td>\n",
       "      <td>-0.179789</td>\n",
       "      <td>-4.135137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580843555584</th>\n",
       "      <td>-0.215025</td>\n",
       "      <td>0.333009</td>\n",
       "      <td>-0.046775</td>\n",
       "      <td>9.749445</td>\n",
       "      <td>-0.024517</td>\n",
       "      <td>-4.077932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580848555520</th>\n",
       "      <td>-0.224798</td>\n",
       "      <td>0.336499</td>\n",
       "      <td>-0.057945</td>\n",
       "      <td>9.496106</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>-4.077932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403636580853555456</th>\n",
       "      <td>-0.237365</td>\n",
       "      <td>0.338594</td>\n",
       "      <td>-0.074700</td>\n",
       "      <td>9.210079</td>\n",
       "      <td>0.032689</td>\n",
       "      <td>-4.061588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     w_RS_S_x [rad s^-1]  w_RS_S_y [rad s^-1]  \\\n",
       "#timestamp [ns]                                                 \n",
       "1403636580838555392            -0.206647             0.325329   \n",
       "1403636580843555584            -0.215025             0.333009   \n",
       "1403636580848555520            -0.224798             0.336499   \n",
       "1403636580853555456            -0.237365             0.338594   \n",
       "\n",
       "                     w_RS_S_z [rad s^-1]  a_RS_S_x [m s^-2]  \\\n",
       "#timestamp [ns]                                               \n",
       "1403636580838555392            -0.040492          10.035472   \n",
       "1403636580843555584            -0.046775           9.749445   \n",
       "1403636580848555520            -0.057945           9.496106   \n",
       "1403636580853555456            -0.074700           9.210079   \n",
       "\n",
       "                     a_RS_S_y [m s^-2]  a_RS_S_z [m s^-2]  \n",
       "#timestamp [ns]                                            \n",
       "1403636580838555392          -0.179789          -4.135137  \n",
       "1403636580843555584          -0.024517          -4.077932  \n",
       "1403636580848555520           0.024517          -4.077932  \n",
       "1403636580853555456           0.032689          -4.061588  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imu_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All input data shape: (90, 400, 6)\n",
      "All output data shape: (90, 400, 7)\n"
     ]
    }
   ],
   "source": [
    "# After studying the dataset, ignore the last data from ground truth. Because the second last element \n",
    "# has the timestamp that happens to be the middle point of image 3661 and 3662 (opened in Office, column index)\n",
    "datasize = imu_data.shape[0] \n",
    "\n",
    "# IMU 200Hz,   1 min = 12000 samples\n",
    "series_length = 400 # This is 2 seconds\n",
    "total_num_sequence = datasize//series_length\n",
    "total_used_size = total_num_sequence*series_length\n",
    "\n",
    "X = np.array(imu_data[:total_used_size])\n",
    "y = np.empty(groudtruth_data_rela_pos[:total_used_size].shape)\n",
    "\n",
    "# The first row of groudtruth_data_rela_pos is already relative displacement to last position\n",
    "y[0,:] = groudtruth_data_rela_pos.iloc[0,:]\n",
    "for i in range(1, y.shape[0] ):\n",
    "    y[i,:3] = groudtruth_data_rela_pos.iloc[i,:3] - groudtruth_data_rela_pos.iloc[i-1,:3]\n",
    "    y[i,3:] = groudtruth_data_rela_pos.iloc[i,3:]\n",
    "\n",
    "# Handle extreme small number\n",
    "y[y<1e-6] = 1e-6\n",
    "    \n",
    "input_dim = X.shape[1]\n",
    "output_dim = y.shape[1] \n",
    "\n",
    "X = X.reshape(total_num_sequence, series_length, input_dim)\n",
    "y = y.reshape(total_num_sequence, series_length, output_dim)\n",
    "\n",
    "\n",
    "# print ('Training input data shape:',train_X.shape)\n",
    "# print ('Testing output data shape:',train_Y.shape)\n",
    "\n",
    "# Use relative position here\n",
    "# X = X[:, :, :3]\n",
    "# y = y[:, :, :3]\n",
    "print ('All input data shape:', X.shape)\n",
    "print ('All output data shape:', y.shape)\n",
    "\n",
    "train_val_split_ratio = 0.8\n",
    "train_num_sequence = round( total_num_sequence*train_val_split_ratio)\n",
    "\n",
    "train_X = np.copy(X[:train_num_sequence,...])\n",
    "train_Y = np.copy(y[:train_num_sequence,...])\n",
    "\n",
    "# train_val_X = train_X.reshape(1,-1,input_dim)\n",
    "# train_val_Y = train_Y.reshape(1,-1,output_dim)\n",
    "\n",
    "test_X = np.copy(X[train_num_sequence:,...])\n",
    "test_Y = np.copy(y[train_num_sequence:,...])\n",
    "\n",
    "new_train_X = np.concatenate((train_X, train_Y), axis=2)\n",
    "new_train_Y = train_Y[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for indexi,i in enumerate(y):\n",
    "#     for indexj,j in enumerate(i):\n",
    "#         if (abs(j)<0.000001).any():\n",
    "#             print(j, indexi, indexj)\n",
    "#             print ('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(y[0,29,:])\n",
    "# test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "\n",
    "log_dir = '../tmp/logs/sensor_fusion_LSTM_rel_pos'\n",
    "training_steps = 1000\n",
    "drift_thredshold = 0.5\n",
    "batch_size = 4\n",
    "\n",
    "class RollingData(object):\n",
    "    ''' X should be of shape (num_of_batches, sequence_length, input_dim)\n",
    "        Shuffle the data!!! Small training batches!!!\n",
    "    '''\n",
    "    def __init__(self, X, Y,):\n",
    "        self.pointer = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self._num_of_samples = X.shape[0]\n",
    "#         self.train_len = X.shape[1]/2\n",
    "    \n",
    "    @property\n",
    "    def num_of_samples(self):\n",
    "        return self._num_of_samples\n",
    "\n",
    "    def next_batch(self):\n",
    "        # Generator random number in the range self._num_of_samples\n",
    "        index = np.random.choice(self._num_of_samples, 4, replace=False)\n",
    "        batch_X = self.X[index,:,:]\n",
    "        batch_Y = self.Y[index,:,:]        \n",
    "#         try:\n",
    "#             train_X = self.X[self.pointer, :self.train_len, :]\n",
    "#             train_Y = self.Y[self.pointer, :self.train_len, :]\n",
    "#             test_X  = self.X[self.pointer, self.train_len:, :]\n",
    "#             test_Y  = self.Y[self.pointer, self.train_len:, :]\n",
    "#             self.pointer += 1\n",
    "#         except IndexError:\n",
    "#             print (\"Index out of range. Index range:{} Query index{}\".format(self.num_of_batches, self.pointer))\n",
    "#             sys.exit(1)\n",
    "        return (batch_X, batch_Y)\n",
    "    \n",
    "#     def reset_pointer(self):\n",
    "#         self.pointer = 0\n",
    "\n",
    "class IMU_LSTM_Model(object):\n",
    "    def __init__(self, lstm_cell=128, batch_size=1):\n",
    "\n",
    "        self.lstm    = BasicLSTMCell(128)    \n",
    "#         self.hidden_state = tf.zeros([batch_size, 64], dtype=tf.float32)\n",
    "#         self.output_state = tf.zeros([batch_size, 64], dtype=tf.float32)\n",
    "        self.state = (tf.zeros([batch_size, 128], dtype=tf.float32),tf.zeros([batch_size, 128], dtype=tf.float32))\n",
    "        \n",
    "#         self.batch_size = batch_size\n",
    "        print('hello, good initialize')\n",
    "#         self.warm_up_loss = 0\n",
    "#         self.predict_loss = 0\n",
    "#         self.loss = self.warm_up_loss + 9*self.predict_loss        \n",
    "        \n",
    "    def forward(self, X, mode=0):\n",
    "#         with tf.variable_scope('fc1_before', reuse=True):\n",
    "#             # Activation function here is tricky\n",
    "# *** Change X \n",
    "        if mode ==0:\n",
    "            self.X = X\n",
    "        if mode ==1:\n",
    "            self.X = tf.concat([X[:,:6],self.output], axis=1 )\n",
    "        output1_before  = fully_connected(inputs=X, num_outputs=64, activation_fn= None)\n",
    "#         with tf.variable_scope('fc2_before', reuse=True):\n",
    "        output2_before  = fully_connected(inputs=output1_before, num_outputs=128, activation_fn= None)\n",
    "#         with tf.variable_scope('lstm64',reuse=True):\n",
    "        output_lstm, self.state = self.lstm(output2_before, self.state)\n",
    "#         with tf.variable_scope('fc2_after',reuse=True):\n",
    "        output_fc2 = fully_connected(inputs=output_lstm, num_outputs=128, activation_fn= None)\n",
    "#         with tf.variable_scope('fc3_after',reuse=True):\n",
    "        output_fc3 = fully_connected(inputs=output_fc2, num_outputs=64, activation_fn= None)\n",
    "#         with tf.variable_scope('fc4_after', reuse=True):\n",
    "        self.output = fully_connected(inputs=output_fc3, num_outputs=7, activation_fn= None)    #    \n",
    "        return self.output\n",
    "                \n",
    "    # This method should be used in warm-up phase and between batches.\n",
    "    def reset_state(self):\n",
    "#         assert mode in (0,1), \"mode should be either 0 or 1: {}\".format(mode) \n",
    "        # Use mode 0 @ batch swtiching\n",
    "\n",
    "        self.state = (tf.zeros([batch_size, 128], dtype=tf.float32),tf.zeros([batch_size, 128], dtype=tf.float32))\n",
    "#             self.hidden_state = tf.zeros([batch_size, 64])\n",
    "#             self.output_state = tf.zeros([batch_size, 64])\n",
    "#             print(self.output_state)\n",
    "        return self.state\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "from tensorflow.python.ops import init_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_one_step(x,previous_hidden_memory_tuple):\n",
    "    with tf.name_scope('fc1'):\n",
    "        with tf.variable_scope('fc1_var', reuse=True, dtype=float32,):\n",
    "            weights = tf.get_variable('weights', shape=[13,64],initializer=initializers.xavier_initializer())\n",
    "            bias = tf.get_variable('bias', shape=[64],initializer=init_ops.zeros_initializer())\n",
    "        fc1 = tf.matmul(x,weights)+bias\n",
    "    with tf.name_scope('fc2'):\n",
    "        with tf.variable_scope('fc2_var', reuse=True, dtype=float32,):\n",
    "            weights = tf.get_variable('weights', shape=[64,128], initializer=initializers.xavier_initializer())\n",
    "            bias = tf.get_variable('bias',  shape=[128],initializer=init_ops.zeros_initializer())\n",
    "        fc2 = tf.matmul(x,weights)+bias\n",
    "    with tf.name_scope('lstm128'):\n",
    "        with tf.variable_scope('lstm128_var', reuse=True, dtype=float32):\n",
    "            \n",
    "                \n",
    "# To be done!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def customized_loss(prediction, groundtruth, orientation_weight=0.10 ):\n",
    "    # Orientation_weight should be decimal between 0 and 1, default=0.01\n",
    "    pos_loss = tf.losses.mean_squared_error(prediction[:,:,:3],groundtruth[:,:,:3])\n",
    "    # Step 1, normalize angle \n",
    "    normalized_orientation = tf.divide(prediction, tf.norm(prediction[:,:,3:], axis=2, keep_dims=True))\n",
    "    ori_loss = tf.losses.mean_squared_error(normalized_orientation[:,:,3:], groundtruth[:,:,3:])\n",
    "    # Step 2, get loss\n",
    "    loss = 100*(pos_loss + orientation_weight*ori_loss)\n",
    "    return loss, pos_loss, ori_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_scope_1/var1:0 [ 1.5455488]\n",
      "var_scope_1/var1:0 [ 1.5455488]\n",
      "aha_1/Add:0 [ 3.09109759]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('aha'):\n",
    "    with tf.variable_scope('var_scope_1', reuse=True):\n",
    "        var1 = tf.get_variable(name='var1', shape=[1], dtype=tf.float32)\n",
    "        var2 = tf.get_variable(name='var1', shape=[1], dtype=tf.float32)\n",
    "    a = tf.add(var1, var2)\n",
    "# with tf.name_scope('fuck'):\n",
    "#     with tf.variable_scope('var_scope_1', reuse=True):\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(var1.name, sess.run(var1))\n",
    "    print(var2.name, sess.run(var2))\n",
    "    print(a.name, sess.run(a))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, good initialize\n",
      "LSTMStateTuple(c=128, h=128)\n",
      "200\n",
      "(4, 7)\n",
      "(4, 200, 7)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size, series_length, 13), name='x-input')\n",
    "    Y = tf.placeholder(tf.float32, shape=(batch_size, series_length-1, 7), name='y-input')\n",
    "#     mode = tf.placeholder(tf.int32, name='mode')\n",
    "    \n",
    "imu_lstm_model = IMU_LSTM_Model(batch_size=batch_size)\n",
    "print(imu_lstm_model.lstm.state_size)\n",
    "\n",
    "\n",
    "train_warm_up_pred = []\n",
    "train_test_pred = []\n",
    "\n",
    "# Because the last input, we don't have groudtruth, so we can not say its pred is right or wrong\n",
    "for i in range(series_length-1):\n",
    "\n",
    "    if i<series_length//2:\n",
    "#         imu_lstm_model.set_state(mode=1, train_Y=Y[:,i,:])\n",
    "#         train_test_pred.append(imu_lstm_model.forward(X[:,i,:]))  \n",
    "#         _ = imu_lstm_model.set_state(mode=1, gt_train_Y = Y[:,series_length,:])\n",
    "        train_warm_up_pred.append(imu_lstm_model.forward(X[:,i,:],  mode=0)) \n",
    "    else:\n",
    "#         train_test_pred[:,i,:] = imu_lstm_model.forward(X[:,i,:])\n",
    "        train_test_pred.append(imu_lstm_model.forward(X[:,i,:], mode=1)) \n",
    "#     if train_num_sequence == series_length-2:\n",
    "#         zero_state = imu_lstm_model.reset_state()\n",
    "\n",
    "\n",
    "print (len(train_warm_up_pred))\n",
    "print (train_warm_up_pred[0].get_shape())\n",
    "\n",
    "train_warm_up_pred = tf.reshape(train_warm_up_pred,[series_length//2,batch_size,output_dim])\n",
    "train_warm_up_pred = tf.transpose(train_warm_up_pred, perm=[1,0,2])\n",
    "train_test_pred = tf.reshape(train_test_pred,[series_length//2-1,batch_size,output_dim])\n",
    "train_test_pred = tf.transpose(train_test_pred, perm=[1,0,2])\n",
    "\n",
    "print (train_warm_up_pred.get_shape())\n",
    "\n",
    "\n",
    "loss_warm_up, pos_loss_train, ori_loss_train = customized_loss(train_warm_up_pred,Y[:,:series_length//2,:])\n",
    "loss_prediction, pos_loss, ori_loss = customized_loss(train_test_pred, Y[:,series_length//2:,:])\n",
    "total_loss = 0.2*loss_warm_up + 0.8*loss_prediction\n",
    "\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 0 is: 11.657685279846191\n",
      "Prediction loss at step 1 is: 7.765103816986084\n",
      "Prediction loss at step 2 is: 8.129000663757324\n",
      "Prediction loss at step 3 is: 6.060035705566406\n",
      "Prediction loss at step 4 is: 4.514664649963379\n",
      "Prediction loss at step 5 is: 3.8734469413757324\n",
      "Prediction loss at step 6 is: 3.7855207920074463\n",
      "Prediction loss at step 7 is: 3.6128692626953125\n",
      "Prediction loss at step 8 is: 3.4527950286865234\n",
      "Prediction loss at step 9 is: 3.5951004028320312\n",
      "Prediction loss at step 10 is: 3.4765264987945557\n",
      "Prediction loss at step 11 is: 2.778843402862549\n",
      "Prediction loss at step 12 is: 2.7393362522125244\n",
      "Prediction loss at step 13 is: 2.735983371734619\n",
      "Prediction loss at step 14 is: 2.678969383239746\n",
      "Prediction loss at step 15 is: 2.7016830444335938\n",
      "Prediction loss at step 16 is: 2.7466416358947754\n",
      "Prediction loss at step 17 is: 2.5328562259674072\n",
      "Prediction loss at step 18 is: 2.2545382976531982\n",
      "Prediction loss at step 19 is: 2.152743101119995\n",
      "Prediction loss at step 20 is: 2.2238504886627197\n",
      "Prediction loss at step 21 is: 2.4215807914733887\n",
      "Prediction loss at step 22 is: 2.6185545921325684\n",
      "Prediction loss at step 23 is: 2.548989772796631\n",
      "Prediction loss at step 24 is: 2.3150551319122314\n",
      "Prediction loss at step 25 is: 2.277287721633911\n",
      "Prediction loss at step 26 is: 2.376610517501831\n",
      "Prediction loss at step 27 is: 2.279752016067505\n",
      "Prediction loss at step 28 is: 1.8007725477218628\n",
      "Prediction loss at step 29 is: 2.4414877891540527\n",
      "Prediction loss at step 30 is: 1.8544174432754517\n",
      "Prediction loss at step 31 is: 1.7266590595245361\n",
      "Prediction loss at step 32 is: 2.1347336769104004\n",
      "Prediction loss at step 33 is: 2.093190908432007\n",
      "Prediction loss at step 34 is: 2.3663439750671387\n",
      "Prediction loss at step 35 is: 2.1704275608062744\n",
      "Prediction loss at step 36 is: 2.0287258625030518\n",
      "Prediction loss at step 37 is: 2.241041898727417\n",
      "Prediction loss at step 38 is: 1.6226438283920288\n",
      "Prediction loss at step 39 is: 2.0243589878082275\n",
      "Prediction loss at step 40 is: 2.2296671867370605\n",
      "Prediction loss at step 41 is: 1.9831123352050781\n",
      "Prediction loss at step 42 is: 2.2002015113830566\n",
      "Prediction loss at step 43 is: 2.3969757556915283\n",
      "Prediction loss at step 44 is: 2.2183423042297363\n",
      "Prediction loss at step 45 is: 1.9915786981582642\n",
      "Prediction loss at step 46 is: 2.307922601699829\n",
      "Prediction loss at step 47 is: 2.1595633029937744\n",
      "Prediction loss at step 48 is: 2.116783857345581\n",
      "Prediction loss at step 49 is: 1.912304401397705\n",
      "Prediction loss at step 50 is: 1.908682107925415\n",
      "Prediction loss at step 51 is: 2.2895092964172363\n",
      "Prediction loss at step 52 is: 2.338625431060791\n",
      "Prediction loss at step 53 is: 2.335486888885498\n",
      "Prediction loss at step 54 is: 2.20219087600708\n",
      "Prediction loss at step 55 is: 1.5902621746063232\n",
      "Prediction loss at step 56 is: 2.116666555404663\n",
      "Prediction loss at step 57 is: 1.7524380683898926\n",
      "Prediction loss at step 58 is: 2.21592378616333\n",
      "Prediction loss at step 59 is: 2.3472933769226074\n",
      "Prediction loss at step 60 is: 1.8412933349609375\n",
      "Prediction loss at step 61 is: 2.2465832233428955\n",
      "Prediction loss at step 62 is: 2.2589070796966553\n",
      "Prediction loss at step 63 is: 1.685255527496338\n",
      "Prediction loss at step 64 is: 2.284672975540161\n",
      "Prediction loss at step 65 is: 1.875123143196106\n",
      "Prediction loss at step 66 is: 2.1844964027404785\n",
      "Prediction loss at step 67 is: 1.8497079610824585\n",
      "Prediction loss at step 68 is: 1.8801519870758057\n",
      "Prediction loss at step 69 is: 2.1250879764556885\n",
      "Prediction loss at step 70 is: 2.1576809883117676\n",
      "Prediction loss at step 71 is: 2.0253684520721436\n",
      "Prediction loss at step 72 is: 2.080623149871826\n",
      "Prediction loss at step 73 is: 1.9701106548309326\n",
      "Prediction loss at step 74 is: 2.170292854309082\n",
      "Prediction loss at step 75 is: 1.9131081104278564\n",
      "Prediction loss at step 76 is: 2.3192057609558105\n",
      "Prediction loss at step 77 is: 1.8106576204299927\n",
      "Prediction loss at step 78 is: 1.8582357168197632\n",
      "Prediction loss at step 79 is: 1.8357431888580322\n",
      "Prediction loss at step 80 is: 1.8046849966049194\n",
      "Prediction loss at step 81 is: 1.9139373302459717\n",
      "Prediction loss at step 82 is: 2.089373826980591\n",
      "Prediction loss at step 83 is: 2.0992045402526855\n",
      "Prediction loss at step 84 is: 2.1497814655303955\n",
      "Prediction loss at step 85 is: 1.7624841928482056\n",
      "Prediction loss at step 86 is: 2.167279005050659\n",
      "Prediction loss at step 87 is: 1.7340058088302612\n",
      "Prediction loss at step 88 is: 2.0051348209381104\n",
      "Prediction loss at step 89 is: 2.2581064701080322\n",
      "Prediction loss at step 90 is: 2.0518651008605957\n",
      "Prediction loss at step 91 is: 1.859248399734497\n",
      "Prediction loss at step 92 is: 1.9851608276367188\n",
      "Prediction loss at step 93 is: 2.167280435562134\n",
      "Prediction loss at step 94 is: 1.9883867502212524\n",
      "Prediction loss at step 95 is: 1.8039640188217163\n",
      "Prediction loss at step 96 is: 2.2077133655548096\n",
      "Prediction loss at step 97 is: 1.7147984504699707\n",
      "Prediction loss at step 98 is: 2.005239486694336\n",
      "Prediction loss at step 99 is: 1.8652347326278687\n",
      "Prediction loss at step 100 is: 2.2035751342773438\n",
      "Prediction loss at step 101 is: 1.7693207263946533\n",
      "Prediction loss at step 102 is: 2.2045841217041016\n",
      "Prediction loss at step 103 is: 2.060861825942993\n",
      "Prediction loss at step 104 is: 1.9559996128082275\n",
      "Prediction loss at step 105 is: 1.6161123514175415\n",
      "Prediction loss at step 106 is: 1.8996076583862305\n",
      "Prediction loss at step 107 is: 2.2459819316864014\n",
      "Prediction loss at step 108 is: 1.9315334558486938\n",
      "Prediction loss at step 109 is: 1.4516810178756714\n",
      "Prediction loss at step 110 is: 1.903510332107544\n",
      "Prediction loss at step 111 is: 1.8466581106185913\n",
      "Prediction loss at step 112 is: 1.564497709274292\n",
      "Prediction loss at step 113 is: 2.1514925956726074\n",
      "Prediction loss at step 114 is: 1.8516663312911987\n",
      "Prediction loss at step 115 is: 1.9869823455810547\n",
      "Prediction loss at step 116 is: 2.317356824874878\n",
      "Prediction loss at step 117 is: 1.853724718093872\n",
      "Prediction loss at step 118 is: 2.2970449924468994\n",
      "Prediction loss at step 119 is: 1.9740421772003174\n",
      "Prediction loss at step 120 is: 2.2087643146514893\n",
      "Prediction loss at step 121 is: 1.4518331289291382\n",
      "Prediction loss at step 122 is: 1.9653836488723755\n",
      "Prediction loss at step 123 is: 1.934023380279541\n",
      "Prediction loss at step 124 is: 1.926278829574585\n",
      "Prediction loss at step 125 is: 1.972031831741333\n",
      "Prediction loss at step 126 is: 1.529115080833435\n",
      "Prediction loss at step 127 is: 1.9454600811004639\n",
      "Prediction loss at step 128 is: 2.1677603721618652\n",
      "Prediction loss at step 129 is: 1.5731805562973022\n",
      "Prediction loss at step 130 is: 1.9449188709259033\n",
      "Prediction loss at step 131 is: 2.284722328186035\n",
      "Prediction loss at step 132 is: 1.9030706882476807\n",
      "Prediction loss at step 133 is: 2.2045583724975586\n",
      "Prediction loss at step 134 is: 1.9193023443222046\n",
      "Prediction loss at step 135 is: 1.6734620332717896\n",
      "Prediction loss at step 136 is: 2.1702933311462402\n",
      "Prediction loss at step 137 is: 2.0119805335998535\n",
      "Prediction loss at step 138 is: 2.1358067989349365\n",
      "Prediction loss at step 139 is: 1.9172217845916748\n",
      "Prediction loss at step 140 is: 1.9841094017028809\n",
      "Prediction loss at step 141 is: 2.161127805709839\n",
      "Prediction loss at step 142 is: 1.6953024864196777\n",
      "Prediction loss at step 143 is: 2.36679744720459\n",
      "Prediction loss at step 144 is: 1.9383296966552734\n",
      "Prediction loss at step 145 is: 2.1302895545959473\n",
      "Prediction loss at step 146 is: 1.6265219449996948\n",
      "Prediction loss at step 147 is: 1.8764855861663818\n",
      "Prediction loss at step 148 is: 1.9863029718399048\n",
      "Prediction loss at step 149 is: 1.5568411350250244\n",
      "Prediction loss at step 150 is: 2.0590829849243164\n",
      "Prediction loss at step 151 is: 1.786841869354248\n",
      "Prediction loss at step 152 is: 1.98269522190094\n",
      "Prediction loss at step 153 is: 1.5781391859054565\n",
      "Prediction loss at step 154 is: 2.0428218841552734\n",
      "Prediction loss at step 155 is: 2.02506160736084\n",
      "Prediction loss at step 156 is: 2.3406124114990234\n",
      "Prediction loss at step 157 is: 1.9693175554275513\n",
      "Prediction loss at step 158 is: 1.9969919919967651\n",
      "Prediction loss at step 159 is: 2.1200411319732666\n",
      "Prediction loss at step 160 is: 2.219398021697998\n",
      "Prediction loss at step 161 is: 1.589097023010254\n",
      "Prediction loss at step 162 is: 2.0448827743530273\n",
      "Prediction loss at step 163 is: 2.1716034412384033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 164 is: 2.137591600418091\n",
      "Prediction loss at step 165 is: 2.0832488536834717\n",
      "Prediction loss at step 166 is: 2.0965681076049805\n",
      "Prediction loss at step 167 is: 2.3630459308624268\n",
      "Prediction loss at step 168 is: 1.9968618154525757\n",
      "Prediction loss at step 169 is: 1.7756319046020508\n",
      "Prediction loss at step 170 is: 2.1222078800201416\n",
      "Prediction loss at step 171 is: 2.0475499629974365\n",
      "Prediction loss at step 172 is: 1.7491616010665894\n",
      "Prediction loss at step 173 is: 1.7413345575332642\n",
      "Prediction loss at step 174 is: 1.988588809967041\n",
      "Prediction loss at step 175 is: 2.244856834411621\n",
      "Prediction loss at step 176 is: 2.0974695682525635\n",
      "Prediction loss at step 177 is: 2.0104825496673584\n",
      "Prediction loss at step 178 is: 1.9529356956481934\n",
      "Prediction loss at step 179 is: 1.713773250579834\n",
      "Prediction loss at step 180 is: 2.2344374656677246\n",
      "Prediction loss at step 181 is: 1.3044564723968506\n",
      "Prediction loss at step 182 is: 1.7870724201202393\n",
      "Prediction loss at step 183 is: 1.885703444480896\n",
      "Prediction loss at step 184 is: 1.9679871797561646\n",
      "Prediction loss at step 185 is: 1.690837025642395\n",
      "Prediction loss at step 186 is: 2.150071859359741\n",
      "Prediction loss at step 187 is: 2.2160775661468506\n",
      "Prediction loss at step 188 is: 1.9143177270889282\n",
      "Prediction loss at step 189 is: 1.9282985925674438\n",
      "Prediction loss at step 190 is: 2.09942364692688\n",
      "Prediction loss at step 191 is: 1.821273922920227\n",
      "Prediction loss at step 192 is: 2.0730459690093994\n",
      "Prediction loss at step 193 is: 1.9612092971801758\n",
      "Prediction loss at step 194 is: 1.798811912536621\n",
      "Prediction loss at step 195 is: 2.181013584136963\n",
      "Prediction loss at step 196 is: 1.675948143005371\n",
      "Prediction loss at step 197 is: 2.2497706413269043\n",
      "Prediction loss at step 198 is: 1.9503663778305054\n",
      "Prediction loss at step 199 is: 1.838977575302124\n",
      "Prediction loss at step 200 is: 2.1432032585144043\n",
      "Prediction loss at step 201 is: 1.8501951694488525\n",
      "Prediction loss at step 202 is: 2.0291779041290283\n",
      "Prediction loss at step 203 is: 1.7245392799377441\n",
      "Prediction loss at step 204 is: 2.020488739013672\n",
      "Prediction loss at step 205 is: 1.9813271760940552\n",
      "Prediction loss at step 206 is: 1.874744176864624\n",
      "Prediction loss at step 207 is: 1.8656184673309326\n",
      "Prediction loss at step 208 is: 2.061981439590454\n",
      "Prediction loss at step 209 is: 2.2721593379974365\n",
      "Prediction loss at step 210 is: 1.9944747686386108\n",
      "Prediction loss at step 211 is: 1.7935090065002441\n",
      "Prediction loss at step 212 is: 1.950662612915039\n",
      "Prediction loss at step 213 is: 1.6962701082229614\n",
      "Prediction loss at step 214 is: 1.931464672088623\n",
      "Prediction loss at step 215 is: 1.9175702333450317\n",
      "Prediction loss at step 216 is: 1.5930099487304688\n",
      "Prediction loss at step 217 is: 1.8668789863586426\n",
      "Prediction loss at step 218 is: 1.6301181316375732\n",
      "Prediction loss at step 219 is: 1.9809626340866089\n",
      "Prediction loss at step 220 is: 2.121584415435791\n",
      "Prediction loss at step 221 is: 2.0725812911987305\n",
      "Prediction loss at step 222 is: 1.961632490158081\n",
      "Prediction loss at step 223 is: 2.1731820106506348\n",
      "Prediction loss at step 224 is: 1.8104534149169922\n",
      "Prediction loss at step 225 is: 1.7817206382751465\n",
      "Prediction loss at step 226 is: 2.0736756324768066\n",
      "Prediction loss at step 227 is: 1.3732030391693115\n",
      "Prediction loss at step 228 is: 1.8345681428909302\n",
      "Prediction loss at step 229 is: 2.1815617084503174\n",
      "Prediction loss at step 230 is: 1.977760672569275\n",
      "Prediction loss at step 231 is: 1.9353294372558594\n",
      "Prediction loss at step 232 is: 1.628007411956787\n",
      "Prediction loss at step 233 is: 2.011989116668701\n",
      "Prediction loss at step 234 is: 1.9927490949630737\n",
      "Prediction loss at step 235 is: 1.8899779319763184\n",
      "Prediction loss at step 236 is: 2.4163286685943604\n",
      "Prediction loss at step 237 is: 1.7013696432113647\n",
      "Prediction loss at step 238 is: 1.6518406867980957\n",
      "Prediction loss at step 239 is: 1.6447089910507202\n",
      "Prediction loss at step 240 is: 1.547553539276123\n",
      "Prediction loss at step 241 is: 1.8181992769241333\n",
      "Prediction loss at step 242 is: 1.5702933073043823\n",
      "Prediction loss at step 243 is: 1.681557297706604\n",
      "Prediction loss at step 244 is: 1.9344558715820312\n",
      "Prediction loss at step 245 is: 2.307889223098755\n",
      "Prediction loss at step 246 is: 1.8711671829223633\n",
      "Prediction loss at step 247 is: 1.4193843603134155\n",
      "Prediction loss at step 248 is: 2.1044063568115234\n",
      "Prediction loss at step 249 is: 2.0991616249084473\n",
      "Prediction loss at step 250 is: 1.7892006635665894\n",
      "Prediction loss at step 251 is: 1.8048629760742188\n",
      "Prediction loss at step 252 is: 2.0896897315979004\n",
      "Prediction loss at step 253 is: 1.6787495613098145\n",
      "Prediction loss at step 254 is: 2.25053071975708\n",
      "Prediction loss at step 255 is: 1.7017751932144165\n",
      "Prediction loss at step 256 is: 2.1447153091430664\n",
      "Prediction loss at step 257 is: 1.6311609745025635\n",
      "Prediction loss at step 258 is: 2.202502489089966\n",
      "Prediction loss at step 259 is: 1.7807046175003052\n",
      "Prediction loss at step 260 is: 2.0524184703826904\n",
      "Prediction loss at step 261 is: 1.8295460939407349\n",
      "Prediction loss at step 262 is: 1.6081455945968628\n",
      "Prediction loss at step 263 is: 1.4125053882598877\n",
      "Prediction loss at step 264 is: 2.1505846977233887\n",
      "Prediction loss at step 265 is: 1.524011492729187\n",
      "Prediction loss at step 266 is: 1.6866769790649414\n",
      "Prediction loss at step 267 is: 1.2366633415222168\n",
      "Prediction loss at step 268 is: 2.0387840270996094\n",
      "Prediction loss at step 269 is: 1.8733677864074707\n",
      "Prediction loss at step 270 is: 1.924277424812317\n",
      "Prediction loss at step 271 is: 1.8720781803131104\n",
      "Prediction loss at step 272 is: 1.9954092502593994\n",
      "Prediction loss at step 273 is: 1.734861969947815\n",
      "Prediction loss at step 274 is: 1.7050995826721191\n",
      "Prediction loss at step 275 is: 1.3008897304534912\n",
      "Prediction loss at step 276 is: 1.7972207069396973\n",
      "Prediction loss at step 277 is: 1.8637968301773071\n",
      "Prediction loss at step 278 is: 2.0345051288604736\n",
      "Prediction loss at step 279 is: 1.572867512702942\n",
      "Prediction loss at step 280 is: 1.612865686416626\n",
      "Prediction loss at step 281 is: 1.7613860368728638\n",
      "Prediction loss at step 282 is: 2.093644857406616\n",
      "Prediction loss at step 283 is: 1.7711188793182373\n",
      "Prediction loss at step 284 is: 2.021601676940918\n",
      "Prediction loss at step 285 is: 2.224034070968628\n",
      "Prediction loss at step 286 is: 1.6977171897888184\n",
      "Prediction loss at step 287 is: 1.3110315799713135\n",
      "Prediction loss at step 288 is: 1.4400615692138672\n",
      "Prediction loss at step 289 is: 1.8681542873382568\n",
      "Prediction loss at step 290 is: 1.5867564678192139\n",
      "Prediction loss at step 291 is: 1.4368393421173096\n",
      "Prediction loss at step 292 is: 1.6873488426208496\n",
      "Prediction loss at step 293 is: 2.207063674926758\n",
      "Prediction loss at step 294 is: 1.848573088645935\n",
      "Prediction loss at step 295 is: 1.7395967245101929\n",
      "Prediction loss at step 296 is: 1.9167778491973877\n",
      "Prediction loss at step 297 is: 1.764257788658142\n",
      "Prediction loss at step 298 is: 2.2704508304595947\n",
      "Prediction loss at step 299 is: 1.8343312740325928\n",
      "Prediction loss at step 300 is: 1.9939098358154297\n",
      "Prediction loss at step 301 is: 1.5455535650253296\n",
      "Prediction loss at step 302 is: 1.6752500534057617\n",
      "Prediction loss at step 303 is: 1.7677266597747803\n",
      "Prediction loss at step 304 is: 2.0343167781829834\n",
      "Prediction loss at step 305 is: 2.184610605239868\n",
      "Prediction loss at step 306 is: 1.6678378582000732\n",
      "Prediction loss at step 307 is: 1.7791260480880737\n",
      "Prediction loss at step 308 is: 1.8516513109207153\n",
      "Prediction loss at step 309 is: 2.1233677864074707\n",
      "Prediction loss at step 310 is: 1.800595998764038\n",
      "Prediction loss at step 311 is: 1.5624359846115112\n",
      "Prediction loss at step 312 is: 1.8125978708267212\n",
      "Prediction loss at step 313 is: 2.0716452598571777\n",
      "Prediction loss at step 314 is: 1.8118895292282104\n",
      "Prediction loss at step 315 is: 1.8358240127563477\n",
      "Prediction loss at step 316 is: 2.294830322265625\n",
      "Prediction loss at step 317 is: 2.1185665130615234\n",
      "Prediction loss at step 318 is: 2.141456365585327\n",
      "Prediction loss at step 319 is: 1.9159635305404663\n",
      "Prediction loss at step 320 is: 1.9485344886779785\n",
      "Prediction loss at step 321 is: 1.5670545101165771\n",
      "Prediction loss at step 322 is: 1.9859263896942139\n",
      "Prediction loss at step 323 is: 1.4549974203109741\n",
      "Prediction loss at step 324 is: 2.2860095500946045\n",
      "Prediction loss at step 325 is: 1.7709550857543945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 326 is: 1.5762590169906616\n",
      "Prediction loss at step 327 is: 1.5678297281265259\n",
      "Prediction loss at step 328 is: 1.855713129043579\n",
      "Prediction loss at step 329 is: 1.742191195487976\n",
      "Prediction loss at step 330 is: 1.7737157344818115\n",
      "Prediction loss at step 331 is: 1.266339898109436\n",
      "Prediction loss at step 332 is: 1.9280602931976318\n",
      "Prediction loss at step 333 is: 1.4582470655441284\n",
      "Prediction loss at step 334 is: 1.6939131021499634\n",
      "Prediction loss at step 335 is: 2.0568859577178955\n",
      "Prediction loss at step 336 is: 1.9742438793182373\n",
      "Prediction loss at step 337 is: 1.5466301441192627\n",
      "Prediction loss at step 338 is: 1.4629850387573242\n",
      "Prediction loss at step 339 is: 1.8289432525634766\n",
      "Prediction loss at step 340 is: 1.8328497409820557\n",
      "Prediction loss at step 341 is: 1.8019044399261475\n",
      "Prediction loss at step 342 is: 2.0353429317474365\n",
      "Prediction loss at step 343 is: 1.4699201583862305\n",
      "Prediction loss at step 344 is: 1.3642178773880005\n",
      "Prediction loss at step 345 is: 1.404779076576233\n",
      "Prediction loss at step 346 is: 1.7102783918380737\n",
      "Prediction loss at step 347 is: 1.7331687211990356\n",
      "Prediction loss at step 348 is: 2.019364833831787\n",
      "Prediction loss at step 349 is: 1.6949834823608398\n",
      "Prediction loss at step 350 is: 2.2052395343780518\n",
      "Prediction loss at step 351 is: 1.6668134927749634\n",
      "Prediction loss at step 352 is: 2.051215648651123\n",
      "Prediction loss at step 353 is: 1.704972743988037\n",
      "Prediction loss at step 354 is: 1.6035184860229492\n",
      "Prediction loss at step 355 is: 1.8461987972259521\n",
      "Prediction loss at step 356 is: 1.7257591485977173\n",
      "Prediction loss at step 357 is: 2.2874886989593506\n",
      "Prediction loss at step 358 is: 1.7404606342315674\n",
      "Prediction loss at step 359 is: 1.961417317390442\n",
      "Prediction loss at step 360 is: 1.9014067649841309\n",
      "Prediction loss at step 361 is: 2.088890314102173\n",
      "Prediction loss at step 362 is: 1.742816686630249\n",
      "Prediction loss at step 363 is: 1.3061332702636719\n",
      "Prediction loss at step 364 is: 1.9120121002197266\n",
      "Prediction loss at step 365 is: 1.2597877979278564\n",
      "Prediction loss at step 366 is: 1.4151575565338135\n",
      "Prediction loss at step 367 is: 1.5495561361312866\n",
      "Prediction loss at step 368 is: 2.1647605895996094\n",
      "Prediction loss at step 369 is: 2.0642685890197754\n",
      "Prediction loss at step 370 is: 1.8940223455429077\n",
      "Prediction loss at step 371 is: 1.4831513166427612\n",
      "Prediction loss at step 372 is: 1.560581088066101\n",
      "Prediction loss at step 373 is: 1.9665751457214355\n",
      "Prediction loss at step 374 is: 2.1441097259521484\n",
      "Prediction loss at step 375 is: 1.727142333984375\n",
      "Prediction loss at step 376 is: 1.8317633867263794\n",
      "Prediction loss at step 377 is: 2.1246111392974854\n",
      "Prediction loss at step 378 is: 2.1110172271728516\n",
      "Prediction loss at step 379 is: 2.0238101482391357\n",
      "Prediction loss at step 380 is: 2.2078874111175537\n",
      "Prediction loss at step 381 is: 1.6781843900680542\n",
      "Prediction loss at step 382 is: 2.038928270339966\n",
      "Prediction loss at step 383 is: 2.1644339561462402\n",
      "Prediction loss at step 384 is: 1.9640350341796875\n",
      "Prediction loss at step 385 is: 1.5947761535644531\n",
      "Prediction loss at step 386 is: 2.1135120391845703\n",
      "Prediction loss at step 387 is: 2.196000814437866\n",
      "Prediction loss at step 388 is: 1.8493767976760864\n",
      "Prediction loss at step 389 is: 1.6205075979232788\n",
      "Prediction loss at step 390 is: 1.8671025037765503\n",
      "Prediction loss at step 391 is: 2.0746819972991943\n",
      "Prediction loss at step 392 is: 1.5841890573501587\n",
      "Prediction loss at step 393 is: 1.5239800214767456\n",
      "Prediction loss at step 394 is: 1.83759605884552\n",
      "Prediction loss at step 395 is: 1.8259098529815674\n",
      "Prediction loss at step 396 is: 2.1607394218444824\n",
      "Prediction loss at step 397 is: 1.9756823778152466\n",
      "Prediction loss at step 398 is: 1.9766416549682617\n",
      "Prediction loss at step 399 is: 1.410841703414917\n",
      "Prediction loss at step 400 is: 1.6746196746826172\n",
      "Prediction loss at step 401 is: 1.847036600112915\n",
      "Prediction loss at step 402 is: 1.4052680730819702\n",
      "Prediction loss at step 403 is: 1.296682357788086\n",
      "Prediction loss at step 404 is: 1.6805084943771362\n",
      "Prediction loss at step 405 is: 2.352079153060913\n",
      "Prediction loss at step 406 is: 1.6382560729980469\n",
      "Prediction loss at step 407 is: 2.1398508548736572\n",
      "Prediction loss at step 408 is: 1.611806869506836\n",
      "Prediction loss at step 409 is: 1.7445839643478394\n",
      "Prediction loss at step 410 is: 2.2281227111816406\n",
      "Prediction loss at step 411 is: 1.9415011405944824\n",
      "Prediction loss at step 412 is: 1.795987606048584\n",
      "Prediction loss at step 413 is: 2.004415273666382\n",
      "Prediction loss at step 414 is: 1.392580509185791\n",
      "Prediction loss at step 415 is: 1.342716097831726\n",
      "Prediction loss at step 416 is: 1.440347671508789\n",
      "Prediction loss at step 417 is: 1.5646653175354004\n",
      "Prediction loss at step 418 is: 1.7477097511291504\n",
      "Prediction loss at step 419 is: 1.2814953327178955\n",
      "Prediction loss at step 420 is: 1.374310851097107\n",
      "Prediction loss at step 421 is: 2.187241315841675\n",
      "Prediction loss at step 422 is: 2.002225637435913\n",
      "Prediction loss at step 423 is: 2.0179097652435303\n",
      "Prediction loss at step 424 is: 1.7022559642791748\n",
      "Prediction loss at step 425 is: 1.9927412271499634\n",
      "Prediction loss at step 426 is: 2.1458189487457275\n",
      "Prediction loss at step 427 is: 1.9282283782958984\n",
      "Prediction loss at step 428 is: 2.170097589492798\n",
      "Prediction loss at step 429 is: 2.0420641899108887\n",
      "Prediction loss at step 430 is: 2.0930981636047363\n",
      "Prediction loss at step 431 is: 2.153947591781616\n",
      "Prediction loss at step 432 is: 1.8076728582382202\n",
      "Prediction loss at step 433 is: 2.154959201812744\n",
      "Prediction loss at step 434 is: 2.1108388900756836\n",
      "Prediction loss at step 435 is: 1.5774246454238892\n",
      "Prediction loss at step 436 is: 2.029111385345459\n",
      "Prediction loss at step 437 is: 2.2227253913879395\n",
      "Prediction loss at step 438 is: 1.5011037588119507\n",
      "Prediction loss at step 439 is: 1.9630060195922852\n",
      "Prediction loss at step 440 is: 2.048948287963867\n",
      "Prediction loss at step 441 is: 1.6305930614471436\n",
      "Prediction loss at step 442 is: 2.0856730937957764\n",
      "Prediction loss at step 443 is: 1.942730188369751\n",
      "Prediction loss at step 444 is: 1.6130385398864746\n",
      "Prediction loss at step 445 is: 2.2277965545654297\n",
      "Prediction loss at step 446 is: 1.713491439819336\n",
      "Prediction loss at step 447 is: 1.2347406148910522\n",
      "Prediction loss at step 448 is: 2.2495524883270264\n",
      "Prediction loss at step 449 is: 1.659378170967102\n",
      "Prediction loss at step 450 is: 1.8139067888259888\n",
      "Prediction loss at step 451 is: 1.6609182357788086\n",
      "Prediction loss at step 452 is: 1.8202509880065918\n",
      "Prediction loss at step 453 is: 1.8095234632492065\n",
      "Prediction loss at step 454 is: 1.7732089757919312\n",
      "Prediction loss at step 455 is: 1.2557834386825562\n",
      "Prediction loss at step 456 is: 2.1921234130859375\n",
      "Prediction loss at step 457 is: 1.7862366437911987\n",
      "Prediction loss at step 458 is: 1.7708951234817505\n",
      "Prediction loss at step 459 is: 1.9714914560317993\n",
      "Prediction loss at step 460 is: 1.6294076442718506\n",
      "Prediction loss at step 461 is: 1.8285348415374756\n",
      "Prediction loss at step 462 is: 1.9783272743225098\n",
      "Prediction loss at step 463 is: 2.0633480548858643\n",
      "Prediction loss at step 464 is: 2.0046470165252686\n",
      "Prediction loss at step 465 is: 2.2044482231140137\n",
      "Prediction loss at step 466 is: 2.2160394191741943\n",
      "Prediction loss at step 467 is: 1.7764976024627686\n",
      "Prediction loss at step 468 is: 2.1041641235351562\n",
      "Prediction loss at step 469 is: 1.907153844833374\n",
      "Prediction loss at step 470 is: 1.7517739534378052\n",
      "Prediction loss at step 471 is: 2.106572151184082\n",
      "Prediction loss at step 472 is: 1.6246602535247803\n",
      "Prediction loss at step 473 is: 2.0385310649871826\n",
      "Prediction loss at step 474 is: 1.3801182508468628\n",
      "Prediction loss at step 475 is: 1.8593087196350098\n",
      "Prediction loss at step 476 is: 2.093602418899536\n",
      "Prediction loss at step 477 is: 1.81621253490448\n",
      "Prediction loss at step 478 is: 2.292557716369629\n",
      "Prediction loss at step 479 is: 1.4449801445007324\n",
      "Prediction loss at step 480 is: 1.3866102695465088\n",
      "Prediction loss at step 481 is: 1.7401542663574219\n",
      "Prediction loss at step 482 is: 2.198939800262451\n",
      "Prediction loss at step 483 is: 2.0259811878204346\n",
      "Prediction loss at step 484 is: 1.9513146877288818\n",
      "Prediction loss at step 485 is: 1.8637971878051758\n",
      "Prediction loss at step 486 is: 1.8025507926940918\n",
      "Prediction loss at step 487 is: 1.52117919921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 488 is: 1.6307188272476196\n",
      "Prediction loss at step 489 is: 1.563658595085144\n",
      "Prediction loss at step 490 is: 1.8850194215774536\n",
      "Prediction loss at step 491 is: 1.8960856199264526\n",
      "Prediction loss at step 492 is: 1.657232403755188\n",
      "Prediction loss at step 493 is: 1.570949673652649\n",
      "Prediction loss at step 494 is: 1.314106822013855\n",
      "Prediction loss at step 495 is: 1.9751026630401611\n",
      "Prediction loss at step 496 is: 2.0263190269470215\n",
      "Prediction loss at step 497 is: 1.4716036319732666\n",
      "Prediction loss at step 498 is: 1.9038667678833008\n",
      "Prediction loss at step 499 is: 2.1471915245056152\n",
      "Prediction loss at step 500 is: 1.9924521446228027\n",
      "Prediction loss at step 501 is: 2.110089063644409\n",
      "Prediction loss at step 502 is: 1.576744556427002\n",
      "Prediction loss at step 503 is: 1.5584588050842285\n",
      "Prediction loss at step 504 is: 2.056501626968384\n",
      "Prediction loss at step 505 is: 1.5716357231140137\n",
      "Prediction loss at step 506 is: 1.5858089923858643\n",
      "Prediction loss at step 507 is: 1.2942856550216675\n",
      "Prediction loss at step 508 is: 1.8458960056304932\n",
      "Prediction loss at step 509 is: 1.6229010820388794\n",
      "Prediction loss at step 510 is: 1.882085919380188\n",
      "Prediction loss at step 511 is: 1.9142405986785889\n",
      "Prediction loss at step 512 is: 1.698830008506775\n",
      "Prediction loss at step 513 is: 1.9692974090576172\n",
      "Prediction loss at step 514 is: 1.277485728263855\n",
      "Prediction loss at step 515 is: 1.5784399509429932\n",
      "Prediction loss at step 516 is: 2.2072253227233887\n",
      "Prediction loss at step 517 is: 1.9619762897491455\n",
      "Prediction loss at step 518 is: 1.8226796388626099\n",
      "Prediction loss at step 519 is: 1.6220179796218872\n",
      "Prediction loss at step 520 is: 2.1187245845794678\n",
      "Prediction loss at step 521 is: 1.8242313861846924\n",
      "Prediction loss at step 522 is: 2.1761114597320557\n",
      "Prediction loss at step 523 is: 2.0610477924346924\n",
      "Prediction loss at step 524 is: 1.7673916816711426\n",
      "Prediction loss at step 525 is: 1.7550263404846191\n",
      "Prediction loss at step 526 is: 2.273508071899414\n",
      "Prediction loss at step 527 is: 2.174262046813965\n",
      "Prediction loss at step 528 is: 2.14945912361145\n",
      "Prediction loss at step 529 is: 2.0821268558502197\n",
      "Prediction loss at step 530 is: 1.628183364868164\n",
      "Prediction loss at step 531 is: 1.7558670043945312\n",
      "Prediction loss at step 532 is: 1.9158071279525757\n",
      "Prediction loss at step 533 is: 1.933830976486206\n",
      "Prediction loss at step 534 is: 2.2068097591400146\n",
      "Prediction loss at step 535 is: 2.078378200531006\n",
      "Prediction loss at step 536 is: 2.1955835819244385\n",
      "Prediction loss at step 537 is: 1.4122750759124756\n",
      "Prediction loss at step 538 is: 2.327545404434204\n",
      "Prediction loss at step 539 is: 1.7056094408035278\n",
      "Prediction loss at step 540 is: 2.29213285446167\n",
      "Prediction loss at step 541 is: 1.876396894454956\n",
      "Prediction loss at step 542 is: 1.977591633796692\n",
      "Prediction loss at step 543 is: 2.284492015838623\n",
      "Prediction loss at step 544 is: 2.2440080642700195\n",
      "Prediction loss at step 545 is: 1.9114270210266113\n",
      "Prediction loss at step 546 is: 1.5767329931259155\n",
      "Prediction loss at step 547 is: 1.9644339084625244\n",
      "Prediction loss at step 548 is: 2.0212273597717285\n",
      "Prediction loss at step 549 is: 1.8673731088638306\n",
      "Prediction loss at step 550 is: 1.9267573356628418\n",
      "Prediction loss at step 551 is: 1.3030086755752563\n",
      "Prediction loss at step 552 is: 1.810940146446228\n",
      "Prediction loss at step 553 is: 2.2132461071014404\n",
      "Prediction loss at step 554 is: 1.8578957319259644\n",
      "Prediction loss at step 555 is: 1.5845222473144531\n",
      "Prediction loss at step 556 is: 1.8425368070602417\n",
      "Prediction loss at step 557 is: 1.5189865827560425\n",
      "Prediction loss at step 558 is: 2.1890618801116943\n",
      "Prediction loss at step 559 is: 1.444671392440796\n",
      "Prediction loss at step 560 is: 1.3701491355895996\n",
      "Prediction loss at step 561 is: 1.7455711364746094\n",
      "Prediction loss at step 562 is: 2.199171781539917\n",
      "Prediction loss at step 563 is: 1.7407495975494385\n",
      "Prediction loss at step 564 is: 2.084141492843628\n",
      "Prediction loss at step 565 is: 2.270824909210205\n",
      "Prediction loss at step 566 is: 2.003650188446045\n",
      "Prediction loss at step 567 is: 1.4934009313583374\n",
      "Prediction loss at step 568 is: 1.5298808813095093\n",
      "Prediction loss at step 569 is: 1.306836724281311\n",
      "Prediction loss at step 570 is: 1.8974642753601074\n",
      "Prediction loss at step 571 is: 2.3374786376953125\n",
      "Prediction loss at step 572 is: 2.0798354148864746\n",
      "Prediction loss at step 573 is: 2.3077120780944824\n",
      "Prediction loss at step 574 is: 1.6126341819763184\n",
      "Prediction loss at step 575 is: 1.9929293394088745\n",
      "Prediction loss at step 576 is: 1.4705358743667603\n",
      "Prediction loss at step 577 is: 2.0990991592407227\n",
      "Prediction loss at step 578 is: 1.7721822261810303\n",
      "Prediction loss at step 579 is: 1.5163013935089111\n",
      "Prediction loss at step 580 is: 1.464646816253662\n",
      "Prediction loss at step 581 is: 2.0877556800842285\n",
      "Prediction loss at step 582 is: 2.128509759902954\n",
      "Prediction loss at step 583 is: 1.8028484582901\n",
      "Prediction loss at step 584 is: 1.4900363683700562\n",
      "Prediction loss at step 585 is: 1.935153841972351\n",
      "Prediction loss at step 586 is: 1.1119804382324219\n",
      "Prediction loss at step 587 is: 2.099452018737793\n",
      "Prediction loss at step 588 is: 2.2221879959106445\n",
      "Prediction loss at step 589 is: 1.9252647161483765\n",
      "Prediction loss at step 590 is: 1.4515104293823242\n",
      "Prediction loss at step 591 is: 1.5799002647399902\n",
      "Prediction loss at step 592 is: 1.6695842742919922\n",
      "Prediction loss at step 593 is: 2.234678268432617\n",
      "Prediction loss at step 594 is: 2.1943070888519287\n",
      "Prediction loss at step 595 is: 1.6320019960403442\n",
      "Prediction loss at step 596 is: 1.452277421951294\n",
      "Prediction loss at step 597 is: 1.2802793979644775\n",
      "Prediction loss at step 598 is: 1.8836323022842407\n",
      "Prediction loss at step 599 is: 1.410418152809143\n",
      "Prediction loss at step 600 is: 1.6563838720321655\n",
      "Prediction loss at step 601 is: 1.4397523403167725\n",
      "Prediction loss at step 602 is: 1.6386538743972778\n",
      "Prediction loss at step 603 is: 2.044248104095459\n",
      "Prediction loss at step 604 is: 1.6902759075164795\n",
      "Prediction loss at step 605 is: 1.9866174459457397\n",
      "Prediction loss at step 606 is: 1.7895857095718384\n",
      "Prediction loss at step 607 is: 2.0457475185394287\n",
      "Prediction loss at step 608 is: 2.2338616847991943\n",
      "Prediction loss at step 609 is: 1.9999626874923706\n",
      "Prediction loss at step 610 is: 2.2126047611236572\n",
      "Prediction loss at step 611 is: 1.757253885269165\n",
      "Prediction loss at step 612 is: 1.7072012424468994\n",
      "Prediction loss at step 613 is: 2.1286840438842773\n",
      "Prediction loss at step 614 is: 2.014768600463867\n",
      "Prediction loss at step 615 is: 1.8028678894042969\n",
      "Prediction loss at step 616 is: 1.374609351158142\n",
      "Prediction loss at step 617 is: 1.6806766986846924\n",
      "Prediction loss at step 618 is: 1.3029868602752686\n",
      "Prediction loss at step 619 is: 1.8368160724639893\n",
      "Prediction loss at step 620 is: 2.0853514671325684\n",
      "Prediction loss at step 621 is: 1.4164831638336182\n",
      "Prediction loss at step 622 is: 1.9885072708129883\n",
      "Prediction loss at step 623 is: 1.7632814645767212\n",
      "Prediction loss at step 624 is: 1.4943912029266357\n",
      "Prediction loss at step 625 is: 1.5977319478988647\n",
      "Prediction loss at step 626 is: 2.1179039478302\n",
      "Prediction loss at step 627 is: 1.6079949140548706\n",
      "Prediction loss at step 628 is: 2.0163464546203613\n",
      "Prediction loss at step 629 is: 1.985917568206787\n",
      "Prediction loss at step 630 is: 2.2039525508880615\n",
      "Prediction loss at step 631 is: 2.0667223930358887\n",
      "Prediction loss at step 632 is: 1.1936752796173096\n",
      "Prediction loss at step 633 is: 1.784507155418396\n",
      "Prediction loss at step 634 is: 2.074620246887207\n",
      "Prediction loss at step 635 is: 1.3743726015090942\n",
      "Prediction loss at step 636 is: 1.9910218715667725\n",
      "Prediction loss at step 637 is: 2.1824235916137695\n",
      "Prediction loss at step 638 is: 1.847373366355896\n",
      "Prediction loss at step 639 is: 2.2027814388275146\n",
      "Prediction loss at step 640 is: 1.6762263774871826\n",
      "Prediction loss at step 641 is: 1.9366737604141235\n",
      "Prediction loss at step 642 is: 2.147808790206909\n",
      "Prediction loss at step 643 is: 1.9825376272201538\n",
      "Prediction loss at step 644 is: 1.624287486076355\n",
      "Prediction loss at step 645 is: 1.808220386505127\n",
      "Prediction loss at step 646 is: 1.639891266822815\n",
      "Prediction loss at step 647 is: 2.4030239582061768\n",
      "Prediction loss at step 648 is: 2.0180535316467285\n",
      "Prediction loss at step 649 is: 2.2839303016662598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 650 is: 1.5363517999649048\n",
      "Prediction loss at step 651 is: 1.6937295198440552\n",
      "Prediction loss at step 652 is: 2.105013132095337\n",
      "Prediction loss at step 653 is: 1.2515219449996948\n",
      "Prediction loss at step 654 is: 0.9678918719291687\n",
      "Prediction loss at step 655 is: 2.3330321311950684\n",
      "Prediction loss at step 656 is: 1.9253572225570679\n",
      "Prediction loss at step 657 is: 1.9951436519622803\n",
      "Prediction loss at step 658 is: 2.1146352291107178\n",
      "Prediction loss at step 659 is: 1.9297844171524048\n",
      "Prediction loss at step 660 is: 1.7988739013671875\n",
      "Prediction loss at step 661 is: 1.4922915697097778\n",
      "Prediction loss at step 662 is: 2.120124340057373\n",
      "Prediction loss at step 663 is: 1.7841557264328003\n",
      "Prediction loss at step 664 is: 1.9775586128234863\n",
      "Prediction loss at step 665 is: 1.6030043363571167\n",
      "Prediction loss at step 666 is: 1.9930343627929688\n",
      "Prediction loss at step 667 is: 1.9738521575927734\n",
      "Prediction loss at step 668 is: 1.6519883871078491\n",
      "Prediction loss at step 669 is: 1.6267541646957397\n",
      "Prediction loss at step 670 is: 2.118654251098633\n",
      "Prediction loss at step 671 is: 1.491768479347229\n",
      "Prediction loss at step 672 is: 2.200117588043213\n",
      "Prediction loss at step 673 is: 1.3180854320526123\n",
      "Prediction loss at step 674 is: 1.4468238353729248\n",
      "Prediction loss at step 675 is: 2.1486361026763916\n",
      "Prediction loss at step 676 is: 1.4785346984863281\n",
      "Prediction loss at step 677 is: 2.027805805206299\n",
      "Prediction loss at step 678 is: 1.9194234609603882\n",
      "Prediction loss at step 679 is: 1.880849003791809\n",
      "Prediction loss at step 680 is: 1.6821017265319824\n",
      "Prediction loss at step 681 is: 1.437619924545288\n",
      "Prediction loss at step 682 is: 1.956294298171997\n",
      "Prediction loss at step 683 is: 1.935604453086853\n",
      "Prediction loss at step 684 is: 1.989494800567627\n",
      "Prediction loss at step 685 is: 1.8059296607971191\n",
      "Prediction loss at step 686 is: 2.002208948135376\n",
      "Prediction loss at step 687 is: 2.0798144340515137\n",
      "Prediction loss at step 688 is: 1.6220738887786865\n",
      "Prediction loss at step 689 is: 1.6187158823013306\n",
      "Prediction loss at step 690 is: 2.275618076324463\n",
      "Prediction loss at step 691 is: 1.3648124933242798\n",
      "Prediction loss at step 692 is: 2.1810688972473145\n",
      "Prediction loss at step 693 is: 1.1788877248764038\n",
      "Prediction loss at step 694 is: 1.8776804208755493\n",
      "Prediction loss at step 695 is: 1.5626556873321533\n",
      "Prediction loss at step 696 is: 1.552731990814209\n",
      "Prediction loss at step 697 is: 1.901955008506775\n",
      "Prediction loss at step 698 is: 1.5482826232910156\n",
      "Prediction loss at step 699 is: 1.77616286277771\n",
      "Prediction loss at step 700 is: 1.7404016256332397\n",
      "Prediction loss at step 701 is: 2.0240139961242676\n",
      "Prediction loss at step 702 is: 2.082242250442505\n",
      "Prediction loss at step 703 is: 1.8644107580184937\n",
      "Prediction loss at step 704 is: 2.2364871501922607\n",
      "Prediction loss at step 705 is: 1.5606635808944702\n",
      "Prediction loss at step 706 is: 1.7625133991241455\n",
      "Prediction loss at step 707 is: 1.8180334568023682\n",
      "Prediction loss at step 708 is: 1.904233694076538\n",
      "Prediction loss at step 709 is: 1.9347933530807495\n",
      "Prediction loss at step 710 is: 1.7035772800445557\n",
      "Prediction loss at step 711 is: 1.7208126783370972\n",
      "Prediction loss at step 712 is: 1.6203850507736206\n",
      "Prediction loss at step 713 is: 1.9284497499465942\n",
      "Prediction loss at step 714 is: 2.064774513244629\n",
      "Prediction loss at step 715 is: 2.1547679901123047\n",
      "Prediction loss at step 716 is: 1.7769138813018799\n",
      "Prediction loss at step 717 is: 1.8321927785873413\n",
      "Prediction loss at step 718 is: 1.736069679260254\n",
      "Prediction loss at step 719 is: 1.4977891445159912\n",
      "Prediction loss at step 720 is: 1.5179383754730225\n",
      "Prediction loss at step 721 is: 1.6243408918380737\n",
      "Prediction loss at step 722 is: 2.186786413192749\n",
      "Prediction loss at step 723 is: 1.5665550231933594\n",
      "Prediction loss at step 724 is: 1.6214404106140137\n",
      "Prediction loss at step 725 is: 2.152930974960327\n",
      "Prediction loss at step 726 is: 2.31170654296875\n",
      "Prediction loss at step 727 is: 1.568787693977356\n",
      "Prediction loss at step 728 is: 2.291428804397583\n",
      "Prediction loss at step 729 is: 1.5312095880508423\n",
      "Prediction loss at step 730 is: 1.2862108945846558\n",
      "Prediction loss at step 731 is: 1.5898847579956055\n",
      "Prediction loss at step 732 is: 1.46220862865448\n",
      "Prediction loss at step 733 is: 2.3688387870788574\n",
      "Prediction loss at step 734 is: 1.8250640630722046\n",
      "Prediction loss at step 735 is: 1.9302089214324951\n",
      "Prediction loss at step 736 is: 1.9178165197372437\n",
      "Prediction loss at step 737 is: 1.917983055114746\n",
      "Prediction loss at step 738 is: 1.7937917709350586\n",
      "Prediction loss at step 739 is: 1.490433931350708\n",
      "Prediction loss at step 740 is: 1.9194635152816772\n",
      "Prediction loss at step 741 is: 2.035097122192383\n",
      "Prediction loss at step 742 is: 1.949265956878662\n",
      "Prediction loss at step 743 is: 1.7146142721176147\n",
      "Prediction loss at step 744 is: 1.8245078325271606\n",
      "Prediction loss at step 745 is: 2.073437452316284\n",
      "Prediction loss at step 746 is: 2.1475729942321777\n",
      "Prediction loss at step 747 is: 1.9718568325042725\n",
      "Prediction loss at step 748 is: 2.036407470703125\n",
      "Prediction loss at step 749 is: 2.040252208709717\n",
      "Prediction loss at step 750 is: 2.1872124671936035\n",
      "Prediction loss at step 751 is: 1.317935585975647\n",
      "Prediction loss at step 752 is: 1.6634794473648071\n",
      "Prediction loss at step 753 is: 1.6470553874969482\n",
      "Prediction loss at step 754 is: 2.2687385082244873\n",
      "Prediction loss at step 755 is: 2.367523193359375\n",
      "Prediction loss at step 756 is: 2.05143141746521\n",
      "Prediction loss at step 757 is: 2.1127307415008545\n",
      "Prediction loss at step 758 is: 1.9960252046585083\n",
      "Prediction loss at step 759 is: 2.1198506355285645\n",
      "Prediction loss at step 760 is: 1.2257024049758911\n",
      "Prediction loss at step 761 is: 1.7916314601898193\n",
      "Prediction loss at step 762 is: 2.3096821308135986\n",
      "Prediction loss at step 763 is: 2.028202533721924\n",
      "Prediction loss at step 764 is: 1.9545482397079468\n",
      "Prediction loss at step 765 is: 1.9801435470581055\n",
      "Prediction loss at step 766 is: 1.6487840414047241\n",
      "Prediction loss at step 767 is: 2.2502293586730957\n",
      "Prediction loss at step 768 is: 1.5625771284103394\n",
      "Prediction loss at step 769 is: 1.9829216003417969\n",
      "Prediction loss at step 770 is: 2.0391807556152344\n",
      "Prediction loss at step 771 is: 2.204465866088867\n",
      "Prediction loss at step 772 is: 2.0817651748657227\n",
      "Prediction loss at step 773 is: 1.8524070978164673\n",
      "Prediction loss at step 774 is: 1.5090337991714478\n",
      "Prediction loss at step 775 is: 1.8812224864959717\n",
      "Prediction loss at step 776 is: 1.9255784749984741\n",
      "Prediction loss at step 777 is: 1.628090262413025\n",
      "Prediction loss at step 778 is: 1.9982984066009521\n",
      "Prediction loss at step 779 is: 1.8586533069610596\n",
      "Prediction loss at step 780 is: 2.077052593231201\n",
      "Prediction loss at step 781 is: 1.366414189338684\n",
      "Prediction loss at step 782 is: 1.4271129369735718\n",
      "Prediction loss at step 783 is: 2.1807608604431152\n",
      "Prediction loss at step 784 is: 1.6624202728271484\n",
      "Prediction loss at step 785 is: 1.3053070306777954\n",
      "Prediction loss at step 786 is: 1.726617693901062\n",
      "Prediction loss at step 787 is: 1.8129358291625977\n",
      "Prediction loss at step 788 is: 1.5526618957519531\n",
      "Prediction loss at step 789 is: 1.6536617279052734\n",
      "Prediction loss at step 790 is: 1.9382007122039795\n",
      "Prediction loss at step 791 is: 1.5838127136230469\n",
      "Prediction loss at step 792 is: 1.9652180671691895\n",
      "Prediction loss at step 793 is: 1.523025631904602\n",
      "Prediction loss at step 794 is: 1.8053078651428223\n",
      "Prediction loss at step 795 is: 2.0004870891571045\n",
      "Prediction loss at step 796 is: 1.7644871473312378\n",
      "Prediction loss at step 797 is: 1.7571823596954346\n",
      "Prediction loss at step 798 is: 1.7502638101577759\n",
      "Prediction loss at step 799 is: 1.3148305416107178\n",
      "Prediction loss at step 800 is: 2.1605615615844727\n",
      "Prediction loss at step 801 is: 2.131706476211548\n",
      "Prediction loss at step 802 is: 2.008331775665283\n",
      "Prediction loss at step 803 is: 2.181723117828369\n",
      "Prediction loss at step 804 is: 1.3116111755371094\n",
      "Prediction loss at step 805 is: 2.0660994052886963\n",
      "Prediction loss at step 806 is: 1.7529897689819336\n",
      "Prediction loss at step 807 is: 1.589760184288025\n",
      "Prediction loss at step 808 is: 1.4258893728256226\n",
      "Prediction loss at step 809 is: 1.5123026371002197\n",
      "Prediction loss at step 810 is: 1.9709206819534302\n",
      "Prediction loss at step 811 is: 2.0012264251708984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 812 is: 1.6663484573364258\n",
      "Prediction loss at step 813 is: 1.476686954498291\n",
      "Prediction loss at step 814 is: 1.8697103261947632\n",
      "Prediction loss at step 815 is: 1.7695773839950562\n",
      "Prediction loss at step 816 is: 1.7948752641677856\n",
      "Prediction loss at step 817 is: 1.7800003290176392\n",
      "Prediction loss at step 818 is: 1.7226626873016357\n",
      "Prediction loss at step 819 is: 1.497407078742981\n",
      "Prediction loss at step 820 is: 2.0597307682037354\n",
      "Prediction loss at step 821 is: 1.825076699256897\n",
      "Prediction loss at step 822 is: 1.6678718328475952\n",
      "Prediction loss at step 823 is: 1.6119619607925415\n",
      "Prediction loss at step 824 is: 1.844329595565796\n",
      "Prediction loss at step 825 is: 1.9135106801986694\n",
      "Prediction loss at step 826 is: 1.5268313884735107\n",
      "Prediction loss at step 827 is: 2.0775187015533447\n",
      "Prediction loss at step 828 is: 1.9613275527954102\n",
      "Prediction loss at step 829 is: 1.8376810550689697\n",
      "Prediction loss at step 830 is: 1.6991549730300903\n",
      "Prediction loss at step 831 is: 2.268141984939575\n",
      "Prediction loss at step 832 is: 0.8979427814483643\n",
      "Prediction loss at step 833 is: 1.5798885822296143\n",
      "Prediction loss at step 834 is: 1.5261578559875488\n",
      "Prediction loss at step 835 is: 1.6490435600280762\n",
      "Prediction loss at step 836 is: 2.1718051433563232\n",
      "Prediction loss at step 837 is: 2.0701699256896973\n",
      "Prediction loss at step 838 is: 2.2515361309051514\n",
      "Prediction loss at step 839 is: 1.7842199802398682\n",
      "Prediction loss at step 840 is: 2.1910717487335205\n",
      "Prediction loss at step 841 is: 2.019474744796753\n",
      "Prediction loss at step 842 is: 1.2928828001022339\n",
      "Prediction loss at step 843 is: 2.2266249656677246\n",
      "Prediction loss at step 844 is: 1.832639217376709\n",
      "Prediction loss at step 845 is: 1.8773247003555298\n",
      "Prediction loss at step 846 is: 1.6317805051803589\n",
      "Prediction loss at step 847 is: 1.7531729936599731\n",
      "Prediction loss at step 848 is: 1.435056209564209\n",
      "Prediction loss at step 849 is: 2.1835944652557373\n",
      "Prediction loss at step 850 is: 2.066958427429199\n",
      "Prediction loss at step 851 is: 1.8656197786331177\n",
      "Prediction loss at step 852 is: 1.3203452825546265\n",
      "Prediction loss at step 853 is: 2.148484706878662\n",
      "Prediction loss at step 854 is: 2.1743574142456055\n",
      "Prediction loss at step 855 is: 2.0182342529296875\n",
      "Prediction loss at step 856 is: 1.7074905633926392\n",
      "Prediction loss at step 857 is: 1.962548851966858\n",
      "Prediction loss at step 858 is: 1.8490252494812012\n",
      "Prediction loss at step 859 is: 1.9091612100601196\n",
      "Prediction loss at step 860 is: 1.84434974193573\n",
      "Prediction loss at step 861 is: 1.56305730342865\n",
      "Prediction loss at step 862 is: 1.684064269065857\n",
      "Prediction loss at step 863 is: 1.6845250129699707\n",
      "Prediction loss at step 864 is: 1.9160196781158447\n",
      "Prediction loss at step 865 is: 1.2958569526672363\n",
      "Prediction loss at step 866 is: 1.507096290588379\n",
      "Prediction loss at step 867 is: 1.5971187353134155\n",
      "Prediction loss at step 868 is: 1.5333667993545532\n",
      "Prediction loss at step 869 is: 1.567568302154541\n",
      "Prediction loss at step 870 is: 1.787314534187317\n",
      "Prediction loss at step 871 is: 0.9561775922775269\n",
      "Prediction loss at step 872 is: 1.7003376483917236\n",
      "Prediction loss at step 873 is: 2.1478095054626465\n",
      "Prediction loss at step 874 is: 1.9554803371429443\n",
      "Prediction loss at step 875 is: 1.9188119173049927\n",
      "Prediction loss at step 876 is: 1.7924809455871582\n",
      "Prediction loss at step 877 is: 1.7982336282730103\n",
      "Prediction loss at step 878 is: 1.6882145404815674\n",
      "Prediction loss at step 879 is: 2.018681764602661\n",
      "Prediction loss at step 880 is: 1.3026643991470337\n",
      "Prediction loss at step 881 is: 2.195197820663452\n",
      "Prediction loss at step 882 is: 2.164829969406128\n",
      "Prediction loss at step 883 is: 1.5778412818908691\n",
      "Prediction loss at step 884 is: 2.1476404666900635\n",
      "Prediction loss at step 885 is: 1.454594612121582\n",
      "Prediction loss at step 886 is: 1.3954575061798096\n",
      "Prediction loss at step 887 is: 2.0070784091949463\n",
      "Prediction loss at step 888 is: 1.7560224533081055\n",
      "Prediction loss at step 889 is: 1.655663013458252\n",
      "Prediction loss at step 890 is: 1.4041427373886108\n",
      "Prediction loss at step 891 is: 2.1000118255615234\n",
      "Prediction loss at step 892 is: 1.962763786315918\n",
      "Prediction loss at step 893 is: 2.2419795989990234\n",
      "Prediction loss at step 894 is: 2.099724292755127\n",
      "Prediction loss at step 895 is: 2.1304116249084473\n",
      "Prediction loss at step 896 is: 1.5567338466644287\n",
      "Prediction loss at step 897 is: 1.765877366065979\n",
      "Prediction loss at step 898 is: 1.4616700410842896\n",
      "Prediction loss at step 899 is: 1.9501395225524902\n",
      "Prediction loss at step 900 is: 1.9710155725479126\n",
      "Prediction loss at step 901 is: 1.725389838218689\n",
      "Prediction loss at step 902 is: 2.237229347229004\n",
      "Prediction loss at step 903 is: 1.2465935945510864\n",
      "Prediction loss at step 904 is: 2.0012617111206055\n",
      "Prediction loss at step 905 is: 1.5489188432693481\n",
      "Prediction loss at step 906 is: 1.574744701385498\n",
      "Prediction loss at step 907 is: 2.073746681213379\n",
      "Prediction loss at step 908 is: 1.5206849575042725\n",
      "Prediction loss at step 909 is: 1.7961952686309814\n",
      "Prediction loss at step 910 is: 1.9076529741287231\n",
      "Prediction loss at step 911 is: 1.5678770542144775\n",
      "Prediction loss at step 912 is: 1.9045155048370361\n",
      "Prediction loss at step 913 is: 1.7769436836242676\n",
      "Prediction loss at step 914 is: 1.4987354278564453\n",
      "Prediction loss at step 915 is: 1.6881929636001587\n",
      "Prediction loss at step 916 is: 1.440466046333313\n",
      "Prediction loss at step 917 is: 1.9173320531845093\n",
      "Prediction loss at step 918 is: 2.1231632232666016\n",
      "Prediction loss at step 919 is: 2.265198230743408\n",
      "Prediction loss at step 920 is: 2.1646440029144287\n",
      "Prediction loss at step 921 is: 1.542144775390625\n",
      "Prediction loss at step 922 is: 1.4213693141937256\n",
      "Prediction loss at step 923 is: 1.7377872467041016\n",
      "Prediction loss at step 924 is: 1.7513600587844849\n",
      "Prediction loss at step 925 is: 1.7252812385559082\n",
      "Prediction loss at step 926 is: 2.0326075553894043\n",
      "Prediction loss at step 927 is: 2.0154330730438232\n",
      "Prediction loss at step 928 is: 1.720670461654663\n",
      "Prediction loss at step 929 is: 1.8909467458724976\n",
      "Prediction loss at step 930 is: 2.102595329284668\n",
      "Prediction loss at step 931 is: 1.7054245471954346\n",
      "Prediction loss at step 932 is: 1.9813792705535889\n",
      "Prediction loss at step 933 is: 1.9128252267837524\n",
      "Prediction loss at step 934 is: 1.61443030834198\n",
      "Prediction loss at step 935 is: 1.5628373622894287\n",
      "Prediction loss at step 936 is: 1.708204984664917\n",
      "Prediction loss at step 937 is: 1.5472277402877808\n",
      "Prediction loss at step 938 is: 2.1539828777313232\n",
      "Prediction loss at step 939 is: 1.152269959449768\n",
      "Prediction loss at step 940 is: 2.155273914337158\n",
      "Prediction loss at step 941 is: 1.9569923877716064\n",
      "Prediction loss at step 942 is: 1.8297556638717651\n",
      "Prediction loss at step 943 is: 1.7065932750701904\n",
      "Prediction loss at step 944 is: 2.008376359939575\n",
      "Prediction loss at step 945 is: 2.2144289016723633\n",
      "Prediction loss at step 946 is: 1.9821922779083252\n",
      "Prediction loss at step 947 is: 1.9959460496902466\n",
      "Prediction loss at step 948 is: 2.1624555587768555\n",
      "Prediction loss at step 949 is: 2.048964500427246\n",
      "Prediction loss at step 950 is: 1.992967963218689\n",
      "Prediction loss at step 951 is: 2.336824417114258\n",
      "Prediction loss at step 952 is: 2.1845154762268066\n",
      "Prediction loss at step 953 is: 2.077899694442749\n",
      "Prediction loss at step 954 is: 1.2829530239105225\n",
      "Prediction loss at step 955 is: 1.567136526107788\n",
      "Prediction loss at step 956 is: 1.9021209478378296\n",
      "Prediction loss at step 957 is: 1.324938416481018\n",
      "Prediction loss at step 958 is: 1.8854764699935913\n",
      "Prediction loss at step 959 is: 1.833120346069336\n",
      "Prediction loss at step 960 is: 2.1593847274780273\n",
      "Prediction loss at step 961 is: 2.155168056488037\n",
      "Prediction loss at step 962 is: 2.1288070678710938\n",
      "Prediction loss at step 963 is: 1.8789020776748657\n",
      "Prediction loss at step 964 is: 1.9778542518615723\n",
      "Prediction loss at step 965 is: 2.1013994216918945\n",
      "Prediction loss at step 966 is: 1.705075740814209\n",
      "Prediction loss at step 967 is: 1.6723732948303223\n",
      "Prediction loss at step 968 is: 1.6696794033050537\n",
      "Prediction loss at step 969 is: 1.877233624458313\n",
      "Prediction loss at step 970 is: 1.6004053354263306\n",
      "Prediction loss at step 971 is: 1.800362229347229\n",
      "Prediction loss at step 972 is: 1.6180551052093506\n",
      "Prediction loss at step 973 is: 1.6422882080078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 974 is: 1.4194607734680176\n",
      "Prediction loss at step 975 is: 1.7505357265472412\n",
      "Prediction loss at step 976 is: 1.6037700176239014\n",
      "Prediction loss at step 977 is: 2.0813658237457275\n",
      "Prediction loss at step 978 is: 1.9206516742706299\n",
      "Prediction loss at step 979 is: 1.5618247985839844\n",
      "Prediction loss at step 980 is: 1.8567214012145996\n",
      "Prediction loss at step 981 is: 1.6384258270263672\n",
      "Prediction loss at step 982 is: 1.7586266994476318\n",
      "Prediction loss at step 983 is: 2.2777369022369385\n",
      "Prediction loss at step 984 is: 2.0868406295776367\n",
      "Prediction loss at step 985 is: 1.7971875667572021\n",
      "Prediction loss at step 986 is: 2.1167922019958496\n",
      "Prediction loss at step 987 is: 1.7244396209716797\n",
      "Prediction loss at step 988 is: 1.8880354166030884\n",
      "Prediction loss at step 989 is: 1.8224056959152222\n",
      "Prediction loss at step 990 is: 2.2750611305236816\n",
      "Prediction loss at step 991 is: 1.9667649269104004\n",
      "Prediction loss at step 992 is: 2.0833168029785156\n",
      "Prediction loss at step 993 is: 1.5153316259384155\n",
      "Prediction loss at step 994 is: 1.9745187759399414\n",
      "Prediction loss at step 995 is: 1.4523677825927734\n",
      "Prediction loss at step 996 is: 1.4865435361862183\n",
      "Prediction loss at step 997 is: 2.090641498565674\n",
      "Prediction loss at step 998 is: 2.354519844055176\n",
      "Prediction loss at step 999 is: 1.7602486610412598\n",
      "Prediction loss at step 1000 is: 1.8851912021636963\n",
      "Prediction loss at step 1001 is: 1.7325488328933716\n",
      "Prediction loss at step 1002 is: 1.4488089084625244\n",
      "Prediction loss at step 1003 is: 1.8121579885482788\n",
      "Prediction loss at step 1004 is: 2.041008472442627\n",
      "Prediction loss at step 1005 is: 1.647995948791504\n",
      "Prediction loss at step 1006 is: 1.542516827583313\n",
      "Prediction loss at step 1007 is: 1.9593747854232788\n",
      "Prediction loss at step 1008 is: 1.046055793762207\n",
      "Prediction loss at step 1009 is: 1.6199026107788086\n",
      "Prediction loss at step 1010 is: 1.8056005239486694\n",
      "Prediction loss at step 1011 is: 1.4708526134490967\n",
      "Prediction loss at step 1012 is: 2.1499946117401123\n",
      "Prediction loss at step 1013 is: 1.4449704885482788\n",
      "Prediction loss at step 1014 is: 1.9381808042526245\n",
      "Prediction loss at step 1015 is: 1.848158836364746\n",
      "Prediction loss at step 1016 is: 1.4357143640518188\n",
      "Prediction loss at step 1017 is: 2.259533405303955\n",
      "Prediction loss at step 1018 is: 1.572455883026123\n",
      "Prediction loss at step 1019 is: 1.7180407047271729\n",
      "Prediction loss at step 1020 is: 1.3822309970855713\n",
      "Prediction loss at step 1021 is: 1.8216487169265747\n",
      "Prediction loss at step 1022 is: 2.0131750106811523\n",
      "Prediction loss at step 1023 is: 1.7413746118545532\n",
      "Prediction loss at step 1024 is: 1.9298944473266602\n",
      "Prediction loss at step 1025 is: 1.4079577922821045\n",
      "Prediction loss at step 1026 is: 1.8132938146591187\n",
      "Prediction loss at step 1027 is: 1.6751352548599243\n",
      "Prediction loss at step 1028 is: 1.7748396396636963\n",
      "Prediction loss at step 1029 is: 1.4943938255310059\n",
      "Prediction loss at step 1030 is: 1.500683069229126\n",
      "Prediction loss at step 1031 is: 1.7418179512023926\n",
      "Prediction loss at step 1032 is: 1.289232850074768\n",
      "Prediction loss at step 1033 is: 1.6894129514694214\n",
      "Prediction loss at step 1034 is: 1.2002053260803223\n",
      "Prediction loss at step 1035 is: 1.8324594497680664\n",
      "Prediction loss at step 1036 is: 1.5591217279434204\n",
      "Prediction loss at step 1037 is: 2.001967430114746\n",
      "Prediction loss at step 1038 is: 1.9794937372207642\n",
      "Prediction loss at step 1039 is: 2.1008639335632324\n",
      "Prediction loss at step 1040 is: 1.9319508075714111\n",
      "Prediction loss at step 1041 is: 1.8484253883361816\n",
      "Prediction loss at step 1042 is: 1.4598764181137085\n",
      "Prediction loss at step 1043 is: 2.0792272090911865\n",
      "Prediction loss at step 1044 is: 1.7932156324386597\n",
      "Prediction loss at step 1045 is: 1.4193154573440552\n",
      "Prediction loss at step 1046 is: 1.6493754386901855\n",
      "Prediction loss at step 1047 is: 1.54658842086792\n",
      "Prediction loss at step 1048 is: 1.7457845211029053\n",
      "Prediction loss at step 1049 is: 1.8314576148986816\n",
      "Prediction loss at step 1050 is: 1.9879987239837646\n",
      "Prediction loss at step 1051 is: 1.7706984281539917\n",
      "Prediction loss at step 1052 is: 1.8582632541656494\n",
      "Prediction loss at step 1053 is: 1.9892243146896362\n",
      "Prediction loss at step 1054 is: 1.572157859802246\n",
      "Prediction loss at step 1055 is: 2.1839349269866943\n",
      "Prediction loss at step 1056 is: 2.154308795928955\n",
      "Prediction loss at step 1057 is: 1.6026437282562256\n",
      "Prediction loss at step 1058 is: 1.8400709629058838\n",
      "Prediction loss at step 1059 is: 1.7490942478179932\n",
      "Prediction loss at step 1060 is: 1.8963191509246826\n",
      "Prediction loss at step 1061 is: 1.7563167810440063\n",
      "Prediction loss at step 1062 is: 1.975265383720398\n",
      "Prediction loss at step 1063 is: 1.9492639303207397\n",
      "Prediction loss at step 1064 is: 1.910935878753662\n",
      "Prediction loss at step 1065 is: 2.1579930782318115\n",
      "Prediction loss at step 1066 is: 1.6827125549316406\n",
      "Prediction loss at step 1067 is: 1.5921053886413574\n",
      "Prediction loss at step 1068 is: 2.034227132797241\n",
      "Prediction loss at step 1069 is: 2.3443856239318848\n",
      "Prediction loss at step 1070 is: 1.8283851146697998\n",
      "Prediction loss at step 1071 is: 1.86057710647583\n",
      "Prediction loss at step 1072 is: 2.221548557281494\n",
      "Prediction loss at step 1073 is: 2.123915195465088\n",
      "Prediction loss at step 1074 is: 1.9742300510406494\n",
      "Prediction loss at step 1075 is: 1.503304123878479\n",
      "Prediction loss at step 1076 is: 2.092181921005249\n",
      "Prediction loss at step 1077 is: 1.513269305229187\n",
      "Prediction loss at step 1078 is: 1.6604582071304321\n",
      "Prediction loss at step 1079 is: 1.7971833944320679\n",
      "Prediction loss at step 1080 is: 1.4975924491882324\n",
      "Prediction loss at step 1081 is: 1.605892539024353\n",
      "Prediction loss at step 1082 is: 1.5792243480682373\n",
      "Prediction loss at step 1083 is: 1.8474678993225098\n",
      "Prediction loss at step 1084 is: 1.8312056064605713\n",
      "Prediction loss at step 1085 is: 1.9568737745285034\n",
      "Prediction loss at step 1086 is: 1.4354254007339478\n",
      "Prediction loss at step 1087 is: 1.5235515832901\n",
      "Prediction loss at step 1088 is: 1.4313501119613647\n",
      "Prediction loss at step 1089 is: 1.6732592582702637\n",
      "Prediction loss at step 1090 is: 2.00797176361084\n",
      "Prediction loss at step 1091 is: 1.882919430732727\n",
      "Prediction loss at step 1092 is: 1.528355360031128\n",
      "Prediction loss at step 1093 is: 1.6889231204986572\n",
      "Prediction loss at step 1094 is: 1.5310982465744019\n",
      "Prediction loss at step 1095 is: 2.031400203704834\n",
      "Prediction loss at step 1096 is: 1.6188079118728638\n",
      "Prediction loss at step 1097 is: 2.0285227298736572\n",
      "Prediction loss at step 1098 is: 1.3633536100387573\n",
      "Prediction loss at step 1099 is: 1.8686141967773438\n",
      "Prediction loss at step 1100 is: 1.849838137626648\n",
      "Prediction loss at step 1101 is: 1.327435851097107\n",
      "Prediction loss at step 1102 is: 1.7870670557022095\n",
      "Prediction loss at step 1103 is: 1.5360597372055054\n",
      "Prediction loss at step 1104 is: 1.9686788320541382\n",
      "Prediction loss at step 1105 is: 1.9187023639678955\n",
      "Prediction loss at step 1106 is: 1.940289855003357\n",
      "Prediction loss at step 1107 is: 1.5463433265686035\n",
      "Prediction loss at step 1108 is: 1.6215227842330933\n",
      "Prediction loss at step 1109 is: 2.0347506999969482\n",
      "Prediction loss at step 1110 is: 1.7590655088424683\n",
      "Prediction loss at step 1111 is: 1.990617275238037\n",
      "Prediction loss at step 1112 is: 1.7921030521392822\n",
      "Prediction loss at step 1113 is: 1.9736982583999634\n",
      "Prediction loss at step 1114 is: 1.958495020866394\n",
      "Prediction loss at step 1115 is: 1.658151626586914\n",
      "Prediction loss at step 1116 is: 2.2000701427459717\n",
      "Prediction loss at step 1117 is: 2.266693592071533\n",
      "Prediction loss at step 1118 is: 1.1927917003631592\n",
      "Prediction loss at step 1119 is: 2.011749267578125\n",
      "Prediction loss at step 1120 is: 1.6840778589248657\n",
      "Prediction loss at step 1121 is: 2.087165117263794\n",
      "Prediction loss at step 1122 is: 1.6882826089859009\n",
      "Prediction loss at step 1123 is: 1.8962554931640625\n",
      "Prediction loss at step 1124 is: 1.956970453262329\n",
      "Prediction loss at step 1125 is: 1.6442559957504272\n",
      "Prediction loss at step 1126 is: 2.1562864780426025\n",
      "Prediction loss at step 1127 is: 1.8260542154312134\n",
      "Prediction loss at step 1128 is: 1.7795771360397339\n",
      "Prediction loss at step 1129 is: 1.4672574996948242\n",
      "Prediction loss at step 1130 is: 1.7917706966400146\n",
      "Prediction loss at step 1131 is: 1.6867932081222534\n",
      "Prediction loss at step 1132 is: 2.046435594558716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 1133 is: 2.0736703872680664\n",
      "Prediction loss at step 1134 is: 1.8788470029830933\n",
      "Prediction loss at step 1135 is: 1.3788881301879883\n",
      "Prediction loss at step 1136 is: 1.7493510246276855\n",
      "Prediction loss at step 1137 is: 1.5406817197799683\n",
      "Prediction loss at step 1138 is: 2.1213393211364746\n",
      "Prediction loss at step 1139 is: 1.4612404108047485\n",
      "Prediction loss at step 1140 is: 1.5338976383209229\n",
      "Prediction loss at step 1141 is: 1.7457823753356934\n",
      "Prediction loss at step 1142 is: 2.0514028072357178\n",
      "Prediction loss at step 1143 is: 0.7777947187423706\n",
      "Prediction loss at step 1144 is: 2.2739694118499756\n",
      "Prediction loss at step 1145 is: 1.5097216367721558\n",
      "Prediction loss at step 1146 is: 1.9040008783340454\n",
      "Prediction loss at step 1147 is: 1.8677430152893066\n",
      "Prediction loss at step 1148 is: 2.0239861011505127\n",
      "Prediction loss at step 1149 is: 1.4050030708312988\n",
      "Prediction loss at step 1150 is: 2.147116184234619\n",
      "Prediction loss at step 1151 is: 1.8822859525680542\n",
      "Prediction loss at step 1152 is: 1.9862828254699707\n",
      "Prediction loss at step 1153 is: 1.4585844278335571\n",
      "Prediction loss at step 1154 is: 1.4343557357788086\n",
      "Prediction loss at step 1155 is: 1.5887210369110107\n",
      "Prediction loss at step 1156 is: 2.133714199066162\n",
      "Prediction loss at step 1157 is: 1.9125518798828125\n",
      "Prediction loss at step 1158 is: 2.2706708908081055\n",
      "Prediction loss at step 1159 is: 1.5919454097747803\n",
      "Prediction loss at step 1160 is: 1.8167108297348022\n",
      "Prediction loss at step 1161 is: 2.1954541206359863\n",
      "Prediction loss at step 1162 is: 1.2802393436431885\n",
      "Prediction loss at step 1163 is: 1.6552757024765015\n",
      "Prediction loss at step 1164 is: 1.6789052486419678\n",
      "Prediction loss at step 1165 is: 1.9924323558807373\n",
      "Prediction loss at step 1166 is: 2.239908218383789\n",
      "Prediction loss at step 1167 is: 2.1080949306488037\n",
      "Prediction loss at step 1168 is: 1.8648358583450317\n",
      "Prediction loss at step 1169 is: 1.6590665578842163\n",
      "Prediction loss at step 1170 is: 1.9941843748092651\n",
      "Prediction loss at step 1171 is: 1.9310541152954102\n",
      "Prediction loss at step 1172 is: 1.5739059448242188\n",
      "Prediction loss at step 1173 is: 1.732985496520996\n",
      "Prediction loss at step 1174 is: 2.1970043182373047\n",
      "Prediction loss at step 1175 is: 1.860115647315979\n",
      "Prediction loss at step 1176 is: 1.937171220779419\n",
      "Prediction loss at step 1177 is: 2.085394859313965\n",
      "Prediction loss at step 1178 is: 1.9032540321350098\n",
      "Prediction loss at step 1179 is: 1.9647433757781982\n",
      "Prediction loss at step 1180 is: 1.315786600112915\n",
      "Prediction loss at step 1181 is: 2.2075822353363037\n",
      "Prediction loss at step 1182 is: 1.8312219381332397\n",
      "Prediction loss at step 1183 is: 1.9677236080169678\n",
      "Prediction loss at step 1184 is: 1.9931204319000244\n",
      "Prediction loss at step 1185 is: 1.4348002672195435\n",
      "Prediction loss at step 1186 is: 1.8554322719573975\n",
      "Prediction loss at step 1187 is: 1.3511836528778076\n",
      "Prediction loss at step 1188 is: 1.6963677406311035\n",
      "Prediction loss at step 1189 is: 1.8663674592971802\n",
      "Prediction loss at step 1190 is: 2.0493087768554688\n",
      "Prediction loss at step 1191 is: 1.7517623901367188\n",
      "Prediction loss at step 1192 is: 1.9766439199447632\n",
      "Prediction loss at step 1193 is: 1.6061899662017822\n",
      "Prediction loss at step 1194 is: 1.7584919929504395\n",
      "Prediction loss at step 1195 is: 1.352765440940857\n",
      "Prediction loss at step 1196 is: 1.8104923963546753\n",
      "Prediction loss at step 1197 is: 2.1419291496276855\n",
      "Prediction loss at step 1198 is: 2.0462498664855957\n",
      "Prediction loss at step 1199 is: 1.5510286092758179\n",
      "Prediction loss at step 1200 is: 1.2333353757858276\n",
      "Prediction loss at step 1201 is: 1.9146538972854614\n",
      "Prediction loss at step 1202 is: 1.4588438272476196\n",
      "Prediction loss at step 1203 is: 1.9398661851882935\n",
      "Prediction loss at step 1204 is: 1.3543015718460083\n",
      "Prediction loss at step 1205 is: 2.2767553329467773\n",
      "Prediction loss at step 1206 is: 2.063619613647461\n",
      "Prediction loss at step 1207 is: 1.7029911279678345\n",
      "Prediction loss at step 1208 is: 2.2728750705718994\n",
      "Prediction loss at step 1209 is: 1.6102561950683594\n",
      "Prediction loss at step 1210 is: 2.0161454677581787\n",
      "Prediction loss at step 1211 is: 2.142312526702881\n",
      "Prediction loss at step 1212 is: 2.153085470199585\n",
      "Prediction loss at step 1213 is: 1.9676815271377563\n",
      "Prediction loss at step 1214 is: 2.1600160598754883\n",
      "Prediction loss at step 1215 is: 1.348143219947815\n",
      "Prediction loss at step 1216 is: 1.9377765655517578\n",
      "Prediction loss at step 1217 is: 1.63121497631073\n",
      "Prediction loss at step 1218 is: 1.7257634401321411\n",
      "Prediction loss at step 1219 is: 1.7524521350860596\n",
      "Prediction loss at step 1220 is: 1.693565011024475\n",
      "Prediction loss at step 1221 is: 1.6556527614593506\n",
      "Prediction loss at step 1222 is: 1.6264792680740356\n",
      "Prediction loss at step 1223 is: 1.9635295867919922\n",
      "Prediction loss at step 1224 is: 1.6290704011917114\n",
      "Prediction loss at step 1225 is: 1.7001997232437134\n",
      "Prediction loss at step 1226 is: 1.7230596542358398\n",
      "Prediction loss at step 1227 is: 1.659834623336792\n",
      "Prediction loss at step 1228 is: 1.5888092517852783\n",
      "Prediction loss at step 1229 is: 1.7954859733581543\n",
      "Prediction loss at step 1230 is: 1.9161826372146606\n",
      "Prediction loss at step 1231 is: 1.6721317768096924\n",
      "Prediction loss at step 1232 is: 1.7575446367263794\n",
      "Prediction loss at step 1233 is: 2.022351026535034\n",
      "Prediction loss at step 1234 is: 2.087247848510742\n",
      "Prediction loss at step 1235 is: 2.2072696685791016\n",
      "Prediction loss at step 1236 is: 1.6796860694885254\n",
      "Prediction loss at step 1237 is: 2.1937689781188965\n",
      "Prediction loss at step 1238 is: 2.1499011516571045\n",
      "Prediction loss at step 1239 is: 1.893782615661621\n",
      "Prediction loss at step 1240 is: 1.6798162460327148\n",
      "Prediction loss at step 1241 is: 1.8941508531570435\n",
      "Prediction loss at step 1242 is: 1.9026620388031006\n",
      "Prediction loss at step 1243 is: 1.4501774311065674\n",
      "Prediction loss at step 1244 is: 1.4077955484390259\n",
      "Prediction loss at step 1245 is: 2.004094123840332\n",
      "Prediction loss at step 1246 is: 1.530983805656433\n",
      "Prediction loss at step 1247 is: 1.7384657859802246\n",
      "Prediction loss at step 1248 is: 1.873142957687378\n",
      "Prediction loss at step 1249 is: 1.6806775331497192\n",
      "Prediction loss at step 1250 is: 1.9828776121139526\n",
      "Prediction loss at step 1251 is: 2.247995376586914\n",
      "Prediction loss at step 1252 is: 1.7438253164291382\n",
      "Prediction loss at step 1253 is: 1.27220618724823\n",
      "Prediction loss at step 1254 is: 1.9242573976516724\n",
      "Prediction loss at step 1255 is: 1.684849739074707\n",
      "Prediction loss at step 1256 is: 1.630246877670288\n",
      "Prediction loss at step 1257 is: 1.792911410331726\n",
      "Prediction loss at step 1258 is: 1.8598690032958984\n",
      "Prediction loss at step 1259 is: 1.9511659145355225\n",
      "Prediction loss at step 1260 is: 1.8630656003952026\n",
      "Prediction loss at step 1261 is: 2.116189479827881\n",
      "Prediction loss at step 1262 is: 1.9024018049240112\n",
      "Prediction loss at step 1263 is: 2.261209726333618\n",
      "Prediction loss at step 1264 is: 1.3392270803451538\n",
      "Prediction loss at step 1265 is: 1.7853643894195557\n",
      "Prediction loss at step 1266 is: 1.9366432428359985\n",
      "Prediction loss at step 1267 is: 2.0605568885803223\n",
      "Prediction loss at step 1268 is: 2.2829747200012207\n",
      "Prediction loss at step 1269 is: 1.9060853719711304\n",
      "Prediction loss at step 1270 is: 2.1586477756500244\n",
      "Prediction loss at step 1271 is: 2.2030110359191895\n",
      "Prediction loss at step 1272 is: 1.7074127197265625\n",
      "Prediction loss at step 1273 is: 2.1522161960601807\n",
      "Prediction loss at step 1274 is: 1.9968907833099365\n",
      "Prediction loss at step 1275 is: 1.893898844718933\n",
      "Prediction loss at step 1276 is: 2.1588830947875977\n",
      "Prediction loss at step 1277 is: 1.5407469272613525\n",
      "Prediction loss at step 1278 is: 1.676152229309082\n",
      "Prediction loss at step 1279 is: 1.7518199682235718\n",
      "Prediction loss at step 1280 is: 1.882121205329895\n",
      "Prediction loss at step 1281 is: 1.8038233518600464\n",
      "Prediction loss at step 1282 is: 1.9807024002075195\n",
      "Prediction loss at step 1283 is: 1.5985772609710693\n",
      "Prediction loss at step 1284 is: 2.177497625350952\n",
      "Prediction loss at step 1285 is: 1.9503759145736694\n",
      "Prediction loss at step 1286 is: 2.002591371536255\n",
      "Prediction loss at step 1287 is: 1.232718825340271\n",
      "Prediction loss at step 1288 is: 1.500583529472351\n",
      "Prediction loss at step 1289 is: 1.8172343969345093\n",
      "Prediction loss at step 1290 is: 2.036503314971924\n",
      "Prediction loss at step 1291 is: 1.7608764171600342\n",
      "Prediction loss at step 1292 is: 2.3990819454193115\n",
      "Prediction loss at step 1293 is: 1.9035696983337402\n",
      "Prediction loss at step 1294 is: 1.098663568496704\n",
      "Prediction loss at step 1295 is: 1.7168136835098267\n",
      "Prediction loss at step 1296 is: 1.8764740228652954\n",
      "Prediction loss at step 1297 is: 2.166036605834961\n",
      "Prediction loss at step 1298 is: 1.7692334651947021\n",
      "Prediction loss at step 1299 is: 1.1972419023513794\n",
      "Prediction loss at step 1300 is: 2.122490406036377\n",
      "Prediction loss at step 1301 is: 1.6898939609527588\n",
      "Prediction loss at step 1302 is: 2.0230846405029297\n",
      "Prediction loss at step 1303 is: 1.5486258268356323\n",
      "Prediction loss at step 1304 is: 2.152045249938965\n",
      "Prediction loss at step 1305 is: 2.023559331893921\n",
      "Prediction loss at step 1306 is: 1.3174986839294434\n",
      "Prediction loss at step 1307 is: 2.008838176727295\n",
      "Prediction loss at step 1308 is: 1.5249674320220947\n",
      "Prediction loss at step 1309 is: 1.985497236251831\n",
      "Prediction loss at step 1310 is: 1.6157718896865845\n",
      "Prediction loss at step 1311 is: 1.4500311613082886\n",
      "Prediction loss at step 1312 is: 2.021182060241699\n",
      "Prediction loss at step 1313 is: 2.2215330600738525\n",
      "Prediction loss at step 1314 is: 1.943741798400879\n",
      "Prediction loss at step 1315 is: 1.7421927452087402\n",
      "Prediction loss at step 1316 is: 2.065094232559204\n",
      "Prediction loss at step 1317 is: 1.7319772243499756\n",
      "Prediction loss at step 1318 is: 2.2276668548583984\n",
      "Prediction loss at step 1319 is: 1.9210072755813599\n",
      "Prediction loss at step 1320 is: 1.6069365739822388\n",
      "Prediction loss at step 1321 is: 1.762908697128296\n",
      "Prediction loss at step 1322 is: 1.9741489887237549\n",
      "Prediction loss at step 1323 is: 1.8435982465744019\n",
      "Prediction loss at step 1324 is: 1.795854091644287\n",
      "Prediction loss at step 1325 is: 2.052182912826538\n",
      "Prediction loss at step 1326 is: 1.5565495491027832\n",
      "Prediction loss at step 1327 is: 2.2009501457214355\n",
      "Prediction loss at step 1328 is: 1.8237286806106567\n",
      "Prediction loss at step 1329 is: 1.4285699129104614\n",
      "Prediction loss at step 1330 is: 1.9837250709533691\n",
      "Prediction loss at step 1331 is: 2.1904611587524414\n",
      "Prediction loss at step 1332 is: 1.7966642379760742\n",
      "Prediction loss at step 1333 is: 2.1112008094787598\n",
      "Prediction loss at step 1334 is: 1.6839897632598877\n",
      "Prediction loss at step 1335 is: 2.0757060050964355\n",
      "Prediction loss at step 1336 is: 2.1683859825134277\n",
      "Prediction loss at step 1337 is: 2.021637439727783\n",
      "Prediction loss at step 1338 is: 1.2146313190460205\n",
      "Prediction loss at step 1339 is: 1.9467986822128296\n",
      "Prediction loss at step 1340 is: 2.136469841003418\n",
      "Prediction loss at step 1341 is: 1.7357466220855713\n",
      "Prediction loss at step 1342 is: 1.8287854194641113\n",
      "Prediction loss at step 1343 is: 1.4512195587158203\n",
      "Prediction loss at step 1344 is: 1.7801284790039062\n",
      "Prediction loss at step 1345 is: 1.7175348997116089\n",
      "Prediction loss at step 1346 is: 1.7287142276763916\n",
      "Prediction loss at step 1347 is: 1.7814702987670898\n",
      "Prediction loss at step 1348 is: 1.5017831325531006\n",
      "Prediction loss at step 1349 is: 1.928178071975708\n",
      "Prediction loss at step 1350 is: 2.145136594772339\n",
      "Prediction loss at step 1351 is: 1.8073445558547974\n",
      "Prediction loss at step 1352 is: 2.042698383331299\n",
      "Prediction loss at step 1353 is: 1.9540233612060547\n",
      "Prediction loss at step 1354 is: 1.9157122373580933\n",
      "Prediction loss at step 1355 is: 1.7769163846969604\n",
      "Prediction loss at step 1356 is: 2.171431303024292\n",
      "Prediction loss at step 1357 is: 1.1263346672058105\n",
      "Prediction loss at step 1358 is: 2.0289340019226074\n",
      "Prediction loss at step 1359 is: 1.4729036092758179\n",
      "Prediction loss at step 1360 is: 2.079157829284668\n",
      "Prediction loss at step 1361 is: 1.133728265762329\n",
      "Prediction loss at step 1362 is: 1.8115607500076294\n",
      "Prediction loss at step 1363 is: 1.8389188051223755\n",
      "Prediction loss at step 1364 is: 2.063992738723755\n",
      "Prediction loss at step 1365 is: 1.5020086765289307\n",
      "Prediction loss at step 1366 is: 1.7346497774124146\n",
      "Prediction loss at step 1367 is: 1.7171882390975952\n",
      "Prediction loss at step 1368 is: 1.9649395942687988\n",
      "Prediction loss at step 1369 is: 1.615387201309204\n",
      "Prediction loss at step 1370 is: 1.3578319549560547\n",
      "Prediction loss at step 1371 is: 2.172363042831421\n",
      "Prediction loss at step 1372 is: 1.8302596807479858\n",
      "Prediction loss at step 1373 is: 1.9542760848999023\n",
      "Prediction loss at step 1374 is: 1.8450191020965576\n",
      "Prediction loss at step 1375 is: 1.6411535739898682\n",
      "Prediction loss at step 1376 is: 1.8822667598724365\n",
      "Prediction loss at step 1377 is: 1.7496827840805054\n",
      "Prediction loss at step 1378 is: 1.9452933073043823\n",
      "Prediction loss at step 1379 is: 1.1488630771636963\n",
      "Prediction loss at step 1380 is: 1.9069406986236572\n",
      "Prediction loss at step 1381 is: 1.9862171411514282\n",
      "Prediction loss at step 1382 is: 1.8600947856903076\n",
      "Prediction loss at step 1383 is: 2.1377177238464355\n",
      "Prediction loss at step 1384 is: 1.747450590133667\n",
      "Prediction loss at step 1385 is: 1.7018911838531494\n",
      "Prediction loss at step 1386 is: 1.8495855331420898\n",
      "Prediction loss at step 1387 is: 1.599416732788086\n",
      "Prediction loss at step 1388 is: 1.620772123336792\n",
      "Prediction loss at step 1389 is: 1.7547613382339478\n",
      "Prediction loss at step 1390 is: 1.5517432689666748\n",
      "Prediction loss at step 1391 is: 1.9375470876693726\n",
      "Prediction loss at step 1392 is: 1.8995705842971802\n",
      "Prediction loss at step 1393 is: 1.9171793460845947\n",
      "Prediction loss at step 1394 is: 1.5064527988433838\n",
      "Prediction loss at step 1395 is: 2.2901506423950195\n",
      "Prediction loss at step 1396 is: 1.9545127153396606\n",
      "Prediction loss at step 1397 is: 1.380949854850769\n",
      "Prediction loss at step 1398 is: 2.148608684539795\n",
      "Prediction loss at step 1399 is: 2.116603136062622\n",
      "Prediction loss at step 1400 is: 2.0508501529693604\n",
      "Prediction loss at step 1401 is: 1.5857657194137573\n",
      "Prediction loss at step 1402 is: 1.328507900238037\n",
      "Prediction loss at step 1403 is: 1.7902095317840576\n",
      "Prediction loss at step 1404 is: 1.7948510646820068\n",
      "Prediction loss at step 1405 is: 1.8555101156234741\n",
      "Prediction loss at step 1406 is: 1.6125272512435913\n",
      "Prediction loss at step 1407 is: 1.6542870998382568\n",
      "Prediction loss at step 1408 is: 1.8540786504745483\n",
      "Prediction loss at step 1409 is: 1.343454360961914\n",
      "Prediction loss at step 1410 is: 1.5168790817260742\n",
      "Prediction loss at step 1411 is: 2.282001256942749\n",
      "Prediction loss at step 1412 is: 1.5472604036331177\n",
      "Prediction loss at step 1413 is: 1.7828730344772339\n",
      "Prediction loss at step 1414 is: 1.6388376951217651\n",
      "Prediction loss at step 1415 is: 1.946804165840149\n",
      "Prediction loss at step 1416 is: 1.633808970451355\n",
      "Prediction loss at step 1417 is: 1.9357380867004395\n",
      "Prediction loss at step 1418 is: 2.2134792804718018\n",
      "Prediction loss at step 1419 is: 1.8115921020507812\n",
      "Prediction loss at step 1420 is: 1.3437249660491943\n",
      "Prediction loss at step 1421 is: 1.4934380054473877\n",
      "Prediction loss at step 1422 is: 2.1329503059387207\n",
      "Prediction loss at step 1423 is: 2.0863094329833984\n",
      "Prediction loss at step 1424 is: 1.746105432510376\n",
      "Prediction loss at step 1425 is: 1.4614547491073608\n",
      "Prediction loss at step 1426 is: 2.219282388687134\n",
      "Prediction loss at step 1427 is: 1.3403520584106445\n",
      "Prediction loss at step 1428 is: 1.5953036546707153\n",
      "Prediction loss at step 1429 is: 1.7752286195755005\n",
      "Prediction loss at step 1430 is: 1.5644190311431885\n",
      "Prediction loss at step 1431 is: 1.86135995388031\n",
      "Prediction loss at step 1432 is: 1.5942251682281494\n",
      "Prediction loss at step 1433 is: 2.265134334564209\n",
      "Prediction loss at step 1434 is: 1.640956997871399\n",
      "Prediction loss at step 1435 is: 1.4500842094421387\n",
      "Prediction loss at step 1436 is: 2.2435457706451416\n",
      "Prediction loss at step 1437 is: 1.6797525882720947\n",
      "Prediction loss at step 1438 is: 2.084468364715576\n",
      "Prediction loss at step 1439 is: 2.1344408988952637\n",
      "Prediction loss at step 1440 is: 1.6293026208877563\n",
      "Prediction loss at step 1441 is: 2.2635741233825684\n",
      "Prediction loss at step 1442 is: 1.665485143661499\n",
      "Prediction loss at step 1443 is: 1.9592878818511963\n",
      "Prediction loss at step 1444 is: 1.9229497909545898\n",
      "Prediction loss at step 1445 is: 1.7095482349395752\n",
      "Prediction loss at step 1446 is: 2.144399404525757\n",
      "Prediction loss at step 1447 is: 1.9364550113677979\n",
      "Prediction loss at step 1448 is: 1.83136785030365\n",
      "Prediction loss at step 1449 is: 1.4847750663757324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 1450 is: 1.6893324851989746\n",
      "Prediction loss at step 1451 is: 1.9326351881027222\n",
      "Prediction loss at step 1452 is: 2.1351444721221924\n",
      "Prediction loss at step 1453 is: 1.657026767730713\n",
      "Prediction loss at step 1454 is: 1.8514974117279053\n",
      "Prediction loss at step 1455 is: 1.9442476034164429\n",
      "Prediction loss at step 1456 is: 1.4318046569824219\n",
      "Prediction loss at step 1457 is: 1.285248875617981\n",
      "Prediction loss at step 1458 is: 2.2621445655822754\n",
      "Prediction loss at step 1459 is: 2.131547689437866\n",
      "Prediction loss at step 1460 is: 1.6343467235565186\n",
      "Prediction loss at step 1461 is: 1.9984725713729858\n",
      "Prediction loss at step 1462 is: 1.96645987033844\n",
      "Prediction loss at step 1463 is: 1.911525011062622\n",
      "Prediction loss at step 1464 is: 2.0295512676239014\n",
      "Prediction loss at step 1465 is: 2.010009765625\n",
      "Prediction loss at step 1466 is: 1.672326922416687\n",
      "Prediction loss at step 1467 is: 1.3913434743881226\n",
      "Prediction loss at step 1468 is: 1.375443935394287\n",
      "Prediction loss at step 1469 is: 1.2654846906661987\n",
      "Prediction loss at step 1470 is: 2.064488172531128\n",
      "Prediction loss at step 1471 is: 1.0802745819091797\n",
      "Prediction loss at step 1472 is: 1.7410222291946411\n",
      "Prediction loss at step 1473 is: 1.8072905540466309\n",
      "Prediction loss at step 1474 is: 1.6309814453125\n",
      "Prediction loss at step 1475 is: 1.5100511312484741\n",
      "Prediction loss at step 1476 is: 1.8594330549240112\n",
      "Prediction loss at step 1477 is: 1.7873268127441406\n",
      "Prediction loss at step 1478 is: 1.651377558708191\n",
      "Prediction loss at step 1479 is: 1.671593427658081\n",
      "Prediction loss at step 1480 is: 1.6221377849578857\n",
      "Prediction loss at step 1481 is: 1.707256555557251\n",
      "Prediction loss at step 1482 is: 2.140120267868042\n",
      "Prediction loss at step 1483 is: 1.8804868459701538\n",
      "Prediction loss at step 1484 is: 1.7191423177719116\n",
      "Prediction loss at step 1485 is: 1.3983275890350342\n",
      "Prediction loss at step 1486 is: 2.354304790496826\n",
      "Prediction loss at step 1487 is: 1.8810014724731445\n",
      "Prediction loss at step 1488 is: 2.099531888961792\n",
      "Prediction loss at step 1489 is: 2.0718765258789062\n",
      "Prediction loss at step 1490 is: 2.197507381439209\n",
      "Prediction loss at step 1491 is: 1.9143779277801514\n",
      "Prediction loss at step 1492 is: 2.0090227127075195\n",
      "Prediction loss at step 1493 is: 2.0376784801483154\n",
      "Prediction loss at step 1494 is: 2.0180656909942627\n",
      "Prediction loss at step 1495 is: 1.705674409866333\n",
      "Prediction loss at step 1496 is: 2.093377113342285\n",
      "Prediction loss at step 1497 is: 1.271396279335022\n",
      "Prediction loss at step 1498 is: 2.0948143005371094\n",
      "Prediction loss at step 1499 is: 1.5373573303222656\n",
      "Prediction loss at step 1500 is: 2.066906213760376\n",
      "Prediction loss at step 1501 is: 1.5008821487426758\n",
      "Prediction loss at step 1502 is: 1.538100004196167\n",
      "Prediction loss at step 1503 is: 1.8911666870117188\n",
      "Prediction loss at step 1504 is: 1.8732144832611084\n",
      "Prediction loss at step 1505 is: 1.3540257215499878\n",
      "Prediction loss at step 1506 is: 1.6212857961654663\n",
      "Prediction loss at step 1507 is: 2.0015130043029785\n",
      "Prediction loss at step 1508 is: 2.1412267684936523\n",
      "Prediction loss at step 1509 is: 1.5836840867996216\n",
      "Prediction loss at step 1510 is: 1.8476197719573975\n",
      "Prediction loss at step 1511 is: 1.5157521963119507\n",
      "Prediction loss at step 1512 is: 1.7922964096069336\n",
      "Prediction loss at step 1513 is: 1.4384180307388306\n",
      "Prediction loss at step 1514 is: 1.8065569400787354\n",
      "Prediction loss at step 1515 is: 1.6322956085205078\n",
      "Prediction loss at step 1516 is: 2.0855729579925537\n",
      "Prediction loss at step 1517 is: 1.98879873752594\n",
      "Prediction loss at step 1518 is: 2.1266677379608154\n",
      "Prediction loss at step 1519 is: 1.452860951423645\n",
      "Prediction loss at step 1520 is: 1.5315355062484741\n",
      "Prediction loss at step 1521 is: 1.8618509769439697\n",
      "Prediction loss at step 1522 is: 1.7703967094421387\n",
      "Prediction loss at step 1523 is: 2.147508144378662\n",
      "Prediction loss at step 1524 is: 1.733677864074707\n",
      "Prediction loss at step 1525 is: 1.753056287765503\n",
      "Prediction loss at step 1526 is: 1.8044517040252686\n",
      "Prediction loss at step 1527 is: 2.0450401306152344\n",
      "Prediction loss at step 1528 is: 1.8529199361801147\n",
      "Prediction loss at step 1529 is: 1.906326413154602\n",
      "Prediction loss at step 1530 is: 1.5788501501083374\n",
      "Prediction loss at step 1531 is: 1.9670032262802124\n",
      "Prediction loss at step 1532 is: 1.6678926944732666\n",
      "Prediction loss at step 1533 is: 1.8113590478897095\n",
      "Prediction loss at step 1534 is: 2.0874407291412354\n",
      "Prediction loss at step 1535 is: 1.384605884552002\n",
      "Prediction loss at step 1536 is: 1.5039445161819458\n",
      "Prediction loss at step 1537 is: 1.9134865999221802\n",
      "Prediction loss at step 1538 is: 0.9447019696235657\n",
      "Prediction loss at step 1539 is: 1.3349195718765259\n",
      "Prediction loss at step 1540 is: 1.5667393207550049\n",
      "Prediction loss at step 1541 is: 1.9292182922363281\n",
      "Prediction loss at step 1542 is: 1.6733421087265015\n",
      "Prediction loss at step 1543 is: 1.8686833381652832\n",
      "Prediction loss at step 1544 is: 1.7574200630187988\n",
      "Prediction loss at step 1545 is: 1.5693612098693848\n",
      "Prediction loss at step 1546 is: 1.7839165925979614\n",
      "Prediction loss at step 1547 is: 1.966416597366333\n",
      "Prediction loss at step 1548 is: 1.7599384784698486\n",
      "Prediction loss at step 1549 is: 2.2407145500183105\n",
      "Prediction loss at step 1550 is: 2.099612236022949\n",
      "Prediction loss at step 1551 is: 1.767258882522583\n",
      "Prediction loss at step 1552 is: 2.0911340713500977\n",
      "Prediction loss at step 1553 is: 1.634243130683899\n",
      "Prediction loss at step 1554 is: 1.4790624380111694\n",
      "Prediction loss at step 1555 is: 2.0646557807922363\n",
      "Prediction loss at step 1556 is: 1.8567062616348267\n",
      "Prediction loss at step 1557 is: 1.563908338546753\n",
      "Prediction loss at step 1558 is: 1.7479281425476074\n",
      "Prediction loss at step 1559 is: 1.6028668880462646\n",
      "Prediction loss at step 1560 is: 1.4138909578323364\n",
      "Prediction loss at step 1561 is: 2.1164731979370117\n",
      "Prediction loss at step 1562 is: 1.5164008140563965\n",
      "Prediction loss at step 1563 is: 2.162111282348633\n",
      "Prediction loss at step 1564 is: 1.3927626609802246\n",
      "Prediction loss at step 1565 is: 1.7359275817871094\n",
      "Prediction loss at step 1566 is: 1.4574204683303833\n",
      "Prediction loss at step 1567 is: 1.6360396146774292\n",
      "Prediction loss at step 1568 is: 2.1541647911071777\n",
      "Prediction loss at step 1569 is: 1.7808737754821777\n",
      "Prediction loss at step 1570 is: 2.2587332725524902\n",
      "Prediction loss at step 1571 is: 1.6496220827102661\n",
      "Prediction loss at step 1572 is: 1.868353009223938\n",
      "Prediction loss at step 1573 is: 1.8157786130905151\n",
      "Prediction loss at step 1574 is: 1.0350875854492188\n",
      "Prediction loss at step 1575 is: 1.8172708749771118\n",
      "Prediction loss at step 1576 is: 1.999752402305603\n",
      "Prediction loss at step 1577 is: 1.8107373714447021\n",
      "Prediction loss at step 1578 is: 2.124507427215576\n",
      "Prediction loss at step 1579 is: 2.0956568717956543\n",
      "Prediction loss at step 1580 is: 1.6131831407546997\n",
      "Prediction loss at step 1581 is: 2.074491500854492\n",
      "Prediction loss at step 1582 is: 2.002671718597412\n",
      "Prediction loss at step 1583 is: 2.0758533477783203\n",
      "Prediction loss at step 1584 is: 1.9600269794464111\n",
      "Prediction loss at step 1585 is: 1.7851349115371704\n",
      "Prediction loss at step 1586 is: 1.9396361112594604\n",
      "Prediction loss at step 1587 is: 1.6655571460723877\n",
      "Prediction loss at step 1588 is: 2.0118393898010254\n",
      "Prediction loss at step 1589 is: 1.6784589290618896\n",
      "Prediction loss at step 1590 is: 2.055039405822754\n",
      "Prediction loss at step 1591 is: 1.5022778511047363\n",
      "Prediction loss at step 1592 is: 1.5667587518692017\n",
      "Prediction loss at step 1593 is: 1.8588868379592896\n",
      "Prediction loss at step 1594 is: 1.8955479860305786\n",
      "Prediction loss at step 1595 is: 1.2037850618362427\n",
      "Prediction loss at step 1596 is: 1.313463568687439\n",
      "Prediction loss at step 1597 is: 1.5915578603744507\n",
      "Prediction loss at step 1598 is: 1.8124080896377563\n",
      "Prediction loss at step 1599 is: 1.5751888751983643\n",
      "Prediction loss at step 1600 is: 1.7842230796813965\n",
      "Prediction loss at step 1601 is: 1.9491562843322754\n",
      "Prediction loss at step 1602 is: 2.0635807514190674\n",
      "Prediction loss at step 1603 is: 1.9030669927597046\n",
      "Prediction loss at step 1604 is: 1.6703083515167236\n",
      "Prediction loss at step 1605 is: 2.10030198097229\n",
      "Prediction loss at step 1606 is: 2.0776965618133545\n",
      "Prediction loss at step 1607 is: 1.7141867876052856\n",
      "Prediction loss at step 1608 is: 1.7120572328567505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 1609 is: 2.086851119995117\n",
      "Prediction loss at step 1610 is: 1.9263414144515991\n",
      "Prediction loss at step 1611 is: 1.630035400390625\n",
      "Prediction loss at step 1612 is: 1.5451633930206299\n",
      "Prediction loss at step 1613 is: 2.007080316543579\n",
      "Prediction loss at step 1614 is: 1.7637418508529663\n",
      "Prediction loss at step 1615 is: 1.8230372667312622\n",
      "Prediction loss at step 1616 is: 1.9800188541412354\n",
      "Prediction loss at step 1617 is: 1.4127322435379028\n",
      "Prediction loss at step 1618 is: 1.8919339179992676\n",
      "Prediction loss at step 1619 is: 1.8093843460083008\n",
      "Prediction loss at step 1620 is: 1.839982509613037\n",
      "Prediction loss at step 1621 is: 1.9590383768081665\n",
      "Prediction loss at step 1622 is: 1.8544316291809082\n",
      "Prediction loss at step 1623 is: 1.2178760766983032\n",
      "Prediction loss at step 1624 is: 1.8021317720413208\n",
      "Prediction loss at step 1625 is: 1.972988486289978\n",
      "Prediction loss at step 1626 is: 2.061239719390869\n",
      "Prediction loss at step 1627 is: 1.6400599479675293\n",
      "Prediction loss at step 1628 is: 1.695494532585144\n",
      "Prediction loss at step 1629 is: 2.0686991214752197\n",
      "Prediction loss at step 1630 is: 1.9240103960037231\n",
      "Prediction loss at step 1631 is: 1.7180930376052856\n",
      "Prediction loss at step 1632 is: 1.9158344268798828\n",
      "Prediction loss at step 1633 is: 1.8434321880340576\n",
      "Prediction loss at step 1634 is: 2.0586440563201904\n",
      "Prediction loss at step 1635 is: 1.8477071523666382\n",
      "Prediction loss at step 1636 is: 1.5510858297348022\n",
      "Prediction loss at step 1637 is: 2.049363374710083\n",
      "Prediction loss at step 1638 is: 1.9959065914154053\n",
      "Prediction loss at step 1639 is: 2.0117270946502686\n",
      "Prediction loss at step 1640 is: 1.9156396389007568\n",
      "Prediction loss at step 1641 is: 1.756239891052246\n",
      "Prediction loss at step 1642 is: 2.0450570583343506\n",
      "Prediction loss at step 1643 is: 1.4424049854278564\n",
      "Prediction loss at step 1644 is: 2.19555401802063\n",
      "Prediction loss at step 1645 is: 1.5681928396224976\n",
      "Prediction loss at step 1646 is: 1.7345322370529175\n",
      "Prediction loss at step 1647 is: 1.7044273614883423\n",
      "Prediction loss at step 1648 is: 2.2363407611846924\n",
      "Prediction loss at step 1649 is: 1.784177541732788\n",
      "Prediction loss at step 1650 is: 1.1803603172302246\n",
      "Prediction loss at step 1651 is: 1.8883520364761353\n",
      "Prediction loss at step 1652 is: 1.5962997674942017\n",
      "Prediction loss at step 1653 is: 1.7685340642929077\n",
      "Prediction loss at step 1654 is: 1.6761053800582886\n",
      "Prediction loss at step 1655 is: 1.750584363937378\n",
      "Prediction loss at step 1656 is: 1.9514821767807007\n",
      "Prediction loss at step 1657 is: 1.5665030479431152\n",
      "Prediction loss at step 1658 is: 1.1074063777923584\n",
      "Prediction loss at step 1659 is: 2.0346028804779053\n",
      "Prediction loss at step 1660 is: 2.003912925720215\n",
      "Prediction loss at step 1661 is: 1.6666170358657837\n",
      "Prediction loss at step 1662 is: 2.1243040561676025\n",
      "Prediction loss at step 1663 is: 1.8191708326339722\n",
      "Prediction loss at step 1664 is: 1.8429821729660034\n",
      "Prediction loss at step 1665 is: 1.284815788269043\n",
      "Prediction loss at step 1666 is: 1.8450053930282593\n",
      "Prediction loss at step 1667 is: 1.8982526063919067\n",
      "Prediction loss at step 1668 is: 1.587595820426941\n",
      "Prediction loss at step 1669 is: 1.7840323448181152\n",
      "Prediction loss at step 1670 is: 1.947757601737976\n",
      "Prediction loss at step 1671 is: 1.6186211109161377\n",
      "Prediction loss at step 1672 is: 1.6537666320800781\n",
      "Prediction loss at step 1673 is: 1.9369423389434814\n",
      "Prediction loss at step 1674 is: 2.2335920333862305\n",
      "Prediction loss at step 1675 is: 1.2340350151062012\n",
      "Prediction loss at step 1676 is: 1.941137433052063\n",
      "Prediction loss at step 1677 is: 1.571175456047058\n",
      "Prediction loss at step 1678 is: 1.845590353012085\n",
      "Prediction loss at step 1679 is: 2.1643717288970947\n",
      "Prediction loss at step 1680 is: 1.474440336227417\n",
      "Prediction loss at step 1681 is: 2.155107021331787\n",
      "Prediction loss at step 1682 is: 2.145750045776367\n",
      "Prediction loss at step 1683 is: 1.3815205097198486\n",
      "Prediction loss at step 1684 is: 2.0032074451446533\n",
      "Prediction loss at step 1685 is: 1.977344274520874\n",
      "Prediction loss at step 1686 is: 1.5189363956451416\n",
      "Prediction loss at step 1687 is: 1.7767313718795776\n",
      "Prediction loss at step 1688 is: 1.7812352180480957\n",
      "Prediction loss at step 1689 is: 1.6734814643859863\n",
      "Prediction loss at step 1690 is: 2.0489063262939453\n",
      "Prediction loss at step 1691 is: 1.4109344482421875\n",
      "Prediction loss at step 1692 is: 1.414283037185669\n",
      "Prediction loss at step 1693 is: 1.3913216590881348\n",
      "Prediction loss at step 1694 is: 2.078421115875244\n",
      "Prediction loss at step 1695 is: 1.2473019361495972\n",
      "Prediction loss at step 1696 is: 2.0470800399780273\n",
      "Prediction loss at step 1697 is: 1.4694939851760864\n",
      "Prediction loss at step 1698 is: 2.165008068084717\n",
      "Prediction loss at step 1699 is: 1.1309987306594849\n",
      "Prediction loss at step 1700 is: 1.213112711906433\n",
      "Prediction loss at step 1701 is: 2.075472354888916\n",
      "Prediction loss at step 1702 is: 1.3586822748184204\n",
      "Prediction loss at step 1703 is: 2.0153398513793945\n",
      "Prediction loss at step 1704 is: 2.083017587661743\n",
      "Prediction loss at step 1705 is: 2.2403273582458496\n",
      "Prediction loss at step 1706 is: 2.080540418624878\n",
      "Prediction loss at step 1707 is: 1.534180760383606\n",
      "Prediction loss at step 1708 is: 1.7193419933319092\n",
      "Prediction loss at step 1709 is: 1.8562018871307373\n",
      "Prediction loss at step 1710 is: 1.3795421123504639\n",
      "Prediction loss at step 1711 is: 1.0639525651931763\n",
      "Prediction loss at step 1712 is: 1.3084138631820679\n",
      "Prediction loss at step 1713 is: 1.9474960565567017\n",
      "Prediction loss at step 1714 is: 1.9796351194381714\n",
      "Prediction loss at step 1715 is: 1.663069248199463\n",
      "Prediction loss at step 1716 is: 2.1364598274230957\n",
      "Prediction loss at step 1717 is: 1.4052492380142212\n",
      "Prediction loss at step 1718 is: 2.201364278793335\n",
      "Prediction loss at step 1719 is: 2.262728452682495\n",
      "Prediction loss at step 1720 is: 2.0716769695281982\n",
      "Prediction loss at step 1721 is: 1.8163200616836548\n",
      "Prediction loss at step 1722 is: 1.6192363500595093\n",
      "Prediction loss at step 1723 is: 2.2846767902374268\n",
      "Prediction loss at step 1724 is: 1.6681421995162964\n",
      "Prediction loss at step 1725 is: 1.6315113306045532\n",
      "Prediction loss at step 1726 is: 1.3866703510284424\n",
      "Prediction loss at step 1727 is: 1.9810477495193481\n",
      "Prediction loss at step 1728 is: 1.508939266204834\n",
      "Prediction loss at step 1729 is: 1.383108139038086\n",
      "Prediction loss at step 1730 is: 1.3444353342056274\n",
      "Prediction loss at step 1731 is: 1.905219316482544\n",
      "Prediction loss at step 1732 is: 1.962749719619751\n",
      "Prediction loss at step 1733 is: 1.8949863910675049\n",
      "Prediction loss at step 1734 is: 2.0272133350372314\n",
      "Prediction loss at step 1735 is: 1.9408870935440063\n",
      "Prediction loss at step 1736 is: 1.9572910070419312\n",
      "Prediction loss at step 1737 is: 2.3289074897766113\n",
      "Prediction loss at step 1738 is: 1.2814124822616577\n",
      "Prediction loss at step 1739 is: 1.0269904136657715\n",
      "Prediction loss at step 1740 is: 1.9508976936340332\n",
      "Prediction loss at step 1741 is: 1.7344034910202026\n",
      "Prediction loss at step 1742 is: 1.5017173290252686\n",
      "Prediction loss at step 1743 is: 1.6407731771469116\n",
      "Prediction loss at step 1744 is: 2.160510778427124\n",
      "Prediction loss at step 1745 is: 1.7754396200180054\n",
      "Prediction loss at step 1746 is: 2.2158854007720947\n",
      "Prediction loss at step 1747 is: 1.346701741218567\n",
      "Prediction loss at step 1748 is: 1.9148441553115845\n",
      "Prediction loss at step 1749 is: 1.496578335762024\n",
      "Prediction loss at step 1750 is: 2.0340089797973633\n",
      "Prediction loss at step 1751 is: 1.8022043704986572\n",
      "Prediction loss at step 1752 is: 1.9369099140167236\n",
      "Prediction loss at step 1753 is: 1.7052081823349\n",
      "Prediction loss at step 1754 is: 1.8030617237091064\n",
      "Prediction loss at step 1755 is: 1.4157884120941162\n",
      "Prediction loss at step 1756 is: 2.1680214405059814\n",
      "Prediction loss at step 1757 is: 1.8611769676208496\n",
      "Prediction loss at step 1758 is: 1.6944074630737305\n",
      "Prediction loss at step 1759 is: 1.6049928665161133\n",
      "Prediction loss at step 1760 is: 2.114738702774048\n",
      "Prediction loss at step 1761 is: 1.3837661743164062\n",
      "Prediction loss at step 1762 is: 1.867314100265503\n",
      "Prediction loss at step 1763 is: 1.7307182550430298\n",
      "Prediction loss at step 1764 is: 1.7196118831634521\n",
      "Prediction loss at step 1765 is: 1.4811288118362427\n",
      "Prediction loss at step 1766 is: 1.779771089553833\n",
      "Prediction loss at step 1767 is: 1.8088023662567139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 1768 is: 1.6011333465576172\n",
      "Prediction loss at step 1769 is: 1.7928613424301147\n",
      "Prediction loss at step 1770 is: 1.5998356342315674\n",
      "Prediction loss at step 1771 is: 1.810201644897461\n",
      "Prediction loss at step 1772 is: 1.1144567728042603\n",
      "Prediction loss at step 1773 is: 1.7456755638122559\n",
      "Prediction loss at step 1774 is: 2.1304211616516113\n",
      "Prediction loss at step 1775 is: 2.093444347381592\n",
      "Prediction loss at step 1776 is: 1.9111570119857788\n",
      "Prediction loss at step 1777 is: 2.1247003078460693\n",
      "Prediction loss at step 1778 is: 1.7070900201797485\n",
      "Prediction loss at step 1779 is: 1.912235140800476\n",
      "Prediction loss at step 1780 is: 2.119868278503418\n",
      "Prediction loss at step 1781 is: 1.7473835945129395\n",
      "Prediction loss at step 1782 is: 1.5388641357421875\n",
      "Prediction loss at step 1783 is: 1.9122354984283447\n",
      "Prediction loss at step 1784 is: 1.9936883449554443\n",
      "Prediction loss at step 1785 is: 2.114962339401245\n",
      "Prediction loss at step 1786 is: 1.5261855125427246\n",
      "Prediction loss at step 1787 is: 2.090421676635742\n",
      "Prediction loss at step 1788 is: 1.7377212047576904\n",
      "Prediction loss at step 1789 is: 1.980461597442627\n",
      "Prediction loss at step 1790 is: 1.7682783603668213\n",
      "Prediction loss at step 1791 is: 1.9742542505264282\n",
      "Prediction loss at step 1792 is: 1.850576639175415\n",
      "Prediction loss at step 1793 is: 1.9185417890548706\n",
      "Prediction loss at step 1794 is: 1.5791494846343994\n",
      "Prediction loss at step 1795 is: 1.4001507759094238\n",
      "Prediction loss at step 1796 is: 1.3892704248428345\n",
      "Prediction loss at step 1797 is: 1.5545424222946167\n",
      "Prediction loss at step 1798 is: 1.7521860599517822\n",
      "Prediction loss at step 1799 is: 1.2360055446624756\n",
      "Prediction loss at step 1800 is: 1.376796841621399\n",
      "Prediction loss at step 1801 is: 1.881668210029602\n",
      "Prediction loss at step 1802 is: 1.8618825674057007\n",
      "Prediction loss at step 1803 is: 2.373775005340576\n",
      "Prediction loss at step 1804 is: 1.7653220891952515\n",
      "Prediction loss at step 1805 is: 1.9515142440795898\n",
      "Prediction loss at step 1806 is: 2.112622022628784\n",
      "Prediction loss at step 1807 is: 1.5888299942016602\n",
      "Prediction loss at step 1808 is: 1.5607918500900269\n",
      "Prediction loss at step 1809 is: 2.056793689727783\n",
      "Prediction loss at step 1810 is: 1.0861200094223022\n",
      "Prediction loss at step 1811 is: 1.8816179037094116\n",
      "Prediction loss at step 1812 is: 2.2534945011138916\n",
      "Prediction loss at step 1813 is: 2.1405029296875\n",
      "Prediction loss at step 1814 is: 1.8479403257369995\n",
      "Prediction loss at step 1815 is: 1.6516509056091309\n",
      "Prediction loss at step 1816 is: 1.1324830055236816\n",
      "Prediction loss at step 1817 is: 1.7005985975265503\n",
      "Prediction loss at step 1818 is: 2.0145063400268555\n",
      "Prediction loss at step 1819 is: 2.1320269107818604\n",
      "Prediction loss at step 1820 is: 1.643440842628479\n",
      "Prediction loss at step 1821 is: 1.7544312477111816\n",
      "Prediction loss at step 1822 is: 1.4669032096862793\n",
      "Prediction loss at step 1823 is: 1.0912777185440063\n",
      "Prediction loss at step 1824 is: 1.358826994895935\n",
      "Prediction loss at step 1825 is: 1.3383827209472656\n",
      "Prediction loss at step 1826 is: 1.4301484823226929\n",
      "Prediction loss at step 1827 is: 1.930545449256897\n",
      "Prediction loss at step 1828 is: 1.8859747648239136\n",
      "Prediction loss at step 1829 is: 2.189415216445923\n",
      "Prediction loss at step 1830 is: 1.9894847869873047\n",
      "Prediction loss at step 1831 is: 0.9406217336654663\n",
      "Prediction loss at step 1832 is: 1.4605683088302612\n",
      "Prediction loss at step 1833 is: 0.9864601492881775\n",
      "Prediction loss at step 1834 is: 2.092076301574707\n",
      "Prediction loss at step 1835 is: 1.8005890846252441\n",
      "Prediction loss at step 1836 is: 1.7344646453857422\n",
      "Prediction loss at step 1837 is: 1.768747329711914\n",
      "Prediction loss at step 1838 is: 2.058212995529175\n",
      "Prediction loss at step 1839 is: 1.6135681867599487\n",
      "Prediction loss at step 1840 is: 1.779657006263733\n",
      "Prediction loss at step 1841 is: 1.164294958114624\n",
      "Prediction loss at step 1842 is: 1.5396358966827393\n",
      "Prediction loss at step 1843 is: 2.1870696544647217\n",
      "Prediction loss at step 1844 is: 1.8289035558700562\n",
      "Prediction loss at step 1845 is: 1.7519440650939941\n",
      "Prediction loss at step 1846 is: 1.742052674293518\n",
      "Prediction loss at step 1847 is: 1.9313637018203735\n",
      "Prediction loss at step 1848 is: 1.635274887084961\n",
      "Prediction loss at step 1849 is: 1.501325249671936\n",
      "Prediction loss at step 1850 is: 1.9513508081436157\n",
      "Prediction loss at step 1851 is: 2.1367437839508057\n",
      "Prediction loss at step 1852 is: 1.8950650691986084\n",
      "Prediction loss at step 1853 is: 2.2505199909210205\n",
      "Prediction loss at step 1854 is: 1.6153298616409302\n",
      "Prediction loss at step 1855 is: 2.3543989658355713\n",
      "Prediction loss at step 1856 is: 1.6012277603149414\n",
      "Prediction loss at step 1857 is: 2.0164549350738525\n",
      "Prediction loss at step 1858 is: 1.6936354637145996\n",
      "Prediction loss at step 1859 is: 2.0285301208496094\n",
      "Prediction loss at step 1860 is: 1.7069826126098633\n",
      "Prediction loss at step 1861 is: 2.1253774166107178\n",
      "Prediction loss at step 1862 is: 1.8860294818878174\n",
      "Prediction loss at step 1863 is: 1.8150551319122314\n",
      "Prediction loss at step 1864 is: 2.0952401161193848\n",
      "Prediction loss at step 1865 is: 1.838233470916748\n",
      "Prediction loss at step 1866 is: 2.025102376937866\n",
      "Prediction loss at step 1867 is: 1.641613245010376\n",
      "Prediction loss at step 1868 is: 1.589617133140564\n",
      "Prediction loss at step 1869 is: 2.084277868270874\n",
      "Prediction loss at step 1870 is: 2.1920461654663086\n",
      "Prediction loss at step 1871 is: 1.884474277496338\n",
      "Prediction loss at step 1872 is: 1.5703670978546143\n",
      "Prediction loss at step 1873 is: 2.1958563327789307\n",
      "Prediction loss at step 1874 is: 1.0652735233306885\n",
      "Prediction loss at step 1875 is: 1.8913928270339966\n",
      "Prediction loss at step 1876 is: 1.343443512916565\n",
      "Prediction loss at step 1877 is: 1.1072245836257935\n",
      "Prediction loss at step 1878 is: 1.5322558879852295\n",
      "Prediction loss at step 1879 is: 1.6743223667144775\n",
      "Prediction loss at step 1880 is: 1.4240442514419556\n",
      "Prediction loss at step 1881 is: 1.5737241506576538\n",
      "Prediction loss at step 1882 is: 1.71199631690979\n",
      "Prediction loss at step 1883 is: 2.1217446327209473\n",
      "Prediction loss at step 1884 is: 1.6261372566223145\n",
      "Prediction loss at step 1885 is: 1.7575117349624634\n",
      "Prediction loss at step 1886 is: 1.8019644021987915\n",
      "Prediction loss at step 1887 is: 1.3745774030685425\n",
      "Prediction loss at step 1888 is: 1.4104119539260864\n",
      "Prediction loss at step 1889 is: 1.8564645051956177\n",
      "Prediction loss at step 1890 is: 1.4414910078048706\n",
      "Prediction loss at step 1891 is: 1.252497911453247\n",
      "Prediction loss at step 1892 is: 2.2718894481658936\n",
      "Prediction loss at step 1893 is: 1.968788743019104\n",
      "Prediction loss at step 1894 is: 1.9177370071411133\n",
      "Prediction loss at step 1895 is: 1.5909174680709839\n",
      "Prediction loss at step 1896 is: 1.7238924503326416\n",
      "Prediction loss at step 1897 is: 1.7731322050094604\n",
      "Prediction loss at step 1898 is: 2.2855658531188965\n",
      "Prediction loss at step 1899 is: 1.5302302837371826\n",
      "Prediction loss at step 1900 is: 1.5583628416061401\n",
      "Prediction loss at step 1901 is: 2.341118335723877\n",
      "Prediction loss at step 1902 is: 2.058211088180542\n",
      "Prediction loss at step 1903 is: 1.9816659688949585\n",
      "Prediction loss at step 1904 is: 1.5937089920043945\n",
      "Prediction loss at step 1905 is: 1.4540685415267944\n",
      "Prediction loss at step 1906 is: 1.7519980669021606\n",
      "Prediction loss at step 1907 is: 1.164319396018982\n",
      "Prediction loss at step 1908 is: 1.7834593057632446\n",
      "Prediction loss at step 1909 is: 1.7326712608337402\n",
      "Prediction loss at step 1910 is: 1.3238667249679565\n",
      "Prediction loss at step 1911 is: 1.531981348991394\n",
      "Prediction loss at step 1912 is: 1.236973524093628\n",
      "Prediction loss at step 1913 is: 1.9767543077468872\n",
      "Prediction loss at step 1914 is: 1.7330296039581299\n",
      "Prediction loss at step 1915 is: 2.1687331199645996\n",
      "Prediction loss at step 1916 is: 1.6644432544708252\n",
      "Prediction loss at step 1917 is: 1.6282869577407837\n",
      "Prediction loss at step 1918 is: 1.484176754951477\n",
      "Prediction loss at step 1919 is: 1.9995825290679932\n",
      "Prediction loss at step 1920 is: 1.9810653924942017\n",
      "Prediction loss at step 1921 is: 1.9203107357025146\n",
      "Prediction loss at step 1922 is: 1.4529852867126465\n",
      "Prediction loss at step 1923 is: 1.9937849044799805\n",
      "Prediction loss at step 1924 is: 1.8399338722229004\n",
      "Prediction loss at step 1925 is: 2.0461511611938477\n",
      "Prediction loss at step 1926 is: 1.9070814847946167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 1927 is: 1.8470710515975952\n",
      "Prediction loss at step 1928 is: 1.5159894227981567\n",
      "Prediction loss at step 1929 is: 1.9699877500534058\n",
      "Prediction loss at step 1930 is: 1.3459676504135132\n",
      "Prediction loss at step 1931 is: 1.724423885345459\n",
      "Prediction loss at step 1932 is: 2.093872308731079\n",
      "Prediction loss at step 1933 is: 1.7908215522766113\n",
      "Prediction loss at step 1934 is: 1.9685145616531372\n",
      "Prediction loss at step 1935 is: 1.9218229055404663\n",
      "Prediction loss at step 1936 is: 1.872727870941162\n",
      "Prediction loss at step 1937 is: 1.8810317516326904\n",
      "Prediction loss at step 1938 is: 1.7042864561080933\n",
      "Prediction loss at step 1939 is: 2.354434013366699\n",
      "Prediction loss at step 1940 is: 1.7644498348236084\n",
      "Prediction loss at step 1941 is: 1.4353089332580566\n",
      "Prediction loss at step 1942 is: 1.4609296321868896\n",
      "Prediction loss at step 1943 is: 2.01924467086792\n",
      "Prediction loss at step 1944 is: 1.5154781341552734\n",
      "Prediction loss at step 1945 is: 2.229153633117676\n",
      "Prediction loss at step 1946 is: 1.2794253826141357\n",
      "Prediction loss at step 1947 is: 2.0110907554626465\n",
      "Prediction loss at step 1948 is: 1.7774332761764526\n",
      "Prediction loss at step 1949 is: 1.741927146911621\n",
      "Prediction loss at step 1950 is: 1.7712777853012085\n",
      "Prediction loss at step 1951 is: 2.1205592155456543\n",
      "Prediction loss at step 1952 is: 1.766656517982483\n",
      "Prediction loss at step 1953 is: 1.7115633487701416\n",
      "Prediction loss at step 1954 is: 1.4839181900024414\n",
      "Prediction loss at step 1955 is: 1.3083690404891968\n",
      "Prediction loss at step 1956 is: 2.0086770057678223\n",
      "Prediction loss at step 1957 is: 1.3870152235031128\n",
      "Prediction loss at step 1958 is: 1.8046066761016846\n",
      "Prediction loss at step 1959 is: 2.235734701156616\n",
      "Prediction loss at step 1960 is: 2.0720388889312744\n",
      "Prediction loss at step 1961 is: 2.08223032951355\n",
      "Prediction loss at step 1962 is: 1.7543891668319702\n",
      "Prediction loss at step 1963 is: 1.7594701051712036\n",
      "Prediction loss at step 1964 is: 1.7679346799850464\n",
      "Prediction loss at step 1965 is: 1.6822623014450073\n",
      "Prediction loss at step 1966 is: 1.9953559637069702\n",
      "Prediction loss at step 1967 is: 2.0559732913970947\n",
      "Prediction loss at step 1968 is: 2.198904037475586\n",
      "Prediction loss at step 1969 is: 1.885394811630249\n",
      "Prediction loss at step 1970 is: 1.9746137857437134\n",
      "Prediction loss at step 1971 is: 1.5314406156539917\n",
      "Prediction loss at step 1972 is: 1.3908532857894897\n",
      "Prediction loss at step 1973 is: 1.60982084274292\n",
      "Prediction loss at step 1974 is: 1.8062390089035034\n",
      "Prediction loss at step 1975 is: 1.8586082458496094\n",
      "Prediction loss at step 1976 is: 2.2512195110321045\n",
      "Prediction loss at step 1977 is: 1.3542183637619019\n",
      "Prediction loss at step 1978 is: 2.0911762714385986\n",
      "Prediction loss at step 1979 is: 2.0496132373809814\n",
      "Prediction loss at step 1980 is: 2.1204373836517334\n",
      "Prediction loss at step 1981 is: 1.7436765432357788\n",
      "Prediction loss at step 1982 is: 2.134218454360962\n",
      "Prediction loss at step 1983 is: 1.675142765045166\n",
      "Prediction loss at step 1984 is: 1.7349233627319336\n",
      "Prediction loss at step 1985 is: 1.435976266860962\n",
      "Prediction loss at step 1986 is: 1.7567107677459717\n",
      "Prediction loss at step 1987 is: 1.769342064857483\n",
      "Prediction loss at step 1988 is: 2.1978936195373535\n",
      "Prediction loss at step 1989 is: 2.031104326248169\n",
      "Prediction loss at step 1990 is: 1.874733328819275\n",
      "Prediction loss at step 1991 is: 1.7987717390060425\n",
      "Prediction loss at step 1992 is: 1.9277255535125732\n",
      "Prediction loss at step 1993 is: 1.8877770900726318\n",
      "Prediction loss at step 1994 is: 1.9546667337417603\n",
      "Prediction loss at step 1995 is: 1.763039231300354\n",
      "Prediction loss at step 1996 is: 1.762442708015442\n",
      "Prediction loss at step 1997 is: 1.5271580219268799\n",
      "Prediction loss at step 1998 is: 1.5961766242980957\n",
      "Prediction loss at step 1999 is: 1.5442672967910767\n",
      "Prediction loss at step 2000 is: 1.4505054950714111\n",
      "Prediction loss at step 2001 is: 1.148677110671997\n",
      "Prediction loss at step 2002 is: 1.9000701904296875\n",
      "Prediction loss at step 2003 is: 1.480587124824524\n",
      "Prediction loss at step 2004 is: 1.9063314199447632\n",
      "Prediction loss at step 2005 is: 1.680564284324646\n",
      "Prediction loss at step 2006 is: 1.2678855657577515\n",
      "Prediction loss at step 2007 is: 1.95596444606781\n",
      "Prediction loss at step 2008 is: 1.7570247650146484\n",
      "Prediction loss at step 2009 is: 1.7524768114089966\n",
      "Prediction loss at step 2010 is: 1.8067532777786255\n",
      "Prediction loss at step 2011 is: 1.867055892944336\n",
      "Prediction loss at step 2012 is: 1.718693733215332\n",
      "Prediction loss at step 2013 is: 1.9159032106399536\n",
      "Prediction loss at step 2014 is: 1.8247504234313965\n",
      "Prediction loss at step 2015 is: 1.9253464937210083\n",
      "Prediction loss at step 2016 is: 1.5528262853622437\n",
      "Prediction loss at step 2017 is: 2.0023460388183594\n",
      "Prediction loss at step 2018 is: 1.815090298652649\n",
      "Prediction loss at step 2019 is: 1.4743659496307373\n",
      "Prediction loss at step 2020 is: 1.9028596878051758\n",
      "Prediction loss at step 2021 is: 1.7233059406280518\n",
      "Prediction loss at step 2022 is: 1.5578073263168335\n",
      "Prediction loss at step 2023 is: 1.5116990804672241\n",
      "Prediction loss at step 2024 is: 1.3589708805084229\n",
      "Prediction loss at step 2025 is: 1.792259693145752\n",
      "Prediction loss at step 2026 is: 1.2882039546966553\n",
      "Prediction loss at step 2027 is: 2.103846788406372\n",
      "Prediction loss at step 2028 is: 1.4026384353637695\n",
      "Prediction loss at step 2029 is: 1.984076976776123\n",
      "Prediction loss at step 2030 is: 1.106205940246582\n",
      "Prediction loss at step 2031 is: 1.5264804363250732\n",
      "Prediction loss at step 2032 is: 2.190481662750244\n",
      "Prediction loss at step 2033 is: 1.6625466346740723\n",
      "Prediction loss at step 2034 is: 2.0724217891693115\n",
      "Prediction loss at step 2035 is: 1.7208274602890015\n",
      "Prediction loss at step 2036 is: 1.8227818012237549\n",
      "Prediction loss at step 2037 is: 1.3278034925460815\n",
      "Prediction loss at step 2038 is: 2.2329893112182617\n",
      "Prediction loss at step 2039 is: 1.4929711818695068\n",
      "Prediction loss at step 2040 is: 2.1389124393463135\n",
      "Prediction loss at step 2041 is: 1.616186261177063\n",
      "Prediction loss at step 2042 is: 2.160738468170166\n",
      "Prediction loss at step 2043 is: 2.2362074851989746\n",
      "Prediction loss at step 2044 is: 1.5137856006622314\n",
      "Prediction loss at step 2045 is: 2.1986544132232666\n",
      "Prediction loss at step 2046 is: 1.5895015001296997\n",
      "Prediction loss at step 2047 is: 1.7889251708984375\n",
      "Prediction loss at step 2048 is: 2.01505446434021\n",
      "Prediction loss at step 2049 is: 2.1135571002960205\n",
      "Prediction loss at step 2050 is: 2.177330255508423\n",
      "Prediction loss at step 2051 is: 2.1000871658325195\n",
      "Prediction loss at step 2052 is: 1.7529507875442505\n",
      "Prediction loss at step 2053 is: 1.5607682466506958\n",
      "Prediction loss at step 2054 is: 2.0267558097839355\n",
      "Prediction loss at step 2055 is: 1.7199959754943848\n",
      "Prediction loss at step 2056 is: 1.8232626914978027\n",
      "Prediction loss at step 2057 is: 1.9560784101486206\n",
      "Prediction loss at step 2058 is: 1.5323164463043213\n",
      "Prediction loss at step 2059 is: 1.6259862184524536\n",
      "Prediction loss at step 2060 is: 1.2953981161117554\n",
      "Prediction loss at step 2061 is: 1.8774092197418213\n",
      "Prediction loss at step 2062 is: 1.0044347047805786\n",
      "Prediction loss at step 2063 is: 1.4152745008468628\n",
      "Prediction loss at step 2064 is: 1.8500514030456543\n",
      "Prediction loss at step 2065 is: 1.4093492031097412\n",
      "Prediction loss at step 2066 is: 1.766089677810669\n",
      "Prediction loss at step 2067 is: 2.0545222759246826\n",
      "Prediction loss at step 2068 is: 1.7188528776168823\n",
      "Prediction loss at step 2069 is: 1.8419179916381836\n",
      "Prediction loss at step 2070 is: 1.47052800655365\n",
      "Prediction loss at step 2071 is: 1.7444541454315186\n",
      "Prediction loss at step 2072 is: 1.9198126792907715\n",
      "Prediction loss at step 2073 is: 2.187420606613159\n",
      "Prediction loss at step 2074 is: 1.993127703666687\n",
      "Prediction loss at step 2075 is: 1.8062827587127686\n",
      "Prediction loss at step 2076 is: 2.0778234004974365\n",
      "Prediction loss at step 2077 is: 2.0774550437927246\n",
      "Prediction loss at step 2078 is: 1.989309310913086\n",
      "Prediction loss at step 2079 is: 1.6973204612731934\n",
      "Prediction loss at step 2080 is: 1.298615574836731\n",
      "Prediction loss at step 2081 is: 2.165829658508301\n",
      "Prediction loss at step 2082 is: 2.0851287841796875\n",
      "Prediction loss at step 2083 is: 1.8980190753936768\n",
      "Prediction loss at step 2084 is: 1.6844239234924316\n",
      "Prediction loss at step 2085 is: 2.2131383419036865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 2086 is: 2.14389967918396\n",
      "Prediction loss at step 2087 is: 1.7021238803863525\n",
      "Prediction loss at step 2088 is: 1.6735610961914062\n",
      "Prediction loss at step 2089 is: 2.1603901386260986\n",
      "Prediction loss at step 2090 is: 1.7723029851913452\n",
      "Prediction loss at step 2091 is: 1.829433798789978\n",
      "Prediction loss at step 2092 is: 1.5292607545852661\n",
      "Prediction loss at step 2093 is: 2.0132012367248535\n",
      "Prediction loss at step 2094 is: 1.635212779045105\n",
      "Prediction loss at step 2095 is: 1.818566083908081\n",
      "Prediction loss at step 2096 is: 1.733881950378418\n",
      "Prediction loss at step 2097 is: 1.9671112298965454\n",
      "Prediction loss at step 2098 is: 1.746202826499939\n",
      "Prediction loss at step 2099 is: 1.7857739925384521\n",
      "Prediction loss at step 2100 is: 1.7660034894943237\n",
      "Prediction loss at step 2101 is: 1.725644588470459\n",
      "Prediction loss at step 2102 is: 2.0649776458740234\n",
      "Prediction loss at step 2103 is: 2.044879913330078\n",
      "Prediction loss at step 2104 is: 1.989553451538086\n",
      "Prediction loss at step 2105 is: 1.275505781173706\n",
      "Prediction loss at step 2106 is: 1.7827110290527344\n",
      "Prediction loss at step 2107 is: 1.7968889474868774\n",
      "Prediction loss at step 2108 is: 2.0155832767486572\n",
      "Prediction loss at step 2109 is: 1.9813880920410156\n",
      "Prediction loss at step 2110 is: 2.0107383728027344\n",
      "Prediction loss at step 2111 is: 1.7497645616531372\n",
      "Prediction loss at step 2112 is: 1.5251106023788452\n",
      "Prediction loss at step 2113 is: 1.8535369634628296\n",
      "Prediction loss at step 2114 is: 1.6443904638290405\n",
      "Prediction loss at step 2115 is: 1.5731916427612305\n",
      "Prediction loss at step 2116 is: 1.2350212335586548\n",
      "Prediction loss at step 2117 is: 1.8339755535125732\n",
      "Prediction loss at step 2118 is: 1.6037307977676392\n",
      "Prediction loss at step 2119 is: 1.4566900730133057\n",
      "Prediction loss at step 2120 is: 2.030518054962158\n",
      "Prediction loss at step 2121 is: 1.9496608972549438\n",
      "Prediction loss at step 2122 is: 1.6840778589248657\n",
      "Prediction loss at step 2123 is: 1.4378925561904907\n",
      "Prediction loss at step 2124 is: 2.1637723445892334\n",
      "Prediction loss at step 2125 is: 1.932199478149414\n",
      "Prediction loss at step 2126 is: 2.0052740573883057\n",
      "Prediction loss at step 2127 is: 1.721612572669983\n",
      "Prediction loss at step 2128 is: 1.6693882942199707\n",
      "Prediction loss at step 2129 is: 1.717299222946167\n",
      "Prediction loss at step 2130 is: 1.817679762840271\n",
      "Prediction loss at step 2131 is: 2.027911424636841\n",
      "Prediction loss at step 2132 is: 1.5879565477371216\n",
      "Prediction loss at step 2133 is: 1.7798066139221191\n",
      "Prediction loss at step 2134 is: 2.2483248710632324\n",
      "Prediction loss at step 2135 is: 1.6498929262161255\n",
      "Prediction loss at step 2136 is: 1.6281040906906128\n",
      "Prediction loss at step 2137 is: 1.6659040451049805\n",
      "Prediction loss at step 2138 is: 0.9628019332885742\n",
      "Prediction loss at step 2139 is: 1.8070188760757446\n",
      "Prediction loss at step 2140 is: 1.4265748262405396\n",
      "Prediction loss at step 2141 is: 1.36588454246521\n",
      "Prediction loss at step 2142 is: 1.9643837213516235\n",
      "Prediction loss at step 2143 is: 1.0285749435424805\n",
      "Prediction loss at step 2144 is: 1.2512906789779663\n",
      "Prediction loss at step 2145 is: 2.0090363025665283\n",
      "Prediction loss at step 2146 is: 2.2143828868865967\n",
      "Prediction loss at step 2147 is: 1.678170084953308\n",
      "Prediction loss at step 2148 is: 2.164466142654419\n",
      "Prediction loss at step 2149 is: 1.8027888536453247\n",
      "Prediction loss at step 2150 is: 1.8521865606307983\n",
      "Prediction loss at step 2151 is: 1.512729287147522\n",
      "Prediction loss at step 2152 is: 2.144857406616211\n",
      "Prediction loss at step 2153 is: 2.0203092098236084\n",
      "Prediction loss at step 2154 is: 2.024019956588745\n",
      "Prediction loss at step 2155 is: 1.9247349500656128\n",
      "Prediction loss at step 2156 is: 1.9445430040359497\n",
      "Prediction loss at step 2157 is: 1.7231765985488892\n",
      "Prediction loss at step 2158 is: 1.5756409168243408\n",
      "Prediction loss at step 2159 is: 2.0322043895721436\n",
      "Prediction loss at step 2160 is: 1.7114847898483276\n",
      "Prediction loss at step 2161 is: 2.162546396255493\n",
      "Prediction loss at step 2162 is: 1.8265334367752075\n",
      "Prediction loss at step 2163 is: 1.9312437772750854\n",
      "Prediction loss at step 2164 is: 1.8345987796783447\n",
      "Prediction loss at step 2165 is: 1.794335961341858\n",
      "Prediction loss at step 2166 is: 1.056411862373352\n",
      "Prediction loss at step 2167 is: 1.7162591218948364\n",
      "Prediction loss at step 2168 is: 2.178201675415039\n",
      "Prediction loss at step 2169 is: 2.0032451152801514\n",
      "Prediction loss at step 2170 is: 2.1150898933410645\n",
      "Prediction loss at step 2171 is: 1.3045673370361328\n",
      "Prediction loss at step 2172 is: 1.6834921836853027\n",
      "Prediction loss at step 2173 is: 1.8355153799057007\n",
      "Prediction loss at step 2174 is: 1.249247670173645\n",
      "Prediction loss at step 2175 is: 1.427978754043579\n",
      "Prediction loss at step 2176 is: 1.373764157295227\n",
      "Prediction loss at step 2177 is: 1.827436923980713\n",
      "Prediction loss at step 2178 is: 1.4524444341659546\n",
      "Prediction loss at step 2179 is: 2.106290817260742\n",
      "Prediction loss at step 2180 is: 1.7731493711471558\n",
      "Prediction loss at step 2181 is: 1.5536874532699585\n",
      "Prediction loss at step 2182 is: 1.9379470348358154\n",
      "Prediction loss at step 2183 is: 1.982172966003418\n",
      "Prediction loss at step 2184 is: 1.893359661102295\n",
      "Prediction loss at step 2185 is: 1.5836549997329712\n",
      "Prediction loss at step 2186 is: 2.254493236541748\n",
      "Prediction loss at step 2187 is: 1.8499547243118286\n",
      "Prediction loss at step 2188 is: 1.8199207782745361\n",
      "Prediction loss at step 2189 is: 1.79044508934021\n",
      "Prediction loss at step 2190 is: 1.8605626821517944\n",
      "Prediction loss at step 2191 is: 1.192371129989624\n",
      "Prediction loss at step 2192 is: 1.9278295040130615\n",
      "Prediction loss at step 2193 is: 1.6797207593917847\n",
      "Prediction loss at step 2194 is: 1.8728301525115967\n",
      "Prediction loss at step 2195 is: 1.7501896619796753\n",
      "Prediction loss at step 2196 is: 1.6996874809265137\n",
      "Prediction loss at step 2197 is: 1.803952693939209\n",
      "Prediction loss at step 2198 is: 2.1096956729888916\n",
      "Prediction loss at step 2199 is: 1.7278155088424683\n",
      "Prediction loss at step 2200 is: 2.1654458045959473\n",
      "Prediction loss at step 2201 is: 1.5985559225082397\n",
      "Prediction loss at step 2202 is: 1.5995726585388184\n",
      "Prediction loss at step 2203 is: 2.0372796058654785\n",
      "Prediction loss at step 2204 is: 2.074821949005127\n",
      "Prediction loss at step 2205 is: 1.6709744930267334\n",
      "Prediction loss at step 2206 is: 1.9495569467544556\n",
      "Prediction loss at step 2207 is: 2.086031675338745\n",
      "Prediction loss at step 2208 is: 2.0432562828063965\n",
      "Prediction loss at step 2209 is: 1.5297194719314575\n",
      "Prediction loss at step 2210 is: 1.874102234840393\n",
      "Prediction loss at step 2211 is: 1.5709593296051025\n",
      "Prediction loss at step 2212 is: 1.801493525505066\n",
      "Prediction loss at step 2213 is: 2.119386672973633\n",
      "Prediction loss at step 2214 is: 1.662232518196106\n",
      "Prediction loss at step 2215 is: 1.5002381801605225\n",
      "Prediction loss at step 2216 is: 1.7543718814849854\n",
      "Prediction loss at step 2217 is: 1.6141667366027832\n",
      "Prediction loss at step 2218 is: 2.0233943462371826\n",
      "Prediction loss at step 2219 is: 1.7348769903182983\n",
      "Prediction loss at step 2220 is: 1.8711864948272705\n",
      "Prediction loss at step 2221 is: 1.6664421558380127\n",
      "Prediction loss at step 2222 is: 2.133826971054077\n",
      "Prediction loss at step 2223 is: 1.343297004699707\n",
      "Prediction loss at step 2224 is: 2.0643374919891357\n",
      "Prediction loss at step 2225 is: 1.948498249053955\n",
      "Prediction loss at step 2226 is: 2.136613368988037\n",
      "Prediction loss at step 2227 is: 1.790075421333313\n",
      "Prediction loss at step 2228 is: 1.1250025033950806\n",
      "Prediction loss at step 2229 is: 1.77535080909729\n",
      "Prediction loss at step 2230 is: 1.9542388916015625\n",
      "Prediction loss at step 2231 is: 1.9066495895385742\n",
      "Prediction loss at step 2232 is: 1.8767755031585693\n",
      "Prediction loss at step 2233 is: 1.7173575162887573\n",
      "Prediction loss at step 2234 is: 2.2670743465423584\n",
      "Prediction loss at step 2235 is: 2.1949033737182617\n",
      "Prediction loss at step 2236 is: 1.5201590061187744\n",
      "Prediction loss at step 2237 is: 1.195195198059082\n",
      "Prediction loss at step 2238 is: 1.7192164659500122\n",
      "Prediction loss at step 2239 is: 1.6077561378479004\n",
      "Prediction loss at step 2240 is: 1.5945466756820679\n",
      "Prediction loss at step 2241 is: 1.6345571279525757\n",
      "Prediction loss at step 2242 is: 1.9331090450286865\n",
      "Prediction loss at step 2243 is: 2.1634521484375\n",
      "Prediction loss at step 2244 is: 2.160076141357422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 2245 is: 2.185793399810791\n",
      "Prediction loss at step 2246 is: 1.977425217628479\n",
      "Prediction loss at step 2247 is: 2.238436222076416\n",
      "Prediction loss at step 2248 is: 2.002563714981079\n",
      "Prediction loss at step 2249 is: 2.020176410675049\n",
      "Prediction loss at step 2250 is: 1.2073156833648682\n",
      "Prediction loss at step 2251 is: 1.8080909252166748\n",
      "Prediction loss at step 2252 is: 1.990980863571167\n",
      "Prediction loss at step 2253 is: 1.5775442123413086\n",
      "Prediction loss at step 2254 is: 1.3917609453201294\n",
      "Prediction loss at step 2255 is: 2.0530202388763428\n",
      "Prediction loss at step 2256 is: 1.6622440814971924\n",
      "Prediction loss at step 2257 is: 1.6942273378372192\n",
      "Prediction loss at step 2258 is: 1.102113127708435\n",
      "Prediction loss at step 2259 is: 1.3224120140075684\n",
      "Prediction loss at step 2260 is: 2.126495838165283\n",
      "Prediction loss at step 2261 is: 1.4048254489898682\n",
      "Prediction loss at step 2262 is: 1.5447449684143066\n",
      "Prediction loss at step 2263 is: 1.953201174736023\n",
      "Prediction loss at step 2264 is: 2.0900466442108154\n",
      "Prediction loss at step 2265 is: 1.9444949626922607\n",
      "Prediction loss at step 2266 is: 1.5973011255264282\n",
      "Prediction loss at step 2267 is: 1.0724397897720337\n",
      "Prediction loss at step 2268 is: 1.7874668836593628\n",
      "Prediction loss at step 2269 is: 1.8728129863739014\n",
      "Prediction loss at step 2270 is: 2.137503147125244\n",
      "Prediction loss at step 2271 is: 1.9118226766586304\n",
      "Prediction loss at step 2272 is: 2.0235252380371094\n",
      "Prediction loss at step 2273 is: 1.412644386291504\n",
      "Prediction loss at step 2274 is: 1.6386276483535767\n",
      "Prediction loss at step 2275 is: 1.6067544221878052\n",
      "Prediction loss at step 2276 is: 1.7475627660751343\n",
      "Prediction loss at step 2277 is: 1.2673571109771729\n",
      "Prediction loss at step 2278 is: 2.0270137786865234\n",
      "Prediction loss at step 2279 is: 2.0965163707733154\n",
      "Prediction loss at step 2280 is: 2.058265447616577\n",
      "Prediction loss at step 2281 is: 2.0497512817382812\n",
      "Prediction loss at step 2282 is: 1.960513710975647\n",
      "Prediction loss at step 2283 is: 2.2618868350982666\n",
      "Prediction loss at step 2284 is: 1.8194642066955566\n",
      "Prediction loss at step 2285 is: 1.9625928401947021\n",
      "Prediction loss at step 2286 is: 1.9643696546554565\n",
      "Prediction loss at step 2287 is: 1.9585864543914795\n",
      "Prediction loss at step 2288 is: 2.334498882293701\n",
      "Prediction loss at step 2289 is: 1.665332555770874\n",
      "Prediction loss at step 2290 is: 1.992667317390442\n",
      "Prediction loss at step 2291 is: 1.5923924446105957\n",
      "Prediction loss at step 2292 is: 1.6663498878479004\n",
      "Prediction loss at step 2293 is: 1.5444988012313843\n",
      "Prediction loss at step 2294 is: 2.0550272464752197\n",
      "Prediction loss at step 2295 is: 1.7185536623001099\n",
      "Prediction loss at step 2296 is: 2.1607649326324463\n",
      "Prediction loss at step 2297 is: 2.0017402172088623\n",
      "Prediction loss at step 2298 is: 1.689098834991455\n",
      "Prediction loss at step 2299 is: 1.9974101781845093\n",
      "Prediction loss at step 2300 is: 2.107342481613159\n",
      "Prediction loss at step 2301 is: 1.5558050870895386\n",
      "Prediction loss at step 2302 is: 1.865666389465332\n",
      "Prediction loss at step 2303 is: 2.068922519683838\n",
      "Prediction loss at step 2304 is: 1.8796676397323608\n",
      "Prediction loss at step 2305 is: 1.8517252206802368\n",
      "Prediction loss at step 2306 is: 1.651296615600586\n",
      "Prediction loss at step 2307 is: 1.4480615854263306\n",
      "Prediction loss at step 2308 is: 2.2851638793945312\n",
      "Prediction loss at step 2309 is: 1.696871042251587\n",
      "Prediction loss at step 2310 is: 2.0626277923583984\n",
      "Prediction loss at step 2311 is: 1.6291247606277466\n",
      "Prediction loss at step 2312 is: 2.0871567726135254\n",
      "Prediction loss at step 2313 is: 1.9041749238967896\n",
      "Prediction loss at step 2314 is: 1.5620628595352173\n",
      "Prediction loss at step 2315 is: 1.5375789403915405\n",
      "Prediction loss at step 2316 is: 1.0452840328216553\n",
      "Prediction loss at step 2317 is: 1.887101173400879\n",
      "Prediction loss at step 2318 is: 1.7873492240905762\n",
      "Prediction loss at step 2319 is: 1.9473533630371094\n",
      "Prediction loss at step 2320 is: 1.6260672807693481\n",
      "Prediction loss at step 2321 is: 1.8089205026626587\n",
      "Prediction loss at step 2322 is: 2.1001079082489014\n",
      "Prediction loss at step 2323 is: 1.9449347257614136\n",
      "Prediction loss at step 2324 is: 1.8997831344604492\n",
      "Prediction loss at step 2325 is: 1.0598384141921997\n",
      "Prediction loss at step 2326 is: 1.8928617238998413\n",
      "Prediction loss at step 2327 is: 1.7310123443603516\n",
      "Prediction loss at step 2328 is: 1.9278874397277832\n",
      "Prediction loss at step 2329 is: 2.2200746536254883\n",
      "Prediction loss at step 2330 is: 2.0859873294830322\n",
      "Prediction loss at step 2331 is: 1.8687735795974731\n",
      "Prediction loss at step 2332 is: 2.006635904312134\n",
      "Prediction loss at step 2333 is: 1.693176507949829\n",
      "Prediction loss at step 2334 is: 1.6373121738433838\n",
      "Prediction loss at step 2335 is: 1.711672306060791\n",
      "Prediction loss at step 2336 is: 1.4473345279693604\n",
      "Prediction loss at step 2337 is: 2.025742530822754\n",
      "Prediction loss at step 2338 is: 1.4198137521743774\n",
      "Prediction loss at step 2339 is: 1.8380393981933594\n",
      "Prediction loss at step 2340 is: 2.1354432106018066\n",
      "Prediction loss at step 2341 is: 1.851304292678833\n",
      "Prediction loss at step 2342 is: 2.0777857303619385\n",
      "Prediction loss at step 2343 is: 1.950719952583313\n",
      "Prediction loss at step 2344 is: 1.530263066291809\n",
      "Prediction loss at step 2345 is: 1.3070124387741089\n",
      "Prediction loss at step 2346 is: 1.8165092468261719\n",
      "Prediction loss at step 2347 is: 1.3638899326324463\n",
      "Prediction loss at step 2348 is: 1.1117022037506104\n",
      "Prediction loss at step 2349 is: 1.445529580116272\n",
      "Prediction loss at step 2350 is: 1.5447193384170532\n",
      "Prediction loss at step 2351 is: 1.6401770114898682\n",
      "Prediction loss at step 2352 is: 1.9209716320037842\n",
      "Prediction loss at step 2353 is: 1.5714834928512573\n",
      "Prediction loss at step 2354 is: 1.5217804908752441\n",
      "Prediction loss at step 2355 is: 2.0755302906036377\n",
      "Prediction loss at step 2356 is: 1.9320104122161865\n",
      "Prediction loss at step 2357 is: 1.8599615097045898\n",
      "Prediction loss at step 2358 is: 1.4595556259155273\n",
      "Prediction loss at step 2359 is: 2.259673595428467\n",
      "Prediction loss at step 2360 is: 2.169328451156616\n",
      "Prediction loss at step 2361 is: 2.1778676509857178\n",
      "Prediction loss at step 2362 is: 1.4238321781158447\n",
      "Prediction loss at step 2363 is: 2.029139280319214\n",
      "Prediction loss at step 2364 is: 1.571586012840271\n",
      "Prediction loss at step 2365 is: 2.2526321411132812\n",
      "Prediction loss at step 2366 is: 1.637489676475525\n",
      "Prediction loss at step 2367 is: 1.7438167333602905\n",
      "Prediction loss at step 2368 is: 1.8373148441314697\n",
      "Prediction loss at step 2369 is: 1.6542019844055176\n",
      "Prediction loss at step 2370 is: 2.0622265338897705\n",
      "Prediction loss at step 2371 is: 1.7312387228012085\n",
      "Prediction loss at step 2372 is: 1.7805097103118896\n",
      "Prediction loss at step 2373 is: 1.6177247762680054\n",
      "Prediction loss at step 2374 is: 1.7986141443252563\n",
      "Prediction loss at step 2375 is: 1.9790928363800049\n",
      "Prediction loss at step 2376 is: 1.581290602684021\n",
      "Prediction loss at step 2377 is: 1.5210400819778442\n",
      "Prediction loss at step 2378 is: 2.043767213821411\n",
      "Prediction loss at step 2379 is: 1.5083788633346558\n",
      "Prediction loss at step 2380 is: 1.5224802494049072\n",
      "Prediction loss at step 2381 is: 1.6062160730361938\n",
      "Prediction loss at step 2382 is: 2.098162889480591\n",
      "Prediction loss at step 2383 is: 1.9938379526138306\n",
      "Prediction loss at step 2384 is: 1.325452446937561\n",
      "Prediction loss at step 2385 is: 1.583573818206787\n",
      "Prediction loss at step 2386 is: 1.8788360357284546\n",
      "Prediction loss at step 2387 is: 1.428320288658142\n",
      "Prediction loss at step 2388 is: 1.625056505203247\n",
      "Prediction loss at step 2389 is: 1.8027669191360474\n",
      "Prediction loss at step 2390 is: 2.1518046855926514\n",
      "Prediction loss at step 2391 is: 1.7773187160491943\n",
      "Prediction loss at step 2392 is: 1.8125739097595215\n",
      "Prediction loss at step 2393 is: 2.157564401626587\n",
      "Prediction loss at step 2394 is: 1.4269556999206543\n",
      "Prediction loss at step 2395 is: 1.827821135520935\n",
      "Prediction loss at step 2396 is: 1.7702966928482056\n",
      "Prediction loss at step 2397 is: 1.8897556066513062\n",
      "Prediction loss at step 2398 is: 1.4860470294952393\n",
      "Prediction loss at step 2399 is: 1.2235443592071533\n",
      "Prediction loss at step 2400 is: 1.7463657855987549\n",
      "Prediction loss at step 2401 is: 1.760143756866455\n",
      "Prediction loss at step 2402 is: 2.0731372833251953\n",
      "Prediction loss at step 2403 is: 2.1123034954071045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 2404 is: 1.3529436588287354\n",
      "Prediction loss at step 2405 is: 1.5981560945510864\n",
      "Prediction loss at step 2406 is: 1.8156607151031494\n",
      "Prediction loss at step 2407 is: 1.7669873237609863\n",
      "Prediction loss at step 2408 is: 1.6204509735107422\n",
      "Prediction loss at step 2409 is: 1.6958147287368774\n",
      "Prediction loss at step 2410 is: 2.1804866790771484\n",
      "Prediction loss at step 2411 is: 2.142524242401123\n",
      "Prediction loss at step 2412 is: 1.6533746719360352\n",
      "Prediction loss at step 2413 is: 1.9259039163589478\n",
      "Prediction loss at step 2414 is: 1.5677671432495117\n",
      "Prediction loss at step 2415 is: 2.0292975902557373\n",
      "Prediction loss at step 2416 is: 1.4537068605422974\n",
      "Prediction loss at step 2417 is: 1.8155643939971924\n",
      "Prediction loss at step 2418 is: 1.5494240522384644\n",
      "Prediction loss at step 2419 is: 2.0387091636657715\n",
      "Prediction loss at step 2420 is: 1.9746713638305664\n",
      "Prediction loss at step 2421 is: 1.3433464765548706\n",
      "Prediction loss at step 2422 is: 1.7114143371582031\n",
      "Prediction loss at step 2423 is: 1.8852018117904663\n",
      "Prediction loss at step 2424 is: 1.5227720737457275\n",
      "Prediction loss at step 2425 is: 1.4229614734649658\n",
      "Prediction loss at step 2426 is: 1.663920521736145\n",
      "Prediction loss at step 2427 is: 1.8125157356262207\n",
      "Prediction loss at step 2428 is: 1.5004899501800537\n",
      "Prediction loss at step 2429 is: 1.5682746171951294\n",
      "Prediction loss at step 2430 is: 1.6636297702789307\n",
      "Prediction loss at step 2431 is: 1.5176811218261719\n",
      "Prediction loss at step 2432 is: 1.873584270477295\n",
      "Prediction loss at step 2433 is: 1.7214487791061401\n",
      "Prediction loss at step 2434 is: 1.9862545728683472\n",
      "Prediction loss at step 2435 is: 1.740910291671753\n",
      "Prediction loss at step 2436 is: 1.8439782857894897\n",
      "Prediction loss at step 2437 is: 2.0541796684265137\n",
      "Prediction loss at step 2438 is: 1.4749263525009155\n",
      "Prediction loss at step 2439 is: 2.0230774879455566\n",
      "Prediction loss at step 2440 is: 1.4963397979736328\n",
      "Prediction loss at step 2441 is: 2.0632236003875732\n",
      "Prediction loss at step 2442 is: 1.4910364151000977\n",
      "Prediction loss at step 2443 is: 1.5303953886032104\n",
      "Prediction loss at step 2444 is: 1.8810644149780273\n",
      "Prediction loss at step 2445 is: 1.9507560729980469\n",
      "Prediction loss at step 2446 is: 1.7068710327148438\n",
      "Prediction loss at step 2447 is: 2.0532095432281494\n",
      "Prediction loss at step 2448 is: 1.8347214460372925\n",
      "Prediction loss at step 2449 is: 2.0957839488983154\n",
      "Prediction loss at step 2450 is: 1.511497139930725\n",
      "Prediction loss at step 2451 is: 1.720005989074707\n",
      "Prediction loss at step 2452 is: 2.228792667388916\n",
      "Prediction loss at step 2453 is: 1.9218120574951172\n",
      "Prediction loss at step 2454 is: 1.9892314672470093\n",
      "Prediction loss at step 2455 is: 2.077073097229004\n",
      "Prediction loss at step 2456 is: 1.943213701248169\n",
      "Prediction loss at step 2457 is: 2.0592660903930664\n",
      "Prediction loss at step 2458 is: 1.2178514003753662\n",
      "Prediction loss at step 2459 is: 1.4390350580215454\n",
      "Prediction loss at step 2460 is: 1.41959810256958\n",
      "Prediction loss at step 2461 is: 1.951720952987671\n",
      "Prediction loss at step 2462 is: 1.872047781944275\n",
      "Prediction loss at step 2463 is: 1.7550406455993652\n",
      "Prediction loss at step 2464 is: 1.977758526802063\n",
      "Prediction loss at step 2465 is: 1.2347993850708008\n",
      "Prediction loss at step 2466 is: 2.100454330444336\n",
      "Prediction loss at step 2467 is: 1.627213478088379\n",
      "Prediction loss at step 2468 is: 1.8555799722671509\n",
      "Prediction loss at step 2469 is: 1.3933770656585693\n",
      "Prediction loss at step 2470 is: 1.5731635093688965\n",
      "Prediction loss at step 2471 is: 1.6678346395492554\n",
      "Prediction loss at step 2472 is: 1.6528427600860596\n",
      "Prediction loss at step 2473 is: 1.5485777854919434\n",
      "Prediction loss at step 2474 is: 2.0368213653564453\n",
      "Prediction loss at step 2475 is: 2.0258629322052\n",
      "Prediction loss at step 2476 is: 1.3996433019638062\n",
      "Prediction loss at step 2477 is: 2.016275405883789\n",
      "Prediction loss at step 2478 is: 2.145320177078247\n",
      "Prediction loss at step 2479 is: 1.2747715711593628\n",
      "Prediction loss at step 2480 is: 1.6334679126739502\n",
      "Prediction loss at step 2481 is: 1.8294494152069092\n",
      "Prediction loss at step 2482 is: 2.033339500427246\n",
      "Prediction loss at step 2483 is: 2.0123658180236816\n",
      "Prediction loss at step 2484 is: 1.8649836778640747\n",
      "Prediction loss at step 2485 is: 1.6220403909683228\n",
      "Prediction loss at step 2486 is: 1.7509384155273438\n",
      "Prediction loss at step 2487 is: 1.6718559265136719\n",
      "Prediction loss at step 2488 is: 1.8424458503723145\n",
      "Prediction loss at step 2489 is: 1.6599787473678589\n",
      "Prediction loss at step 2490 is: 1.3484960794448853\n",
      "Prediction loss at step 2491 is: 2.009122371673584\n",
      "Prediction loss at step 2492 is: 2.261655807495117\n",
      "Prediction loss at step 2493 is: 2.122068405151367\n",
      "Prediction loss at step 2494 is: 1.8259018659591675\n",
      "Prediction loss at step 2495 is: 1.8483521938323975\n",
      "Prediction loss at step 2496 is: 1.5423356294631958\n",
      "Prediction loss at step 2497 is: 1.8703970909118652\n",
      "Prediction loss at step 2498 is: 1.2614285945892334\n",
      "Prediction loss at step 2499 is: 1.6042882204055786\n",
      "Prediction loss at step 2500 is: 1.9193979501724243\n",
      "Prediction loss at step 2501 is: 1.9933741092681885\n",
      "Prediction loss at step 2502 is: 1.982824683189392\n",
      "Prediction loss at step 2503 is: 2.1772799491882324\n",
      "Prediction loss at step 2504 is: 1.8713282346725464\n",
      "Prediction loss at step 2505 is: 1.8850475549697876\n",
      "Prediction loss at step 2506 is: 0.7072716951370239\n",
      "Prediction loss at step 2507 is: 1.8265087604522705\n",
      "Prediction loss at step 2508 is: 2.1595652103424072\n",
      "Prediction loss at step 2509 is: 1.069262981414795\n",
      "Prediction loss at step 2510 is: 1.539572834968567\n",
      "Prediction loss at step 2511 is: 1.5713480710983276\n",
      "Prediction loss at step 2512 is: 1.3857790231704712\n",
      "Prediction loss at step 2513 is: 1.9372714757919312\n",
      "Prediction loss at step 2514 is: 1.2791893482208252\n",
      "Prediction loss at step 2515 is: 1.542341709136963\n",
      "Prediction loss at step 2516 is: 1.8441492319107056\n",
      "Prediction loss at step 2517 is: 1.5357517004013062\n",
      "Prediction loss at step 2518 is: 2.0953547954559326\n",
      "Prediction loss at step 2519 is: 1.44136643409729\n",
      "Prediction loss at step 2520 is: 2.224919080734253\n",
      "Prediction loss at step 2521 is: 1.7592251300811768\n",
      "Prediction loss at step 2522 is: 1.952164649963379\n",
      "Prediction loss at step 2523 is: 2.2458252906799316\n",
      "Prediction loss at step 2524 is: 2.008976459503174\n",
      "Prediction loss at step 2525 is: 1.246313452720642\n",
      "Prediction loss at step 2526 is: 1.8268009424209595\n",
      "Prediction loss at step 2527 is: 1.9019116163253784\n",
      "Prediction loss at step 2528 is: 1.5087679624557495\n",
      "Prediction loss at step 2529 is: 1.75613534450531\n",
      "Prediction loss at step 2530 is: 1.86515474319458\n",
      "Prediction loss at step 2531 is: 1.8821183443069458\n",
      "Prediction loss at step 2532 is: 2.003513813018799\n",
      "Prediction loss at step 2533 is: 1.7124894857406616\n",
      "Prediction loss at step 2534 is: 2.0416224002838135\n",
      "Prediction loss at step 2535 is: 1.9042285680770874\n",
      "Prediction loss at step 2536 is: 1.6950289011001587\n",
      "Prediction loss at step 2537 is: 1.490899682044983\n",
      "Prediction loss at step 2538 is: 1.6512099504470825\n",
      "Prediction loss at step 2539 is: 1.7883005142211914\n",
      "Prediction loss at step 2540 is: 2.135390281677246\n",
      "Prediction loss at step 2541 is: 1.9915852546691895\n",
      "Prediction loss at step 2542 is: 1.0966063737869263\n",
      "Prediction loss at step 2543 is: 1.287720799446106\n",
      "Prediction loss at step 2544 is: 1.5700865983963013\n",
      "Prediction loss at step 2545 is: 1.7761317491531372\n",
      "Prediction loss at step 2546 is: 1.8047759532928467\n",
      "Prediction loss at step 2547 is: 1.4803191423416138\n",
      "Prediction loss at step 2548 is: 2.0394675731658936\n",
      "Prediction loss at step 2549 is: 2.019195556640625\n",
      "Prediction loss at step 2550 is: 2.0413291454315186\n",
      "Prediction loss at step 2551 is: 2.137333631515503\n",
      "Prediction loss at step 2552 is: 1.714501142501831\n",
      "Prediction loss at step 2553 is: 2.02282452583313\n",
      "Prediction loss at step 2554 is: 1.8520621061325073\n",
      "Prediction loss at step 2555 is: 1.7832692861557007\n",
      "Prediction loss at step 2556 is: 1.7833160161972046\n",
      "Prediction loss at step 2557 is: 1.9339964389801025\n",
      "Prediction loss at step 2558 is: 1.3138747215270996\n",
      "Prediction loss at step 2559 is: 2.064849376678467\n",
      "Prediction loss at step 2560 is: 1.8780089616775513\n",
      "Prediction loss at step 2561 is: 1.7210582494735718\n",
      "Prediction loss at step 2562 is: 2.0464730262756348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 2563 is: 2.0575194358825684\n",
      "Prediction loss at step 2564 is: 2.119204521179199\n",
      "Prediction loss at step 2565 is: 1.6711689233779907\n",
      "Prediction loss at step 2566 is: 1.9644865989685059\n",
      "Prediction loss at step 2567 is: 2.1193270683288574\n",
      "Prediction loss at step 2568 is: 1.4794440269470215\n",
      "Prediction loss at step 2569 is: 2.0108535289764404\n",
      "Prediction loss at step 2570 is: 2.061753034591675\n",
      "Prediction loss at step 2571 is: 2.269681930541992\n",
      "Prediction loss at step 2572 is: 2.0076398849487305\n",
      "Prediction loss at step 2573 is: 1.9418730735778809\n",
      "Prediction loss at step 2574 is: 1.382230281829834\n",
      "Prediction loss at step 2575 is: 1.8981592655181885\n",
      "Prediction loss at step 2576 is: 1.6699678897857666\n",
      "Prediction loss at step 2577 is: 1.8630212545394897\n",
      "Prediction loss at step 2578 is: 2.2584784030914307\n",
      "Prediction loss at step 2579 is: 2.122802972793579\n",
      "Prediction loss at step 2580 is: 2.2463531494140625\n",
      "Prediction loss at step 2581 is: 2.136063575744629\n",
      "Prediction loss at step 2582 is: 1.4946691989898682\n",
      "Prediction loss at step 2583 is: 2.102280855178833\n",
      "Prediction loss at step 2584 is: 1.4904365539550781\n",
      "Prediction loss at step 2585 is: 1.70824134349823\n",
      "Prediction loss at step 2586 is: 1.720880150794983\n",
      "Prediction loss at step 2587 is: 2.0052919387817383\n",
      "Prediction loss at step 2588 is: 1.7149494886398315\n",
      "Prediction loss at step 2589 is: 1.967134714126587\n",
      "Prediction loss at step 2590 is: 1.7859294414520264\n",
      "Prediction loss at step 2591 is: 1.1940627098083496\n",
      "Prediction loss at step 2592 is: 1.9078459739685059\n",
      "Prediction loss at step 2593 is: 1.7271031141281128\n",
      "Prediction loss at step 2594 is: 1.6360962390899658\n",
      "Prediction loss at step 2595 is: 1.8676211833953857\n",
      "Prediction loss at step 2596 is: 1.3937653303146362\n",
      "Prediction loss at step 2597 is: 1.3840672969818115\n",
      "Prediction loss at step 2598 is: 1.9901567697525024\n",
      "Prediction loss at step 2599 is: 1.5119878053665161\n",
      "Prediction loss at step 2600 is: 1.9117872714996338\n",
      "Prediction loss at step 2601 is: 1.5720731019973755\n",
      "Prediction loss at step 2602 is: 1.669403314590454\n",
      "Prediction loss at step 2603 is: 1.5816656351089478\n",
      "Prediction loss at step 2604 is: 2.080993890762329\n",
      "Prediction loss at step 2605 is: 1.701393485069275\n",
      "Prediction loss at step 2606 is: 1.650294303894043\n",
      "Prediction loss at step 2607 is: 1.4639946222305298\n",
      "Prediction loss at step 2608 is: 1.887589931488037\n",
      "Prediction loss at step 2609 is: 1.7313344478607178\n",
      "Prediction loss at step 2610 is: 1.4625121355056763\n",
      "Prediction loss at step 2611 is: 1.4634428024291992\n",
      "Prediction loss at step 2612 is: 2.1204416751861572\n",
      "Prediction loss at step 2613 is: 2.171785831451416\n",
      "Prediction loss at step 2614 is: 2.190092086791992\n",
      "Prediction loss at step 2615 is: 1.756314754486084\n",
      "Prediction loss at step 2616 is: 2.246750593185425\n",
      "Prediction loss at step 2617 is: 1.2120115756988525\n",
      "Prediction loss at step 2618 is: 1.994218111038208\n",
      "Prediction loss at step 2619 is: 2.1855921745300293\n",
      "Prediction loss at step 2620 is: 2.0404956340789795\n",
      "Prediction loss at step 2621 is: 2.063401460647583\n",
      "Prediction loss at step 2622 is: 1.687038540840149\n",
      "Prediction loss at step 2623 is: 2.093989133834839\n",
      "Prediction loss at step 2624 is: 1.6095492839813232\n",
      "Prediction loss at step 2625 is: 1.1180888414382935\n",
      "Prediction loss at step 2626 is: 2.2037248611450195\n",
      "Prediction loss at step 2627 is: 2.0091121196746826\n",
      "Prediction loss at step 2628 is: 1.5359094142913818\n",
      "Prediction loss at step 2629 is: 1.8914591073989868\n",
      "Prediction loss at step 2630 is: 1.5693786144256592\n",
      "Prediction loss at step 2631 is: 1.6759669780731201\n",
      "Prediction loss at step 2632 is: 1.7420742511749268\n",
      "Prediction loss at step 2633 is: 1.8873270750045776\n",
      "Prediction loss at step 2634 is: 1.7279307842254639\n",
      "Prediction loss at step 2635 is: 1.6815139055252075\n",
      "Prediction loss at step 2636 is: 1.0597842931747437\n",
      "Prediction loss at step 2637 is: 1.2428548336029053\n",
      "Prediction loss at step 2638 is: 1.7397146224975586\n",
      "Prediction loss at step 2639 is: 2.026630401611328\n",
      "Prediction loss at step 2640 is: 1.8382936716079712\n",
      "Prediction loss at step 2641 is: 1.7633881568908691\n",
      "Prediction loss at step 2642 is: 1.2128674983978271\n",
      "Prediction loss at step 2643 is: 1.972548246383667\n",
      "Prediction loss at step 2644 is: 1.6950379610061646\n",
      "Prediction loss at step 2645 is: 2.088170051574707\n",
      "Prediction loss at step 2646 is: 2.107818603515625\n",
      "Prediction loss at step 2647 is: 2.0082356929779053\n",
      "Prediction loss at step 2648 is: 1.7080992460250854\n",
      "Prediction loss at step 2649 is: 1.4453051090240479\n",
      "Prediction loss at step 2650 is: 1.4715090990066528\n",
      "Prediction loss at step 2651 is: 2.114981174468994\n",
      "Prediction loss at step 2652 is: 1.8739548921585083\n",
      "Prediction loss at step 2653 is: 1.359490156173706\n",
      "Prediction loss at step 2654 is: 1.7492271661758423\n",
      "Prediction loss at step 2655 is: 1.5370426177978516\n",
      "Prediction loss at step 2656 is: 2.207660675048828\n",
      "Prediction loss at step 2657 is: 1.8557125329971313\n",
      "Prediction loss at step 2658 is: 0.974323034286499\n",
      "Prediction loss at step 2659 is: 1.9681181907653809\n",
      "Prediction loss at step 2660 is: 1.783592939376831\n",
      "Prediction loss at step 2661 is: 1.751787781715393\n",
      "Prediction loss at step 2662 is: 1.8153712749481201\n",
      "Prediction loss at step 2663 is: 1.969306230545044\n",
      "Prediction loss at step 2664 is: 1.5302979946136475\n",
      "Prediction loss at step 2665 is: 1.7869478464126587\n",
      "Prediction loss at step 2666 is: 2.154947519302368\n",
      "Prediction loss at step 2667 is: 1.6218119859695435\n",
      "Prediction loss at step 2668 is: 2.1432242393493652\n",
      "Prediction loss at step 2669 is: 1.5290396213531494\n",
      "Prediction loss at step 2670 is: 1.4878504276275635\n",
      "Prediction loss at step 2671 is: 2.001051902770996\n",
      "Prediction loss at step 2672 is: 1.9052751064300537\n",
      "Prediction loss at step 2673 is: 1.6390528678894043\n",
      "Prediction loss at step 2674 is: 2.3748176097869873\n",
      "Prediction loss at step 2675 is: 2.1439530849456787\n",
      "Prediction loss at step 2676 is: 1.5027379989624023\n",
      "Prediction loss at step 2677 is: 1.9323819875717163\n",
      "Prediction loss at step 2678 is: 1.8796941041946411\n",
      "Prediction loss at step 2679 is: 1.8647797107696533\n",
      "Prediction loss at step 2680 is: 1.3693275451660156\n",
      "Prediction loss at step 2681 is: 1.8798106908798218\n",
      "Prediction loss at step 2682 is: 1.6841799020767212\n",
      "Prediction loss at step 2683 is: 2.029385805130005\n",
      "Prediction loss at step 2684 is: 1.8729437589645386\n",
      "Prediction loss at step 2685 is: 1.5852113962173462\n",
      "Prediction loss at step 2686 is: 2.125608444213867\n",
      "Prediction loss at step 2687 is: 1.9431627988815308\n",
      "Prediction loss at step 2688 is: 1.8630573749542236\n",
      "Prediction loss at step 2689 is: 1.1404054164886475\n",
      "Prediction loss at step 2690 is: 1.893539309501648\n",
      "Prediction loss at step 2691 is: 1.9093396663665771\n",
      "Prediction loss at step 2692 is: 1.675532579421997\n",
      "Prediction loss at step 2693 is: 1.8086400032043457\n",
      "Prediction loss at step 2694 is: 2.123971462249756\n",
      "Prediction loss at step 2695 is: 1.6189396381378174\n",
      "Prediction loss at step 2696 is: 1.8596652746200562\n",
      "Prediction loss at step 2697 is: 2.098479986190796\n",
      "Prediction loss at step 2698 is: 1.7367479801177979\n",
      "Prediction loss at step 2699 is: 1.730206847190857\n",
      "Prediction loss at step 2700 is: 1.9639246463775635\n",
      "Prediction loss at step 2701 is: 2.1178317070007324\n",
      "Prediction loss at step 2702 is: 1.517109990119934\n",
      "Prediction loss at step 2703 is: 1.6689531803131104\n",
      "Prediction loss at step 2704 is: 1.5649713277816772\n",
      "Prediction loss at step 2705 is: 2.1510589122772217\n",
      "Prediction loss at step 2706 is: 2.028062582015991\n",
      "Prediction loss at step 2707 is: 1.7558505535125732\n",
      "Prediction loss at step 2708 is: 1.717422366142273\n",
      "Prediction loss at step 2709 is: 2.1362295150756836\n",
      "Prediction loss at step 2710 is: 1.8914544582366943\n",
      "Prediction loss at step 2711 is: 1.9547410011291504\n",
      "Prediction loss at step 2712 is: 2.3283138275146484\n",
      "Prediction loss at step 2713 is: 1.6328327655792236\n",
      "Prediction loss at step 2714 is: 1.8619914054870605\n",
      "Prediction loss at step 2715 is: 2.1470396518707275\n",
      "Prediction loss at step 2716 is: 1.6060950756072998\n",
      "Prediction loss at step 2717 is: 2.260681629180908\n",
      "Prediction loss at step 2718 is: 1.642359733581543\n",
      "Prediction loss at step 2719 is: 1.8234606981277466\n",
      "Prediction loss at step 2720 is: 1.406256914138794\n",
      "Prediction loss at step 2721 is: 1.494171142578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 2722 is: 1.747593879699707\n",
      "Prediction loss at step 2723 is: 1.9673707485198975\n",
      "Prediction loss at step 2724 is: 1.316205382347107\n",
      "Prediction loss at step 2725 is: 1.9230583906173706\n",
      "Prediction loss at step 2726 is: 2.0114641189575195\n",
      "Prediction loss at step 2727 is: 1.270478367805481\n",
      "Prediction loss at step 2728 is: 1.5452063083648682\n",
      "Prediction loss at step 2729 is: 1.6862335205078125\n",
      "Prediction loss at step 2730 is: 1.830629825592041\n",
      "Prediction loss at step 2731 is: 1.6469141244888306\n",
      "Prediction loss at step 2732 is: 2.22019362449646\n",
      "Prediction loss at step 2733 is: 1.8035190105438232\n",
      "Prediction loss at step 2734 is: 1.9766453504562378\n",
      "Prediction loss at step 2735 is: 2.054387092590332\n",
      "Prediction loss at step 2736 is: 1.772512435913086\n",
      "Prediction loss at step 2737 is: 1.9085605144500732\n",
      "Prediction loss at step 2738 is: 1.8009718656539917\n",
      "Prediction loss at step 2739 is: 1.982130527496338\n",
      "Prediction loss at step 2740 is: 2.0239033699035645\n",
      "Prediction loss at step 2741 is: 1.9162771701812744\n",
      "Prediction loss at step 2742 is: 1.5941143035888672\n",
      "Prediction loss at step 2743 is: 1.4766608476638794\n",
      "Prediction loss at step 2744 is: 2.087700605392456\n",
      "Prediction loss at step 2745 is: 1.8390568494796753\n",
      "Prediction loss at step 2746 is: 1.9989633560180664\n",
      "Prediction loss at step 2747 is: 1.886614203453064\n",
      "Prediction loss at step 2748 is: 2.1756651401519775\n",
      "Prediction loss at step 2749 is: 2.1527469158172607\n",
      "Prediction loss at step 2750 is: 1.341086745262146\n",
      "Prediction loss at step 2751 is: 1.6203869581222534\n",
      "Prediction loss at step 2752 is: 1.6250872611999512\n",
      "Prediction loss at step 2753 is: 1.6345540285110474\n",
      "Prediction loss at step 2754 is: 1.3780739307403564\n",
      "Prediction loss at step 2755 is: 1.5082976818084717\n",
      "Prediction loss at step 2756 is: 2.0166025161743164\n",
      "Prediction loss at step 2757 is: 1.770655632019043\n",
      "Prediction loss at step 2758 is: 1.863203525543213\n",
      "Prediction loss at step 2759 is: 1.6465712785720825\n",
      "Prediction loss at step 2760 is: 1.9953770637512207\n",
      "Prediction loss at step 2761 is: 1.7121741771697998\n",
      "Prediction loss at step 2762 is: 1.8519387245178223\n",
      "Prediction loss at step 2763 is: 1.505773663520813\n",
      "Prediction loss at step 2764 is: 1.775475263595581\n",
      "Prediction loss at step 2765 is: 1.8692188262939453\n",
      "Prediction loss at step 2766 is: 2.090481758117676\n",
      "Prediction loss at step 2767 is: 1.7704918384552002\n",
      "Prediction loss at step 2768 is: 2.068356513977051\n",
      "Prediction loss at step 2769 is: 1.7082884311676025\n",
      "Prediction loss at step 2770 is: 1.2518399953842163\n",
      "Prediction loss at step 2771 is: 1.9499887228012085\n",
      "Prediction loss at step 2772 is: 2.1593565940856934\n",
      "Prediction loss at step 2773 is: 2.2523553371429443\n",
      "Prediction loss at step 2774 is: 2.025054693222046\n",
      "Prediction loss at step 2775 is: 1.278246521949768\n",
      "Prediction loss at step 2776 is: 1.7471132278442383\n",
      "Prediction loss at step 2777 is: 1.558215856552124\n",
      "Prediction loss at step 2778 is: 2.0636086463928223\n",
      "Prediction loss at step 2779 is: 1.681800365447998\n",
      "Prediction loss at step 2780 is: 1.2336533069610596\n",
      "Prediction loss at step 2781 is: 2.0606839656829834\n",
      "Prediction loss at step 2782 is: 1.4591819047927856\n",
      "Prediction loss at step 2783 is: 2.1930484771728516\n",
      "Prediction loss at step 2784 is: 1.482414722442627\n",
      "Prediction loss at step 2785 is: 2.1695661544799805\n",
      "Prediction loss at step 2786 is: 1.8758138418197632\n",
      "Prediction loss at step 2787 is: 1.6317778825759888\n",
      "Prediction loss at step 2788 is: 2.270914316177368\n",
      "Prediction loss at step 2789 is: 1.7525503635406494\n",
      "Prediction loss at step 2790 is: 1.671900749206543\n",
      "Prediction loss at step 2791 is: 1.8032761812210083\n",
      "Prediction loss at step 2792 is: 1.850633144378662\n",
      "Prediction loss at step 2793 is: 1.9285920858383179\n",
      "Prediction loss at step 2794 is: 1.688539743423462\n",
      "Prediction loss at step 2795 is: 1.4450984001159668\n",
      "Prediction loss at step 2796 is: 1.5678718090057373\n",
      "Prediction loss at step 2797 is: 1.8824750185012817\n",
      "Prediction loss at step 2798 is: 1.655529499053955\n",
      "Prediction loss at step 2799 is: 1.4603829383850098\n",
      "Prediction loss at step 2800 is: 1.87449049949646\n",
      "Prediction loss at step 2801 is: 1.5315511226654053\n",
      "Prediction loss at step 2802 is: 2.036771297454834\n",
      "Prediction loss at step 2803 is: 1.9939748048782349\n",
      "Prediction loss at step 2804 is: 1.7637845277786255\n",
      "Prediction loss at step 2805 is: 1.3709386587142944\n",
      "Prediction loss at step 2806 is: 1.8497675657272339\n",
      "Prediction loss at step 2807 is: 1.6055794954299927\n",
      "Prediction loss at step 2808 is: 2.137606143951416\n",
      "Prediction loss at step 2809 is: 1.8642994165420532\n",
      "Prediction loss at step 2810 is: 1.6012316942214966\n",
      "Prediction loss at step 2811 is: 2.220055341720581\n",
      "Prediction loss at step 2812 is: 2.0173587799072266\n",
      "Prediction loss at step 2813 is: 1.9950523376464844\n",
      "Prediction loss at step 2814 is: 1.4621732234954834\n",
      "Prediction loss at step 2815 is: 2.104804515838623\n",
      "Prediction loss at step 2816 is: 1.5297688245773315\n",
      "Prediction loss at step 2817 is: 1.9641984701156616\n",
      "Prediction loss at step 2818 is: 1.600728988647461\n",
      "Prediction loss at step 2819 is: 2.23521089553833\n",
      "Prediction loss at step 2820 is: 2.2068870067596436\n",
      "Prediction loss at step 2821 is: 1.978248119354248\n",
      "Prediction loss at step 2822 is: 1.7095448970794678\n",
      "Prediction loss at step 2823 is: 1.8034493923187256\n",
      "Prediction loss at step 2824 is: 1.4727741479873657\n",
      "Prediction loss at step 2825 is: 2.18226957321167\n",
      "Prediction loss at step 2826 is: 1.2056344747543335\n",
      "Prediction loss at step 2827 is: 1.7105244398117065\n",
      "Prediction loss at step 2828 is: 1.5823440551757812\n",
      "Prediction loss at step 2829 is: 1.90773344039917\n",
      "Prediction loss at step 2830 is: 1.82003915309906\n",
      "Prediction loss at step 2831 is: 2.158325433731079\n",
      "Prediction loss at step 2832 is: 1.7439253330230713\n",
      "Prediction loss at step 2833 is: 1.9918259382247925\n",
      "Prediction loss at step 2834 is: 1.480355978012085\n",
      "Prediction loss at step 2835 is: 1.5611388683319092\n",
      "Prediction loss at step 2836 is: 2.011406183242798\n",
      "Prediction loss at step 2837 is: 1.674004316329956\n",
      "Prediction loss at step 2838 is: 2.04721999168396\n",
      "Prediction loss at step 2839 is: 1.5883187055587769\n",
      "Prediction loss at step 2840 is: 1.9361642599105835\n",
      "Prediction loss at step 2841 is: 1.9635558128356934\n",
      "Prediction loss at step 2842 is: 1.370425820350647\n",
      "Prediction loss at step 2843 is: 1.7970900535583496\n",
      "Prediction loss at step 2844 is: 2.153341770172119\n",
      "Prediction loss at step 2845 is: 2.117431640625\n",
      "Prediction loss at step 2846 is: 1.607775092124939\n",
      "Prediction loss at step 2847 is: 1.918690800666809\n",
      "Prediction loss at step 2848 is: 2.0759012699127197\n",
      "Prediction loss at step 2849 is: 1.8903143405914307\n",
      "Prediction loss at step 2850 is: 1.9213451147079468\n",
      "Prediction loss at step 2851 is: 2.1326699256896973\n",
      "Prediction loss at step 2852 is: 1.5970520973205566\n",
      "Prediction loss at step 2853 is: 1.685935616493225\n",
      "Prediction loss at step 2854 is: 2.000760078430176\n",
      "Prediction loss at step 2855 is: 1.8386662006378174\n",
      "Prediction loss at step 2856 is: 1.7847636938095093\n",
      "Prediction loss at step 2857 is: 2.2137296199798584\n",
      "Prediction loss at step 2858 is: 1.8092410564422607\n",
      "Prediction loss at step 2859 is: 1.5983617305755615\n",
      "Prediction loss at step 2860 is: 1.1350276470184326\n",
      "Prediction loss at step 2861 is: 1.4044759273529053\n",
      "Prediction loss at step 2862 is: 1.4598984718322754\n",
      "Prediction loss at step 2863 is: 2.1184823513031006\n",
      "Prediction loss at step 2864 is: 1.7610095739364624\n",
      "Prediction loss at step 2865 is: 1.4437990188598633\n",
      "Prediction loss at step 2866 is: 1.5313150882720947\n",
      "Prediction loss at step 2867 is: 1.8056280612945557\n",
      "Prediction loss at step 2868 is: 1.6596883535385132\n",
      "Prediction loss at step 2869 is: 1.6137745380401611\n",
      "Prediction loss at step 2870 is: 1.5620536804199219\n",
      "Prediction loss at step 2871 is: 1.8257701396942139\n",
      "Prediction loss at step 2872 is: 1.5464178323745728\n",
      "Prediction loss at step 2873 is: 1.8336124420166016\n",
      "Prediction loss at step 2874 is: 2.091350555419922\n",
      "Prediction loss at step 2875 is: 1.9418305158615112\n",
      "Prediction loss at step 2876 is: 1.8446587324142456\n",
      "Prediction loss at step 2877 is: 1.541527509689331\n",
      "Prediction loss at step 2878 is: 1.4804365634918213\n",
      "Prediction loss at step 2879 is: 1.872578501701355\n",
      "Prediction loss at step 2880 is: 1.4668689966201782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss at step 2881 is: 2.084947347640991\n",
      "Prediction loss at step 2882 is: 2.1637959480285645\n",
      "Prediction loss at step 2883 is: 1.6575653553009033\n",
      "Prediction loss at step 2884 is: 2.1195454597473145\n",
      "Prediction loss at step 2885 is: 1.857672095298767\n",
      "Prediction loss at step 2886 is: 2.1025712490081787\n",
      "Prediction loss at step 2887 is: 2.023937702178955\n",
      "Prediction loss at step 2888 is: 1.9078893661499023\n",
      "Prediction loss at step 2889 is: 1.7667744159698486\n",
      "Prediction loss at step 2890 is: 1.5940639972686768\n",
      "Prediction loss at step 2891 is: 2.1114444732666016\n",
      "Prediction loss at step 2892 is: 1.2922427654266357\n",
      "Prediction loss at step 2893 is: 1.9634031057357788\n",
      "Prediction loss at step 2894 is: 2.1344902515411377\n",
      "Prediction loss at step 2895 is: 1.7042967081069946\n",
      "Prediction loss at step 2896 is: 1.8680201768875122\n",
      "Prediction loss at step 2897 is: 2.005751848220825\n",
      "Prediction loss at step 2898 is: 1.9263248443603516\n",
      "Prediction loss at step 2899 is: 1.930100440979004\n",
      "Prediction loss at step 2900 is: 1.9580423831939697\n",
      "Prediction loss at step 2901 is: 2.081000566482544\n",
      "Prediction loss at step 2902 is: 1.3659504652023315\n",
      "Prediction loss at step 2903 is: 1.4248565435409546\n",
      "Prediction loss at step 2904 is: 1.8161979913711548\n",
      "Prediction loss at step 2905 is: 1.77778959274292\n",
      "Prediction loss at step 2906 is: 1.6955641508102417\n",
      "Prediction loss at step 2907 is: 1.663894772529602\n",
      "Prediction loss at step 2908 is: 2.037781238555908\n",
      "Prediction loss at step 2909 is: 1.9984792470932007\n",
      "Prediction loss at step 2910 is: 1.4233884811401367\n",
      "Prediction loss at step 2911 is: 1.611628770828247\n",
      "Prediction loss at step 2912 is: 1.1498866081237793\n",
      "Prediction loss at step 2913 is: 1.194092869758606\n",
      "Prediction loss at step 2914 is: 1.8396461009979248\n",
      "Prediction loss at step 2915 is: 1.5385183095932007\n",
      "Prediction loss at step 2916 is: 1.9566980600357056\n",
      "Prediction loss at step 2917 is: 2.0155155658721924\n",
      "Prediction loss at step 2918 is: 1.2929778099060059\n",
      "Prediction loss at step 2919 is: 1.9496338367462158\n",
      "Prediction loss at step 2920 is: 1.6549632549285889\n",
      "Prediction loss at step 2921 is: 2.217639684677124\n",
      "Prediction loss at step 2922 is: 1.9029912948608398\n",
      "Prediction loss at step 2923 is: 2.1278076171875\n",
      "Prediction loss at step 2924 is: 1.9151966571807861\n",
      "Prediction loss at step 2925 is: 1.8290212154388428\n",
      "Prediction loss at step 2926 is: 1.6695797443389893\n",
      "Prediction loss at step 2927 is: 1.8953092098236084\n",
      "Prediction loss at step 2928 is: 1.8771750926971436\n",
      "Prediction loss at step 2929 is: 2.095405340194702\n",
      "Prediction loss at step 2930 is: 1.8218845129013062\n",
      "Prediction loss at step 2931 is: 1.6401493549346924\n",
      "Prediction loss at step 2932 is: 2.0471811294555664\n",
      "Prediction loss at step 2933 is: 1.6107561588287354\n",
      "Prediction loss at step 2934 is: 2.006157398223877\n",
      "Prediction loss at step 2935 is: 1.947571873664856\n",
      "Prediction loss at step 2936 is: 2.0291900634765625\n",
      "Prediction loss at step 2937 is: 1.9942115545272827\n",
      "Prediction loss at step 2938 is: 1.5966815948486328\n",
      "Prediction loss at step 2939 is: 1.6125794649124146\n",
      "Prediction loss at step 2940 is: 1.489599585533142\n",
      "Prediction loss at step 2941 is: 1.0344440937042236\n",
      "Prediction loss at step 2942 is: 1.4945855140686035\n",
      "Prediction loss at step 2943 is: 1.882015585899353\n",
      "Prediction loss at step 2944 is: 1.9598569869995117\n",
      "Prediction loss at step 2945 is: 2.1187591552734375\n",
      "Prediction loss at step 2946 is: 1.2787892818450928\n",
      "Prediction loss at step 2947 is: 1.2896183729171753\n",
      "Prediction loss at step 2948 is: 1.7907264232635498\n",
      "Prediction loss at step 2949 is: 1.0928244590759277\n",
      "Prediction loss at step 2950 is: 2.0872550010681152\n",
      "Prediction loss at step 2951 is: 1.4877023696899414\n",
      "Prediction loss at step 2952 is: 1.883914828300476\n",
      "Prediction loss at step 2953 is: 1.3385306596755981\n",
      "Prediction loss at step 2954 is: 2.033331871032715\n",
      "Prediction loss at step 2955 is: 1.9976102113723755\n",
      "Prediction loss at step 2956 is: 2.099940299987793\n",
      "Prediction loss at step 2957 is: 2.070955753326416\n",
      "Prediction loss at step 2958 is: 1.8941681385040283\n",
      "Prediction loss at step 2959 is: 1.6568148136138916\n",
      "Prediction loss at step 2960 is: 1.7399708032608032\n",
      "Prediction loss at step 2961 is: 1.9459108114242554\n",
      "Prediction loss at step 2962 is: 1.9408679008483887\n",
      "Prediction loss at step 2963 is: 2.2836809158325195\n",
      "Prediction loss at step 2964 is: 1.9247163534164429\n",
      "Prediction loss at step 2965 is: 2.067044496536255\n",
      "Prediction loss at step 2966 is: 2.236172676086426\n",
      "Prediction loss at step 2967 is: 2.190420150756836\n",
      "Prediction loss at step 2968 is: 1.6229592561721802\n",
      "Prediction loss at step 2969 is: 1.0788931846618652\n",
      "Prediction loss at step 2970 is: 1.2073962688446045\n",
      "Prediction loss at step 2971 is: 1.6830397844314575\n",
      "Prediction loss at step 2972 is: 1.7584713697433472\n",
      "Prediction loss at step 2973 is: 1.6432017087936401\n",
      "Prediction loss at step 2974 is: 2.0142030715942383\n",
      "Prediction loss at step 2975 is: 2.1223561763763428\n",
      "Prediction loss at step 2976 is: 1.9796427488327026\n",
      "Prediction loss at step 2977 is: 2.2356224060058594\n",
      "Prediction loss at step 2978 is: 1.4820895195007324\n",
      "Prediction loss at step 2979 is: 1.5281593799591064\n",
      "Prediction loss at step 2980 is: 1.6398193836212158\n",
      "Prediction loss at step 2981 is: 1.659559965133667\n",
      "Prediction loss at step 2982 is: 0.9201326370239258\n",
      "Prediction loss at step 2983 is: 1.466240406036377\n",
      "Prediction loss at step 2984 is: 1.6028825044631958\n",
      "Prediction loss at step 2985 is: 2.0620839595794678\n",
      "Prediction loss at step 2986 is: 1.8977032899856567\n",
      "Prediction loss at step 2987 is: 1.41108238697052\n",
      "Prediction loss at step 2988 is: 1.7687711715698242\n",
      "Prediction loss at step 2989 is: 1.7228703498840332\n",
      "Prediction loss at step 2990 is: 1.8179492950439453\n",
      "Prediction loss at step 2991 is: 1.5720384120941162\n",
      "Prediction loss at step 2992 is: 1.8214629888534546\n",
      "Prediction loss at step 2993 is: 1.5235122442245483\n",
      "Prediction loss at step 2994 is: 1.7367730140686035\n",
      "Prediction loss at step 2995 is: 2.0149283409118652\n",
      "Prediction loss at step 2996 is: 1.9436051845550537\n",
      "Prediction loss at step 2997 is: 1.5389823913574219\n",
      "Prediction loss at step 2998 is: 1.718544363975525\n",
      "Prediction loss at step 2999 is: 1.6327804327011108\n"
     ]
    }
   ],
   "source": [
    "#-------***************--------------#\n",
    "\n",
    "tf.summary.scalar('total_loss',total_loss)\n",
    "tf.summary.scalar('loss_warm_up', loss_warm_up)\n",
    "tf.summary.scalar('pos_loss_train', pos_loss_train)\n",
    "tf.summary.scalar('ori_loss_train', ori_loss_train)\n",
    "tf.summary.scalar('pred_loss',loss_prediction)\n",
    "tf.summary.scalar('pos_loss',pos_loss)\n",
    "tf.summary.scalar('ori_loss', ori_loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "train_rolling = RollingData(new_train_X, new_train_Y)\n",
    "train_writer = tf.summary.FileWriter(log_dir+'/train', sess.graph, flush_secs=2, max_queue=2)\n",
    "\n",
    "\n",
    "for e in range(3000):\n",
    "    print ('Epoch {}/{} '.format(e, training_steps), end='\\r')\n",
    "    train_X_batch, train_Y_batch = train_rolling.next_batch()\n",
    "    \n",
    "    _,pred_loss, _= sess.run([train_step,loss_prediction, total_loss], feed_dict={X:train_X_batch, Y:train_Y_batch})\n",
    "    zero_state = sess.run(imu_lstm_model.reset_state())\n",
    "    print (\"Prediction loss at step {} is: {}\".format(e, pred_loss))\n",
    "#     print ('Output state at the end of step {} is {}'.format(e, zero_state))\n",
    "\n",
    "    summary,  = sess.run([merged], feed_dict={X:train_X_batch, Y:train_Y_batch})\n",
    "    train_writer.add_summary(summary,e) \n",
    "    if e%300 ==0:\n",
    "        save_path = saver.save(sess,'../tmp/logs/sensor_fusion_LSTM_rel_pos/model_%d.ckpt'%(e))\n",
    "#     print('Hello test positioni correct ratio:{} \\t and loss:{}'.format(test_correct_ratio, test_loss))\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Drift + means prediction is always more extreme;  drift - prediction is mild (in relative percentage)\n",
    "# with tf.name_scope('metrics'):\n",
    "#     drift = (preds[:,:,:3]-Y[:,:,:3])/Y[:,:,:3]\n",
    "#     drift_truth_table = tf.to_float(tf.abs(drift) < drift_thredshold)\n",
    "#     drift_within_t = tf.reduce_prod(drift_truth_table, axis=2)\n",
    "#     correct_pred_ratio = tf.reduce_mean(drift_within_t)\n",
    "    \n",
    "# tf.summary.histogram('drift', drift)\n",
    "# tf.summary.histogram('drift_within_threshold', drift_within_t)   \n",
    "# tf.summary.scalar('loss',loss)\n",
    "# tf.summary.scalar('correct_pred_ratio', correct_pred_ratio)\n",
    "# merged = tf.summary.merge_all()\n",
    "\n",
    "# # my_metrics = my_metrics(Y, preds)\n",
    "# # p_x = my_metrics[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
